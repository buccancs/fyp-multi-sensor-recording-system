"""
Logger Utilities for Multi-Sensor Recording System Controller

⚠️  DEPRECATED: This module is being phased out in favor of the modern logging system
    in utils/logging_config.py. New code should use:
    
    from utils.logging_config import get_logger
    logger = get_logger(__name__)

This module provides placeholder functionality for advanced logging capabilities.
Some functionality like export_logs and cleanup_old_logs may be migrated to the
new logging system.

Author: Multi-Sensor Recording System Team
Date: 2025-07-29
Status: DEPRECATED - Use utils.logging_config instead
"""

from enum import Enum


class LogLevel(Enum):
    """Log level enumeration for structured logging."""

    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class LoggerManager:
    """
    Advanced logging management for the Multi-Sensor Recording System.

    Provides comprehensive logging functionality including:
    - File-based logging with rotation
    - Structured logging with JSON format
    - Log filtering and categorization
    - Performance logging and metrics
    - Integration with external logging services
    - Log analysis and reporting tools
    """

    def __init__(self, log_directory="logs", max_file_size_mb=10, backup_count=5):
        self.log_directory = log_directory
        self.max_file_size_mb = max_file_size_mb
        self.backup_count = backup_count
        self.loggers = {}
        self.log_handlers = {}

        # Initialize logging system
        self.setup_logging_directory()
        self.setup_default_loggers()

    def setup_logging_directory(self):
        """
        Set up the logging directory structure.
        """
        import os
        
        try:
            os.makedirs(self.log_directory, exist_ok=True)
            os.makedirs(os.path.join(self.log_directory, "application"), exist_ok=True)
            os.makedirs(os.path.join(self.log_directory, "network"), exist_ok=True)
            os.makedirs(os.path.join(self.log_directory, "calibration"), exist_ok=True)
            os.makedirs(os.path.join(self.log_directory, "performance"), exist_ok=True)
        except Exception as e:
            print(f"Error creating log directories: {e}")

    def setup_default_loggers(self):
        """
        Set up default loggers for different system components.

        Creates loggers for different modules (GUI, network, calibration)
        with appropriate log levels and formatting.
        """
        import logging
        
        print("[DEBUG_LOG] Setting up default loggers")

        logger_configs = {
            'application': {
                'level': logging.INFO,
                'file': 'application/application.log',
                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            },
            'network': {
                'level': logging.DEBUG,
                'file': 'network/network.log',
                'format': '%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s'
            },
            'calibration': {
                'level': logging.INFO,
                'file': 'calibration/calibration.log',
                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            },
            'performance': {
                'level': logging.INFO,
                'file': 'performance/performance.log',
                'format': '%(asctime)s - %(message)s'
            }
        }

        for logger_name, config in logger_configs.items():
            self.create_logger(logger_name, config)

    def create_logger(self, name, config):
        """
        Create a configured logger instance.

        Args:
            name (str): Logger name
            config (dict): Logger configuration

        Returns:
            logging.Logger: Configured logger instance
        """
        import logging
        import os
        from logging.handlers import RotatingFileHandler
        
        try:
            # Initialize loggers dict if not exists
            if not hasattr(self, 'loggers'):
                self.loggers = {}
            
            # Ensure log directory exists
            self.setup_logging_directory()
            
            logger = logging.getLogger(name)
            logger.setLevel(config['level'])

            # Create rotating file handler
            log_file = os.path.join(self.log_directory, config['file'])
            file_handler = RotatingFileHandler(
                log_file,
                maxBytes=self.max_file_size_mb * 1024 * 1024,
                backupCount=self.backup_count
            )
            file_handler.setLevel(config['level'])

            # Create console handler for development
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.WARNING)

            # Create formatter
            formatter = logging.Formatter(config['format'])
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)

            # Add handlers to logger
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)

            self.loggers[name] = logger
            return logger
            
        except Exception as e:
            print(f"Error creating logger '{name}': {e}")
            return None

    def get_logger(self, name):
        """
        Get a logger instance by name.

        Args:
            name (str): Logger name

        Returns:
            logging.Logger: Logger instance
        """
        import logging
        
        # Initialize loggers dict if not exists
        if not hasattr(self, 'loggers'):
            self.loggers = {}
        
        if name in self.loggers:
            return self.loggers[name]
        else:
            # Create logger with default configuration
            default_config = {
                'level': logging.INFO,
                'file': f'{name}.log',
                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            }
            return self.create_logger(name, default_config)

    def log_structured(self, logger_name, level, message, **kwargs):
        """
        Log a structured message with additional metadata.

        Args:
            logger_name (str): Name of the logger to use
            level (LogLevel): Log level
            message (str): Log message
            **kwargs: Additional structured data
        """
        import json
        import threading
        from datetime import datetime
        
        logger = self.get_logger(logger_name)
        if logger:
            structured_data = {
                'timestamp': datetime.utcnow().isoformat(),
                'level': level.value,
                'message': message,
                'thread_id': threading.current_thread().ident,
                'module': logger_name,
                **kwargs
            }

            json_message = json.dumps(structured_data, default=str)

            if level == LogLevel.DEBUG:
                logger.debug(json_message)
            elif level == LogLevel.INFO:
                logger.info(json_message)
            elif level == LogLevel.WARNING:
                logger.warning(json_message)
            elif level == LogLevel.ERROR:
                logger.error(json_message)
            elif level == LogLevel.CRITICAL:
                logger.critical(json_message)
        else:
            # Fallback to print if logger not available
            print(f"[{level.value}] {logger_name}: {message}")

    def log_performance(self, operation, duration_ms, **metadata):
        """
        Log performance metrics for operations.

        Args:
            operation (str): Name of the operation
            duration_ms (float): Operation duration in milliseconds
            **metadata: Additional performance metadata
        """
        from datetime import datetime
        
        try:
            # Try to get system resource info, fallback gracefully if not available
            try:
                import psutil
                memory_usage_mb = psutil.Process().memory_info().rss / 1024 / 1024
                cpu_percent = psutil.Process().cpu_percent()
            except ImportError:
                memory_usage_mb = None
                cpu_percent = None
            
            performance_data = {
                'operation': operation,
                'duration_ms': duration_ms,
                'timestamp': datetime.utcnow().isoformat(),
                'memory_usage_mb': memory_usage_mb,
                'cpu_percent': cpu_percent,
                **metadata
            }

            self.log_structured('performance', LogLevel.INFO, f"Operation completed: {operation}", **performance_data)
            
        except Exception as e:
            # Fallback to simple print if structured logging fails
            print(f"[PERFORMANCE] {operation} took {duration_ms}ms (logging error: {e})")

    def log_network_event(self, event_type, device_id, **details):
        """
        Log network-related events.

        Args:
            event_type (str): Type of network event
            device_id (str): Device identifier
            **details: Additional event details
        """
        from datetime import datetime
        
        try:
            network_data = {
                'event_type': event_type,
                'device_id': device_id,
                'timestamp': datetime.utcnow().isoformat(),
                **details
            }

            self.log_structured('network', LogLevel.INFO, f"Network event: {event_type}", **network_data)
            
        except Exception as e:
            # Fallback to simple print if structured logging fails
            print(f"[NETWORK] {event_type} for device {device_id} (logging error: {e})")

    def log_calibration_event(self, event_type, **details):
        """
        Log calibration-related events.

        Args:
            event_type (str): Type of calibration event
            **details: Additional event details
        """
        from datetime import datetime
        
        try:
            calibration_data = {
                'event_type': event_type,
                'timestamp': datetime.utcnow().isoformat(),
                **details
            }

            self.log_structured('calibration', LogLevel.INFO, f"Calibration event: {event_type}", **calibration_data)
            
        except Exception as e:
            # Fallback to simple print if structured logging fails
            print(f"[CALIBRATION] {event_type} (logging error: {e})")

    def export_logs(self, start_date, end_date, output_format="json"):
        """
        Export logs for a specific date range.

        Args:
            start_date (datetime): Start date for log export
            end_date (datetime): End date for log export
            output_format (str): Export format ('json', 'csv', 'txt')

        Returns:
            str: Path to exported log file
        """
        import os
        import json
        import csv
        import gzip
        from datetime import datetime
        
        try:
            # Create exports directory
            exports_dir = os.path.join(self.log_directory, "exports")
            os.makedirs(exports_dir, exist_ok=True)
            
            # Generate export filename
            export_filename = f"logs_export_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}.{output_format}"
            export_path = os.path.join(exports_dir, export_filename)
            
            # Collect log entries from all log files
            log_entries = []
            
            # Walk through log directory and process files
            for root, dirs, files in os.walk(self.log_directory):
                for file in files:
                    if file.endswith('.log'):
                        file_path = os.path.join(root, file)
                        file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                        
                        # Check if file is within date range
                        if start_date <= file_mtime <= end_date:
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    for line in f:
                                        line = line.strip()
                                        if line:
                                            # Try to parse as JSON first
                                            try:
                                                log_entry = json.loads(line)
                                                log_entries.append(log_entry)
                                            except json.JSONDecodeError:
                                                # If not JSON, treat as plain text log
                                                log_entries.append({
                                                    'timestamp': file_mtime.isoformat(),
                                                    'level': 'INFO',
                                                    'message': line,
                                                    'source_file': file
                                                })
                            except Exception as e:
                                print(f"Error reading log file {file_path}: {e}")
            
            # Export based on format
            if output_format.lower() == 'json':
                with open(export_path, 'w', encoding='utf-8') as f:
                    json.dump(log_entries, f, indent=2, default=str)
            
            elif output_format.lower() == 'csv':
                if log_entries:
                    # Get all possible keys for CSV headers
                    all_keys = set()
                    for entry in log_entries:
                        all_keys.update(entry.keys())
                    
                    with open(export_path, 'w', newline='', encoding='utf-8') as f:
                        writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
                        writer.writeheader()
                        writer.writerows(log_entries)
            
            elif output_format.lower() == 'txt':
                with open(export_path, 'w', encoding='utf-8') as f:
                    for entry in log_entries:
                        if isinstance(entry, dict):
                            timestamp = entry.get('timestamp', 'Unknown')
                            level = entry.get('level', 'INFO')
                            message = entry.get('message', str(entry))
                            f.write(f"[{timestamp}] {level}: {message}\n")
                        else:
                            f.write(f"{entry}\n")
            
            # Compress if file is large (>1MB)
            if os.path.getsize(export_path) > 1024 * 1024:
                compressed_path = export_path + '.gz'
                with open(export_path, 'rb') as f_in:
                    with gzip.open(compressed_path, 'wb') as f_out:
                        f_out.writelines(f_in)
                os.remove(export_path)
                export_path = compressed_path
            
            print(f"Logs exported to: {export_path}")
            return export_path
            
        except Exception as e:
            print(f"Error exporting logs: {e}")
            return ""

    def cleanup_old_logs(self, retention_days=30):
        """
        Clean up old log files based on retention policy.

        Args:
            retention_days (int): Number of days to retain logs

        Returns:
            dict: Cleanup report with removed files and errors
        """
        import os
        import gzip
        import shutil
        from datetime import datetime, timedelta
        
        cleanup_report = {
            'removed_files': [],
            'compressed_files': [],
            'errors': [],
            'total_space_freed': 0
        }
        
        try:
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            archive_cutoff = datetime.now() - timedelta(days=7)  # Compress files older than 7 days
            
            for root, dirs, files in os.walk(self.log_directory):
                # Skip exports directory
                if 'exports' in root:
                    continue
                    
                for file in files:
                    if file.endswith('.log') or file.endswith('.log.gz'):
                        file_path = os.path.join(root, file)
                        
                        try:
                            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                            file_size = os.path.getsize(file_path)
                            
                            # Remove files older than retention period
                            if file_mtime < cutoff_date:
                                os.remove(file_path)
                                cleanup_report['removed_files'].append(file_path)
                                cleanup_report['total_space_freed'] += file_size
                                print(f"Removed old log file: {file_path}")
                            
                            # Compress files older than 7 days but within retention period
                            elif file_mtime < archive_cutoff and file.endswith('.log'):
                                compressed_path = file_path + '.gz'
                                with open(file_path, 'rb') as f_in:
                                    with gzip.open(compressed_path, 'wb') as f_out:
                                        shutil.copyfileobj(f_in, f_out)
                                
                                # Remove original file after compression
                                os.remove(file_path)
                                cleanup_report['compressed_files'].append(compressed_path)
                                space_saved = file_size - os.path.getsize(compressed_path)
                                cleanup_report['total_space_freed'] += space_saved
                                print(f"Compressed log file: {file_path} -> {compressed_path}")
                        
                        except Exception as e:
                            error_msg = f"Error processing log file {file_path}: {e}"
                            cleanup_report['errors'].append(error_msg)
                            print(error_msg)
            
            print(f"Log cleanup completed. Freed {cleanup_report['total_space_freed']} bytes")
            return cleanup_report
            
        except Exception as e:
            error_msg = f"Error during log cleanup: {e}"
            cleanup_report['errors'].append(error_msg)
            print(error_msg)
            return cleanup_report


# Global logger manager instance
logger_manager = None


def get_logger_manager():
    """
    Get the global logger manager instance.

    Returns:
        LoggerManager: Global logger manager
    """
    global logger_manager
    if logger_manager is None:
        try:
            # Try to load configuration from file if it exists
            import os
            config_path = os.path.join(os.path.dirname(__file__), '..', '..', 'config', 'logging.json')
            if os.path.exists(config_path):
                import json
                with open(config_path, 'r') as f:
                    config = json.load(f)
                logger_manager = LoggerManager(
                    log_directory=config.get('log_directory', 'logs'),
                    max_file_size_mb=config.get('max_file_size_mb', 10),
                    backup_count=config.get('backup_count', 5)
                )
            else:
                # Use default configuration
                logger_manager = LoggerManager()
        except Exception as e:
            print(f"Warning: Error loading logger configuration: {e}. Using defaults.")
            logger_manager = LoggerManager()
    return logger_manager


def log_info(logger_name, message, **kwargs):
    """
    Convenience function for info-level logging.

    Args:
        logger_name (str): Logger name
        message (str): Log message
        **kwargs: Additional structured data
    """
    get_logger_manager().log_structured(logger_name, LogLevel.INFO, message, **kwargs)


def log_error(logger_name, message, **kwargs):
    """
    Convenience function for error-level logging.

    Args:
        logger_name (str): Logger name
        message (str): Log message
        **kwargs: Additional structured data
    """
    get_logger_manager().log_structured(logger_name, LogLevel.ERROR, message, **kwargs)


def log_debug(logger_name, message, **kwargs):
    """
    Convenience function for debug-level logging.

    Args:
        logger_name (str): Logger name
        message (str): Log message
        **kwargs: Additional structured data
    """
    get_logger_manager().log_structured(logger_name, LogLevel.DEBUG, message, **kwargs)


def log_warning(logger_name, message, **kwargs):
    """
    Convenience function for warning-level logging.

    Args:
        logger_name (str): Logger name
        message (str): Log message
        **kwargs: Additional structured data
    """
    get_logger_manager().log_structured(logger_name, LogLevel.WARNING, message, **kwargs)


def log_critical(logger_name, message, **kwargs):
    """
    Convenience function for critical-level logging.

    Args:
        logger_name (str): Logger name
        message (str): Log message
        **kwargs: Additional structured data
    """
    get_logger_manager().log_structured(logger_name, LogLevel.CRITICAL, message, **kwargs)
