<h1 id="chapter-1-introduction">Chapter 1: Introduction</h1>
<h2 id="motivation-and-research-context">1.1 Motivation and Research
Context</h2>
<p>In recent years, interest in <strong>physiological computing</strong>
has grown — using bodily signals to infer a person’s internal states in
health monitoring, affective computing, and human-computer interaction.
One especially valuable physiological signal is the <strong>Galvanic
Skin Response (GSR)</strong> (also known as electrodermal activity or
skin conductance). GSR measures subtle changes in the skin’s electrical
conductance caused by sweat gland activity, which is directly modulated
by the sympathetic nervous system [1]. These involuntary changes reflect
emotional arousal and stress, making GSR a widely accepted indicator of
autonomic nervous system activity [1]. GSR is used in clinical
psychology (e.g., biofeedback therapy and polygraph tests) and in user
experience research, where it can reveal unconscious stress or emotional
responses. Even consumer technology now leverages skin conductance:
modern smartwatches from Apple and Samsung include sensors for
continuous stress monitoring via GSR or related metrics [2]. This surge
of interest underscores the <em>motivation</em> to harness physiological
signals like GSR in everyday contexts.</p>
<p>Despite GSR’s value, traditional measurement requires skin-contact
electrodes (typically attached to fingers or palms with conductive gel)
[3]. This method is obtrusive: wires and electrodes restrict movement,
and long-term use can cause discomfort or skin irritation [3]. These
limitations make it difficult to use GSR outside the lab. Consequently,
<strong>contactless GSR measurement</strong> has become an appealing
research direction [4]. The idea is to infer GSR (or the underlying
psychophysiological arousal) using remote sensors that require no
physical contact with the user. For example, thermal infrared cameras
detect subtle skin temperature changes from blood flow and perspiration,
offering a proxy for stress responses [5].</p>
<p>Facial thermal imaging is promising as a complementary measure in
emotion research, given that stress and thermoregulation are linked
(e.g., perspiration causes cooling) [5]. Similarly, high-resolution RGB
video combined with advanced computer vision can non-invasively capture
other physiological signals. Prior work shows that heart rate and
breathing can be measured from video of a person’s face or body [6].
These developments suggest that <em>multi-modal sensing</em> — combining
traditional biosensors with imaging — could enable <strong>contactless
physiological monitoring</strong> in the near future. Affective
computing research indicates that fusing multiple modalities (e.g., GSR,
heart rate, facial thermal data) can capture emotional or stress states
more robustly [1].</p>
<p>However, significant challenges remain. A key <em>research gap</em>
is the lack of an integrated platform to synchronise these diverse data
streams. Most prior studies addressed contactless GSR estimation in
isolation or in highly controlled conditions, often using separate
devices that were not synchronised in real time [7]. For instance,
thermal cameras and wearable GSR sensors have typically been used
independently, with any data fusion done post hoc. This piecemeal
approach complicates machine learning model development, since models
require well-aligned datasets of inputs (e.g., video or thermal data)
and outputs (measured GSR). Clearly, a <strong>multi-modal data
collection platform</strong> is needed to record GSR and other sensor
modalities simultaneously with proper synchronisation. Such a platform
would allow researchers to gather rich, time-aligned datasets. For
example, thermal video of a participant’s face could be recorded in sync
with their GSR signal. These combined data would lay the groundwork for
training and validating models that infer GSR from camera-based sensors.
<strong>The primary contribution of this thesis is the development of
just such a platform:</strong> a modular, multi-sensor system for
synchronised physiological data acquisition designed for future GSR
prediction research. In summary, this work is motivated by recent trends
in physiological computing and multimodal sensing, and by the need for
robust, synchronised datasets to advance <em>contactless</em> GSR
measurement.</p>
<h2 id="research-problem-and-objectives">1.2 Research Problem and
Objectives</h2>
<p>Given this context, the <strong>research problem</strong> can be
stated as follows: <em>there is no system readily available to
synchronously collect GSR signals alongside complementary data streams
(such as thermal and visual data) in naturalistic settings, which
hinders the development of machine learning models for contactless GSR
prediction</em>. While traditional GSR sensors provide reliable ground
truth, they are intrusive in real-world use, and fully contactless
approaches remain unvalidated or imprecise [8]. Bridging this gap
requires a platform that records <strong>multiple modalities
simultaneously</strong> — for example, capturing a person’s skin
conductance with a wearable sensor while concurrently recording thermal
and visual data. Crucially, all data must be tightly time-synchronised
to allow meaningful correlation and learning. The absence of such an
integrated system is the core problem that this thesis addresses.</p>
<p>The <em>objective</em> of this research is to design and implement a
<strong>multi-modal physiological data collection platform</strong> to
create a synchronised dataset for future GSR prediction models. Unlike
end-user applications or final predictive systems, this work focuses on
the data acquisition infrastructure — essentially building the
<em>foundation</em> on which real-time GSR inference algorithms can be
developed later. Note that <strong>real-time GSR prediction is outside
the scope</strong> of this thesis. Instead, the project aims to
facilitate future machine learning by providing a robust way to gather
ground-truth GSR and candidate predictor signals together. The following
specific objectives have been defined to achieve this aim:</p>
<ul>
<li><p><strong>Objective 1: Multi-Modal Platform Development.</strong>
<em>Design and develop a modular data acquisition system capable of
recording synchronised physiological and imaging data.</em> This
involves integrating a wearable GSR sensor and camera-based sensors into
one platform. In practice, the system uses a
<strong>research-grade</strong> Shimmer3 GSR+ sensor [8] for
ground-truth skin conductance, a Topdon TC001 thermal camera [Topdon
Technology(2024a)] attached to a smartphone for thermal video, and the
smartphone’s built-in RGB camera for high-resolution video. A
smartphone-based sensor node will be coordinated with a desktop
controller to start and stop recordings in unison and to timestamp all
data consistently. The architecture should ensure that all modalities
are recorded simultaneously with millisecond-level alignment.</p></li>
<li><p><strong>Objective 2: Synchronised Data Acquisition and
Management.</strong> <em>Implement methods for precise time
synchronisation and data handling across devices.</em> A custom
<strong>control and synchronisation layer</strong> (in Python) will
coordinate the sensor node(s) and ensure that GSR readings, thermal
frames, and RGB frames are all logged with synchronised timestamps. This
includes establishing a reliable communication protocol between the
smartphone and the PC controller to transmit control commands and
streaming data [9]. Data management is also addressed: multi-modal data
will be stored in appropriate formats with metadata for easy combination
and analysis. The outcome should be a well-synchronised dataset (e.g.,
physiological sample timestamps aligned with video frame times) that can
serve as a training corpus for machine learning.</p></li>
<li><p><strong>Objective 3: System Validation through Pilot Data
Collection.</strong> <em>Evaluate the integrated platform’s performance
and data integrity in a real recording scenario.</em> Test recording
sessions will be conducted to verify that the system meets
research-grade requirements. For example, pilot experiments may involve
human participants performing tasks designed to elicit varying GSR
responses (stress, stimuli, etc.) while the platform records all
modalities. Validation will focus on temporal synchronisation accuracy
(e.g., confirming events are correctly aligned across sensor streams)
and the quality of the recorded signals (e.g., GSR signal-to-noise
ratio, thermal image resolution). The collected data will be analysed to
ensure that GSR signals and corresponding thermal/RGB data show expected
correlations or time-locked changes. Successful validation will
demonstrate that the platform can reliably capture synchronised
multi-modal data suitable for subsequent machine learning analysis.
(Developing the predictive model itself is left for future work; here
the focus is on validating the <em>data pipeline</em> that would feed
such a model.)</p></li>
</ul>
<p>By accomplishing these objectives, this thesis delivers a
multi-sensor data collection platform that fills the current gap. The
platform will enable researchers to build <strong>multimodal
datasets</strong> for GSR prediction, helping pave the way toward fully
contactless, real-time stress monitoring. The project emphasises a
flexible, extensible setup — a <strong>modular sensing system</strong> —
that integrates the devices used here (GSR sensor and thermal/RGB
cameras) and can be extended to additional modalities in the future.
Ultimately, this work lays the groundwork for future studies to train
and test machine learning algorithms to estimate GSR from camera data,
by first solving the critical challenge of <em>acquiring synchronised
ground-truth data</em>.</p>
<h2 id="thesis-outline">1.3 Thesis Outline</h2>
<p>This thesis is organised into six chapters, following a logical
progression from background concepts through system development to
evaluation:</p>
<ul>
<li><p><strong>Chapter 2 -- Background and Research Context:</strong>
This chapter reviews relevant literature and technical background. It
covers physiological computing and emotion recognition, the importance
of GSR in stress research, and prior approaches to contactless
physiological measurement. Key related works in <strong>multimodal data
collection</strong> and sensor fusion are examined to show the state of
the art and the gap addressed by this research. The chapter also
explains the rationale for choosing the specific sensors (Shimmer3 GSR+
and Topdon thermal camera) and the expected advantages of a multimodal
approach.</p></li>
<li><p><strong>Chapter 3 -- Requirements Analysis:</strong> This chapter
defines the specific requirements for the data collection platform. The
research problem is analysed to derive both <strong>functional
requirements</strong> (e.g., the ability to record multiple streams
concurrently, synchronisation accuracy, user interface needs) and
<strong>non-functional requirements</strong> (e.g., system reliability,
timing precision, data storage considerations). Use-case scenarios and
user stories illustrate these requirements in practical research
situations. By the end of this chapter, the scope of the system and the
criteria for success are clearly established.</p></li>
<li><p><strong>Chapter 4 -- System Design and Architecture:</strong>
This chapter describes the design of the proposed multi-modal recording
system, presenting the overall <strong>architecture</strong> and how
hardware and software components interact. Key design decisions are
discussed (e.g., choosing a distributed setup with an Android smartphone
as a sensor hub and a PC as the central controller [9]). The chapter
details how the <strong>hardware integration</strong> is achieved
(mounting the thermal camera on the phone, Bluetooth pairing with the
GSR sensor, etc.) and how the software is structured into modules for
camera capture, sensor communication, network synchronisation, and data
logging. Diagrams illustrate the flow of data and control commands
between the Android app and the Python desktop application. The design
emphasises modularity, so each sensing component (thermal, RGB, GSR)
operates in sync under the coordination of the central controller.
Important considerations such as timestamp synchronisation, latency
handling, and error recovery are also described.</p></li>
<li><p><strong>Chapter 5 -- Evaluation and Testing:</strong> This
chapter documents the testing methodology, implementation, and results
for the multi-sensor recording system. The testing covers unit tests,
integration tests, and system-level performance evaluation. Unit tests
on Android (JUnit/Robolectric) and PC (pytest/unittest) verify
functionality, including error handling and security features.
Integration tests use a DeviceSimulator and a JSON-based message
protocol (with optional TLS) to validate multi-device synchronisation.
System performance is evaluated through 8-hour endurance testing, memory
leak detection (via linear regression analysis), and CPU/throughput
monitoring with resource utilisation tracking. The results confirm that
the system meets its functionality, reliability, and performance
requirements, demonstrating research-grade reliability for scientific
data collection. This validation shows the platform’s capability as a
data collection tool for future GSR prediction research. Any observed
limitations (e.g., minor synchronisation offsets or sensor noise issues)
are noted to guide future improvements.</p></li>
<li><p><strong>Chapter 6 -- Conclusion and Future Work:</strong> This
final chapter summarises the thesis contributions and reflects on how
well the objectives were achieved. It highlights the success of
developing the multi-modal data collection platform and discusses its
significance for the research community. The chapter also addresses the
current system’s limitations (e.g., lack of real-time analysis or
untested environments). Finally, it outlines future work and
recommendations, including next steps like using the collected data to
train GSR prediction models, improving the platform’s real-time
capabilities, and extending the system with additional sensors (such as
heart rate or respiration) to broaden its application. These future
directions provide a roadmap for moving from this data collection
foundation toward full-fledged <strong>real-time GSR inference</strong>
in subsequent research.</p></li>
</ul>
<h2 id="references">References</h2>
<p>See <a href="references.md">centralized references</a> for all
citations used throughout this thesis.</p>
<h1 id="chapter-2.-background-and-literature-review">Chapter 2.
Background and Literature Review</h1>
<h2 id="emotion-analysis-applications">2.1 Emotion Analysis
Applications</h2>
<p>Emotion analysis – the automated detection of human emotional states
– has broad applications across psychology, human-computer interaction,
healthcare, and other fields. In <strong>affective computing</strong>,
researchers leverage physiological signals to recognize emotions and
improve user interfaces or social robots [10]. For example, **galvanic
skin response</p>
<p>(GSR)** and related biosignals are commonly used to gauge emotional
arousal in lab studies and real-world monitoring. GSR data can reveal
how subjects react to stimuli even when self-reports are biased, making
it valuable in areas like <em>stress and anxiety studies</em>, <em>user
experience testing</em>, and <em>neuromarketing</em> [3]. In
psychological research, changes in skin conductance have been analyzed
to understand anxiety during therapy or excitement during gameplay [3].
In **human-computer</p>
<p>interaction (HCI)**, measuring unconscious physiological responses
(e.g. via GSR, heart rate, facial cues) helps evaluate user stress or
engagement with software interfaces [3]. Even in critical domains such
as driver monitoring and workplace safety, detecting stress or fatigue
through physiological signals is being explored to prevent
accidents.</p>
<p>Multi-modal emotion recognition systems often combine signals – e.g.
facial expressions (from video) with physiological sensors – to improve
robustness. Visible behaviours alone can be masked or voluntarily
controlled, whereas internal signals like GSR or heart rate reflect
involuntary arousal [1]. This has motivated the integration of
<strong>wearable sensors</strong> and <strong>imaging</strong> for
richer emotion analysis. This work follows this trend: the system
employs a wearable GSR sensor alongside camera-based thermal imaging to
capture both external and internal indicators of stress. By collecting
synchronised video, thermal, and biosensor data, the platform caters to
emerging applications that require <strong>contactless yet reliable
emotion sensing</strong> – for instance, continuous stress monitoring in
everyday environments or adaptive systems that respond to a user’s
hidden emotional state. Such a multi-modal approach can enhance
detection accuracy and provide insight into the physiological
underpinnings of emotional reactions.</p>
<h2 id="rationale-for-contactless-physiological-measurement">2.2
Rationale for Contactless Physiological Measurement</h2>
<p>Traditional methods of measuring physiological signals often rely on
contact sensors (electrodes, chest straps, finger clips, etc.), which,
while accurate, can be obtrusive. For example, capturing GSR
conventionally requires attaching electrodes to the fingers or palm, and
measuring stress hormones requires drawing blood or saliva. These
intrusive methods may interfere with natural behaviour and are
impractical for continuous real-life monitoring [1]. In contrast,
<strong>contactless measurement</strong> uses remote sensors like
cameras to gauge physiological changes without direct skin contact. The
rationale for pursuing contactless techniques is twofold:
<strong>improved comfort and ecological validity</strong>, and
<strong>broader deployment potential</strong>.</p>
<p>From a research perspective, non-intrusive monitoring helps preserve
the subject’s natural physiological response. Wearing electrodes or
being tethered to instruments can itself induce stress or discomfort,
potentially confounding the very signals under study [1]. By using
cameras (thermal or optical) at a distance, one can monitor heart rate,
facial temperature, respiration, or other stress markers while the
person remains unencumbered. This approach is especially valuable in
settings like psychotherapy sessions, classrooms, or daily work
environments where attaching sensors would be impractical. Researchers
have explicitly called for <strong>contactless alternatives</strong> to
common wearables, noting that camera-based methods could capture
autonomic responses without the need for electrodes or wires [5].</p>
<p>The second advantage is scalability and convenience. Camera-based
physiological monitoring can leverage ubiquitous devices (smartphones,
CCTV, laptop cameras), enabling stress detection in the wild. For
instance, recent work has shown that a simple smartphone camera
(recording a person’s face or fingertip) can extract heart rate via
photoplethysmography, and a compact thermal camera can capture
stress-induced temperature changes[3]. Such approaches open the door to
<strong>ambient stress sensing</strong> – imagine vehicles or smart
rooms that assess occupants’ stress without requiring wearables.
Moreover, in scenarios like public health screening or large-scale
studies, contactless methods allow rapid measurements while maintaining
hygiene and physical distancing. This proved especially pertinent during
the COVID-19 pandemic, where contact-free vital sign measurement gained
interest.</p>
<p>In the context of GSR prediction, the goal is to <strong>infer
stress-related GSR levels without the GSR sensor</strong>. Achieving
this requires collecting data from a contact sensor (for ground truth)
in parallel with contactless surrogates, then training models to predict
the former from the latter. The motivation is that if GSR can be
reliably predicted from, say, thermal imaging and RGB video, future
stress monitoring could eliminate the need for a physically attached
galvanic sensor. Overall, the pursuit of contactless physiological
measurement aligns with making stress and emotion monitoring more
<strong>natural, scalable, and user-friendly</strong>, which is why the
platform emphasises integrating a thermal camera and other non-contact
modalities alongside the traditional sensors.</p>
<h2 id="definitions-of-stress-scientific-vs.-colloquial">2.3 Definitions
of "Stress" (Scientific vs. Colloquial)</h2>
<p>The term “stress” carries distinct meanings in scientific literature
versus everyday language. In colloquial use, <em>stress</em> often
refers to a subjective feeling of pressure, anxiety, or being
overwhelmed. People say they are “stressed out” referring to
psychological strain or emotional tension. In scientific terms, however,
stress is defined more broadly as the body’s <strong>physiological and
psychological response to any demand or challenge</strong>. Hans Selye
famously defined stress as “the nonspecific result of any demand upon
the body, whether mental or somatic”[4]. This definition frames stress
as an <strong>adaptive response</strong> by the organism, encompassing a
wide range of stimuli (stressors) and responses, not all of which are
negative. Scientifically, stress involves activation of neural and
endocrine pathways (notably the sympathetic nervous system and the
hypothalamic–pituitary–adrenal axis) that mobilise the body to cope with
perceived challenges.</p>
<p>A key distinction is that colloquial usage nearly always implies
<em>distress</em> – an undesirable state of worry or nervousness –
whereas scientific discourse recognises that not all stress is harmful.
Selye introduced terms like <em>eustress</em> (positive, beneficial
stress) and <em>distress</em> (negative, harmful stress)[8]. For
example, excitement before a competition might be considered eustress
(heightened arousal that can improve performance), in contrast to
chronic anxiety which is distress. In daily language, this nuance is
often lost, as any intense pressure tends to be labeled simply as
“stress.” Another difference lies in perspective: colloquially one might
say “this job is causing me stress,” focusing on external stressors,
whereas scientifically this thesis distinguishes the <em>stressor</em>
(e.g. job demands) from the <em>stress response</em> (the person’s
physiological reaction).</p>
<p>It is important in a study about stress to clarify definitions, since
the goal is to measure and predict a person’s <strong>stress
state</strong>. In this thesis, stress is aligned with the scientific
view: stress is treated as a psychophysiological state arising from
certain demands or challenges, characterised by activation of specific
biological systems. The thesis is particularly concerned with the
<strong>acute stress response</strong>, which involves sympathetic
nervous system arousal (“fight-or-flight”) and the release of stress
hormones. This response can be triggered by both negative and positive
stimuli (fear, workload, excitement, etc.), so mere detection of arousal
(e.g. via GSR) does not tell us if the person is “stressed” in the
everyday sense of anxious or upset. Throughout this work, elevated GSR
or cortisol are interpreted as indicators of <em>physiological stress
arousal</em>, which typically correlates with what people consider
stress but still requires context. In sum, <em>scientific stress</em>
refers to a measurable response of the organism (which can be neutral or
even beneficial in moderation), whereas <em>colloquial stress</em>
usually denotes an excessive or unpleasant psychological state[4].
Bridging this gap is part of the challenge in stress monitoring: the aim
is to predict and ultimately detect when someone’s physiological signals
suggest they are under strain likely to be perceived as “stress” in the
everyday sense.</p>
<h2 id="cortisol-vs.-gsr-as-stress-indicators">2.4 Cortisol vs. GSR as
Stress Indicators</h2>
<p>When measuring stress, researchers often distinguish between
<strong>endocrine indicators</strong> and <strong>electrodermal
indicators</strong>. <em>Cortisol</em>, a glucocorticoid hormone
released by the adrenal cortex, is widely regarded as a
<em>gold-standard</em> biochemical indicator of stress, reflecting
activation of the hypothalamic–pituitary–adrenal (HPA) axis[8].
<em>Galvanic Skin Response (GSR)</em>, also known as electrodermal
activity (EDA), is a peripheral measure of sympathetic nervous system
arousal (part of the “fight-or-flight” response) detectable as changes
in skin conductance. Both have been used extensively in stress research,
but they differ significantly in physiology, time course, and
practicality.</p>
<p><strong>Cortisol:</strong> Upon a significant stressor, the HPA axis
is engaged – the hypothalamus and pituitary trigger cortisol release
from the adrenal glands. Cortisol’s effects (raising blood sugar,
suppressing non-essential functions) prepare the body to handle a
prolonged challenge. A hallmark of cortisol is its <em>delayed
peak</em>: cortisol levels rise gradually, typically peaking about 20–30
minutes after the onset of an acute stressor[7]. For example, a sudden
fright or mental challenge will elicit immediate nervous system
responses, but the maximum cortisol concentration in saliva or blood
occurs roughly half an hour later as part of the recovery/adaptation
phase. Additionally, cortisol is usually measured through analysis of
saliva, blood, or hair samples. These methods, while accurate, are
<strong>intrusive or slow</strong> – requiring sampling and laboratory
assays – making them impractical for real-time or continuous
monitoring[11]. Despite these challenges, cortisol provides a <em>direct
index of HPA-axis activity</em> and is invaluable for validating other
stress measures. It is often considered a ground truth for chronic or
cumulative stress load and is sensitive to factors like circadian rhythm
(e.g. the cortisol awakening response each morning).</p>
<p><strong>GSR (Electrodermal Activity):</strong> In contrast, GSR
reflects <em>sympathetic nervous system (SNS) activation</em> and can
change within seconds of a stressor. Psychological or physical stress
leads to an immediate surge in sympathetic signals, causing sweat glands
(especially on palms and soles) to secrete moisture. Even before visible
perspiration, this sweat increases the skin’s electrical conductance.
Thus, a GSR sensor can detect a spike often <strong>within 1–5
seconds</strong> of a stimulus – far faster than cortisol changes[7].
GSR is essentially measuring one facet of the “fight-or-flight”
response: the eccrine sweat gland activation that accompanies arousal.
Because of this, skin conductance peaks are tightly coupled with moments
of surprise, anxiety, or effort. <em>Each GSR peak can be thought of as
the footprint of a sympathetic arousal event</em>, typically followed by
a rise in cortisol 20–30 minutes later[7]. Researchers have leveraged
this relationship – for example, using patterns of GSR peaks to predict
impending cortisol elevations in stress experiments[7]. Unlike cortisol,
measuring GSR is non-invasive and continuous: a pair of electrodes on
the skin can stream data in real time. Modern wearable devices make it
relatively easy to log GSR over hours or days.</p>
<p><strong>Indicator Comparison:</strong> Both cortisol and GSR are
valid stress indicators, but they tap into different arms of the stress
response. Cortisol is a <strong>hormonal indicator</strong> (HPA axis)
with a slow, sustained response, useful for assessing total stress
exposure and recovery over tens of minutes to hours[7]. GSR is a
<strong>neuronal indicator</strong> (sympathetic SAM axis) with an
immediate, phasic response, useful for detecting brief arousal events
and moment-to-moment intensity. Cortisol is specific to stress in that
large increases (beyond normal diurnal variation) usually imply a
significant stressor; however, cortisol measurement is costly and cannot
easily distinguish multiple short stress incidents from one prolonged
event without frequent sampling. GSR, by contrast, is extremely
responsive but <em>not specific to stress per se</em> – any stimulus
that triggers arousal (startle, pain, excitement, cognitive effort,
etc.) will produce a GSR change[12]. Thus, context is required to
interpret GSR: one typically combines it with known experimental
conditions or other signals to infer “stress” as opposed to general
arousal.</p>
<p>Another practical difference is data quality and ease of measurement.
Cortisol assessment requires trained personnel and lab equipment;
salivary cortisol, for instance, involves participants drooling into
tubes and then lab immunoassays or mass spectrometry. The delay in
obtaining results (hours or days) means cortisol cannot provide
real-time feedback. GSR sensors, on the other hand, are inexpensive and
offer immediate data, but are susceptible to noise (e.g. motion
artifacts or temperature effects on the skin). Moreover, while cortisol
readings are scalar values at specific sample times, GSR is a continuous
waveform requiring interpretation (e.g. tonic level vs. phasic peaks).
Despite these differences, studies frequently observe that GSR and
cortisol correlate under certain stress paradigms – acute stressors that
cause a clear cortisol rise also tend to evoke increased skin
conductance, though the correlation is far from perfect[7]. This
underlines that they measure related but distinct aspects of the stress
response.</p>
<p>In summary, cortisol and GSR serve complementary roles in stress
research. Cortisol is a <strong>direct chemical marker</strong> of
stress with high specificity but poor timeliness, whereas GSR is an
<strong>immediate electrical marker</strong> of arousal with excellent
temporal resolution but lower specificity. This project focuses on GSR
as the target for prediction due to its real-time nature – the
envisioned system could eventually estimate “what would the person’s GSR
be now” from contactless signals. However, understanding cortisol’s
behavior is important for broader context, and one could extend this
work by also predicting cortisol levels from non-invasive signals.
Notably, one recent study modeled cortisol responses by convolving skin
conductance peaks, effectively estimating cortisol from GSR[7]. This
reinforces the tight coupling of the two measures: the fast SNS-mediated
GSR and the slower HPA-mediated cortisol are successive waves of the
integrated stress response.</p>
<h2 id="gsr-physiology-and-measurement-limitations">2.5 GSR Physiology
and Measurement Limitations</h2>
<p><strong>Physiology of GSR:</strong> Galvanic Skin Response is
grounded in the physiology of sweat glands and skin conductance. The
human skin, especially on the palms and fingers, is densely populated
with <em>eccrine sweat glands</em> (on the order of 2–3 million glands
over the body, with high density on palms, fingers, and soles)[13].
These glands are innervated solely by the sympathetic branch of the
autonomic nervous system. When the sympathetic nervous system activates
(due to emotional arousal, cognitive effort, thermoregulation, etc.), it
triggers these glands to produce sweat – even in the absence of overt
sweating, microscopic changes occur. Sweat is rich in water and
electrolytes; as it fills the ducts and moistens the skin surface, it
alters the electrical properties of the skin. Specifically, the presence
of sweat <em>lowers the skin’s electrical resistance</em> and thus
<em>raises its conductance</em>. GSR refers to measuring this change: a
small voltage or current is applied across two points on the skin, and
the conductance (or its reciprocal, resistance) is recorded.
<strong>Emotional arousal leads to distinctive GSR patterns</strong>:
for instance, a sudden startle or mental stress can cause a sharp
increase in skin conductance – a <em>skin conductance response</em>
(SCR) – superimposed on a slowly shifting baseline level (<em>skin
conductance level</em>, SCL)[13][14].</p>
<p>These patterns are easily observed – even a subtle stimulus like an
exciting image or a deep breath can produce a visible deflection in a
high-resolution GSR signal. Because these changes are not under
conscious control (one cannot easily suppress or fake them), GSR is
regarded as a pure measure of <em>autonomic arousal</em>[15].
Physiologically, the mechanism can be summarized as: <strong>emotional
sweating</strong> causes ionic changes that increase skin
conductance[16]. The palmar and plantar surfaces (hands and feet) are
most commonly used because they exhibit the largest and most reliable
conductance changes linked to psychological stimuli[13]. (Historically,
this is why the polygraph “lie detector” often measures palm GSR – lying
is presumed to induce a stress response detectable as sweaty palms.) The
GSR signal thus directly reflects sympathetic nervous system activity.
It does not tell us <em>why</em> the SNS is activated – only that it is.
However, in controlled experiments or context-specific applications, GSR
peaks are highly informative. For example, in a stress test, an increase
in GSR correlates with moments of perceived challenge or surprise. In
fear-conditioning research, conditioned stimuli elicit SCRs as an index
of learned fear response.</p>
<p><strong>Measurement Limitations:</strong> Despite its usefulness, GSR
comes with several considerations and limitations:</p>
<ul>
<li><p><strong>Non-specificity of Arousal:</strong> As noted, GSR
measures arousal, not valence or specific emotion. A high GSR could mean
stress or fear, but equally could indicate excitement or surprise.
Context (or additional signals) is needed to interpret the meaning of a
GSR change[12]. Thus, using GSR alone to infer “stress” can be
problematic unless the scenario is well-defined. In this work, the
system pairs GSR with known stressors or user-reported stress to ensure
the GSR changes are indeed stress-related. Machine learning models may
also incorporate other modalities (like facial expression or heart rate)
to help disambiguate the cause of arousal.</p></li>
<li><p><strong>Inter- and Intra-person Variability:</strong> Skin
conductance responses vary widely between individuals, and even within
an individual over time. Some people (so-called <em>non-responders</em>)
exhibit very low GSR reactivity, perhaps due to skin properties or
autonomic differences. Others have high tonic levels or exaggerated
responses. Factors like skin dryness, hydration, and even personality
traits can affect GSR amplitude. Within the same person, factors such as
time of day, skin temperature, and fatigue can change the baseline
conductivity. This variability means that often one must use relative
changes or individual calibration rather than absolute GSR values when
comparing stress levels across people.</p></li>
<li><p><strong>Environmental Factors:</strong> GSR data can be
influenced by the environment. Ambient temperature and humidity affect
how quickly sweat evaporates and the skin’s natural moisture. A hot
environment might raise baseline skin moisture (increasing conductance)
even without psychological arousal; a cold, dry environment might
suppress or delay GSR responses. Similarly, if a person is physically
active (raising body temperature and sweating for thermoregulation), it
can confound the GSR that is supposed to reflect psychological factors.
Researchers must control or at least record these variables. For
instance, maintaining a consistent room temperature and ensuring the
participant is at rest before measurement helps[12]. In the platform,
the system logs environmental conditions and incorporate calibration
periods to establish baseline conductance.</p></li>
<li><p><strong>Motion Artifacts and Contact Issues:</strong> Because GSR
electrodes are usually attached to fingers with gel or straps, movement
can introduce artifacts. Even slight finger movements can change contact
pressure or create electrical noise. Good practice is to secure
electrodes firmly and ask participants to minimise hand movement. Still,
artifact removal algorithms (detecting rapid, implausible spikes) are
often needed. The system addresses this by including an accelerometer
channel from the Shimmer sensor to detect motion that can be used to
flag affected data segments[1]. Contact quality is another issue: if
electrodes dry out or are not well attached, the signal can drift or
drop out. Regularly checking electrode adhesion and using conductive gel
can mitigate this.</p></li>
<li><p><strong>Slow Recovery and Habituation:</strong> After a
significant SCR, it takes time for skin conductance to return to
baseline (on the order of tens of seconds). If stressors occur in rapid
succession, the signals can overlap. Moreover, people habituate –
repeated exposure to the same stimulus yields smaller GSR responses over
time as the novelty wears off. This must be considered in experimental
design: one should allow enough time between stimuli or use analysis
methods (e.g. deconvolving overlapping SCRs) to address this.</p></li>
<li><p><strong>Units and Calibration:</strong> GSR can be reported
either as conductance (often in microsiemens, μS) or resistance
(kilohms). The Shimmer3 GSR+ device, for example, measures skin
resistance in kΩ and can internally convert to conductance[8].
Calibration to absolute units can be tricky because skin conductance has
no fixed zero – even dry skin has some conductance. Many researchers use
relative change (ΔμS) or standard scores. Nonetheless, for the
predictive modeling it is helpful to work in consistent units (the
system uses μS), so the calibration Shimmer output accordingly.</p></li>
</ul>
<p>Despite these limitations, GSR remains one of the most sensitive and
convenient measures of emotional arousal[14]. Its drawbacks can be
managed through careful design and data processing. In the context of
this thesis, GSR provides the ground truth “stress signal” the aim is to
predict using other sensors. the system leverages the Shimmer GSR
sensor’s high resolution (16-bit data at 128 Hz sampling[8]) to capture
fine-grained electrodermal dynamics. At the same time, the
implementation includes strategies like baseline normalization,
synchronisation with other channels, and artifact filtering to ensure
the GSR data is reliable. By acknowledging GSR’s limitations, this
approach – especially the integration of additional modalities like
thermal imaging – is designed to compensate for them (for instance,
using thermal cues to help identify true stress responses versus
environmental sweating).</p>
<h2 id="thermal-cues-of-stress-in-humans">2.6 Thermal Cues of Stress in
Humans</h2>
<p>Beyond “cold sweat” and heart palpitations, stress manifests in
subtle thermal changes on the human body. <strong>Infrared
thermography</strong> provides a way to observe these changes: it
measures the heat emitted from the skin, revealing patterns of blood
flow and perspiration that are invisible to the naked eye. <em>Skin
temperature</em> is a known physiological correlate of autonomic
activity – changes in emotional or mental state can cause measurable
shifts in facial and peripheral skin temperature[10]. Under stress, the
autonomic nervous system alters both <strong>vasomotor tone</strong>
(blood vessel diameter) and <strong>sudomotor activity</strong>
(sweating), each of which has thermal consequences[10][5].
Vasoconstriction in surface vessels tends to <em>cool the skin</em> in
those areas, while increased blood flow (vasodilation) <em>warms the
skin</em>. Meanwhile, evaporative cooling from sweat can locally reduce
skin temperature (hence the term “cold sweat”)[5]. These physiological
adjustments form a complex thermal signature of stress.</p>
<p>Researchers have identified several characteristic thermal cues
associated with acute stress or fear. One of the most replicated
findings is a drop in temperature at the tip of the nose (and sometimes
the cheeks) during a startle or mental stress task[10][5]. The nose,
being highly vascular and exposed, is particularly sensitive to
stress-driven vasoconstriction. In one study with an acute psychological
stressor (Stroop test), the nose was the only facial region whose
temperature changed significantly – <em>specifically, it cooled under
stress</em> compared to baseline[10]. This nose-tip cooling is on the
order of 0.1°C to 0.5°C, detectable with a high-sensitivity thermal
camera. The mechanism is thought to be that under stress, blood is
shunted from the periphery (nose, ears, fingers) to deeper tissues (a
primitive response to prepare for injury or conserve core heat),
combined with possible evaporative cooling from subtle sweat. Another
well-known thermal sign is around the eyes: the periorbital region
(inner eye area and forehead) often <em>warms up</em> during stress.
This is attributed to increased blood flow in the supraorbital region as
part of the fight-or-flight response, and the relative insulation of
that area (less exposed than the nose). Some thermal imaging studies of
fear or startle have observed an increase in temperature around the eyes
simultaneous with a nose temperature drop[10][5]. Essentially, the face
shows a pattern of cooling in some regions and warming in others,
reflecting this redistribution of blood.</p>
<p>Beyond the face, stress can cause cooling of the extremities (hands,
fingers) due to vasoconstriction. Thermographic studies of stress in
hands (e.g. during a public speaking task) have found that fingertip
temperature can decrease when a person is anxious – consistent with the
classic anxiety symptom of “cold, clammy hands.” Thermal cameras have
even been used to detect deception or fear by monitoring facial
temperature changes. For example, one experiment showed that when
subjects were startled or lying, the temperature of the cheeks and
forehead increased (flushing) but the nose temperature dropped markedly
– a pattern dubbed the “Pinocchio effect” in thermal
imaging[10][14].</p>
<p>Importantly, thermal cues of stress are <strong>contactless</strong>
and involuntary, making them attractive for monitoring. A person cannot
easily control their skin blood flow or where they radiate heat.
However, interpreting these cues requires careful analysis because many
factors influence skin temperature. Ambient temperature and airflow can
change absolute skin readings. Physical activity or posture changes can
also alter circulation. Thus, stress-related thermal changes are often
extracted by looking at <em>relative changes</em> in specific regions or
by using algorithms that correlate thermal data with known autonomic
signals. For example, one approach is to track the temperature of the
nose over time and look for sudden drops that coincide with stressors or
high GSR readings. In the platform, the system records thermal video of
the face and aim to derive features (like nose tip temperature or the
gradient between inner eye and nose) that correlate with stress
events.</p>
<p>Recent research has applied advanced analyses to thermal data to
quantify these responses. One study combined thermal imaging with heart
rate variability and GSR, and found through cross-mapping analysis that
facial skin temperature dynamics were significantly coupled with those
autonomic measures under stress[10][5]. They confirmed the
<strong>“well-known decrease in nose temperature”</strong> during acute
stress and linked it quantitatively to both increased electrodermal
activity and shifts in cardiac autonomic balance[5]. Another line of
work involves measuring the thermal signature of breathing: under
stress, breathing patterns can change (often becoming faster or
shallower), and this is detectable as changes in the temperature of
exhaled air around the nostrils. Thermal cameras can capture this by the
cyclical warming and cooling near the nose as one breathes; irregular or
rapid breathing under stress is thus another thermal cue[1]. In fact,
one system by Murthy <em>et al.</em> used a thermal camera to monitor
respiration rate and could classify high vs. low stress levels with good
accuracy based on breathing changes alone[1].</p>
<p>In summary, human stress leaves a <strong>thermal
fingerprint</strong>: a constellation of temperature shifts (nose
cooling, possible forehead warming, peripheral cooling, altered
breathing heat patterns) that can be measured remotely. These changes
are subtle (fractions of a degree) but detectable with modern thermal
sensors that have sensitivities &lt;0.1°C. By leveraging these cues,
thermal imaging offers a unique window into the physiological stress
response. It essentially visualizes some of the same processes that GSR
and heart rate indicate – sympathetic activation and its effects – but
in a 2D spatial manner across the skin. In this thesis, thermal cues
form a crucial part of the multi-modal data. The hypothesis is that by
feeding these thermal features into a predictive model, the detection
and prediction of stress (as reflected in GSR) beyond what traditional
cameras or single-modality sensors could achieve.</p>
<h2
id="rgb-vs.-thermal-imaging-for-stress-detection-machine-learning-hypothesis">2.7
RGB vs. Thermal Imaging for Stress Detection (Machine Learning
Hypothesis)</h2>
<p>Given the capabilities described, an important question arises:
<strong>How does traditional RGB video compare to thermal imaging for
detecting stress, and can combining them improve machine learning
predictions?</strong> This section outlines the rationale and hypothesis
guiding the use of both an RGB camera (visible spectrum) and a thermal
camera.</p>
<p><strong>RGB Video (Visible Light Imaging):</strong> A normal camera
captures facial expressions, body movements, and skin colour changes in
the visible spectrum. These can carry stress information. For instance,
facial expression analysis might detect a furrowed brow or frown
associated with stress or concentration. Skin colour changes – though
minute – can also reveal physiology: a technique known as <strong>remote
photoplethysmography (rPPG)</strong> uses a regular camera to detect
slight pulsatile changes in skin colouration due to blood flow. From
rPPG, one can derive heart rate and heart rate variability, which are
known stress correlates (e.g. stress typically elevates heart rate and
reduces HRV). Indeed, <strong>Cho et al. (2019)</strong> combined a
smartphone’s RGB camera (for rPPG) with a thermal camera for stress
monitoring, achieving about <strong>78.3% accuracy</strong> in binary
stress classification[1]. RGB cameras are also high resolution and
capture identity and context – e.g. who the person is, what their
posture is, and environmental context (are they at a computer, in
traffic, etc.). This contextual information could indirectly help
predict stress (for example, seeing that someone is in a noisy crowd
vs. a quiet room). Crucially, RGB imaging is <em>passive in terms of
physiology</em> – it observes external cues that might be voluntarily
controlled or masked. A person might smile to hide stress or remain
expressionless, and normal video could then miss the internal
turmoil.</p>
<p><strong>Thermal Imaging:</strong> Thermal cameras, as discussed,
directly capture <em>physiological signatures</em> such as skin
temperature distribution and breathing patterns. They do not see facial
expressions in the traditional sense (a smile and a grimace might look
similar in pure temperature terms if muscle movements do not alter blood
flow). Instead, they pick up on things like the warmth of blood in the
face, sweat evaporation cooling the skin, and the heat of exhaled air.
Thermal imaging is largely insensitive to lighting conditions and works
in darkness, which is an advantage over RGB when light is limited. It
also sees through certain obscurants like light fog (though not glass),
which is why thermal is used in night vision and surveillance[15]. For
stress detection, the key advantage is that <strong>thermal focuses on
involuntary physiological changes</strong> that a person cannot easily
hide or fake. Even if someone maintains a poker face, a thermal camera
might catch their nose cooling or their breathing becoming rapid. On the
downside, thermal cameras have much lower resolution (the device is
256×192 pixels[Topdon Technology(2024a)], compared to a typical RGB
video of 1920×1080 or higher). They also lack colour or texture
information – everything is a temperature reading – which means they
will not capture certain stress cues like trembling (unless it causes
temperature fluctuation) or facial flushing that does not significantly
change heat emission. Additionally, thermal images of different people
look more similar than RGB images (thermal ignores features like skin
pigment or hair colour), so identifying individuals or analysing facial
expressions is harder.</p>
<p><strong>Hypothesis – Complementary Strengths:</strong> The hypothesis
is that <strong>thermal imaging will provide complementary information
to RGB, leading to better stress (GSR) prediction than RGB
alone</strong>. In other words, a model with access to both visible
facial cues and thermal physiological cues should outperform a model
with only one modality. Thermal can pick up subtle autonomic cues, while
RGB can capture behavioural cues and provide context for alignment.
Prior studies support this idea. For example, in a controlled
experiment, Cho et al. (2019) used a FLIR One thermal camera attached to
a smartphone along with the phone’s regular camera to classify mental
stress. By analysing the nose-tip temperature from thermal and the blood
volume pulse from the RGB camera, they achieved ~<strong>78%
accuracy</strong>, comparable to state-of-the-art methods with much more
equipment[1]. This shows that combining thermal and visual physiological
signals is feasible and effective. In another study, <strong>Basu et
al. (2020)</strong> fused features from thermal and visible facial
images to recognise emotional states, using a blood perfusion model on
the thermal data. The fused model reached <strong>87.9%
accuracy</strong>, significantly higher than using visible images
alone[1]. Such results suggest that thermal data adds discriminative
power. Researchers have noted that thermal imaging can capture
stress-related changes non-intrusively and is a promising solution for
affective computing[16]. Moreover, unlike purely vision-based methods on
RGB (which often rely on facial expressions that can be deliberately
controlled), thermal provides a more objective measure of inner
state[1].</p>
<p><strong>Considerations:</strong> There are practical considerations
in using both modalities. Aligning thermal and RGB images is
non-trivial, since they are different spectra and resolutions –
calibration and software image registration are needed. The system
tackles this with calibration procedures (e.g. using an Android
calibration routine and OpenCV) to align the two camera views[1]. There
is also the issue of data volume: combining two video streams increases
data size and model complexity. However, modern deep learning methods
and precise time synchronisation make this manageable. The design
includes a synchronisation engine that timestamps frames from both the
RGB and thermal cameras to within 1 ms, ensuring data streams can be
fused accurately in time[1].</p>
<p>We also hypothesize that <strong>under certain conditions thermal may
outperform RGB alone for stress detection</strong>. For instance, in
darkness or when a person maintains a neutral expression, an RGB-based
approach might fail, while thermal would still catch physiological
changes. Conversely, in scenarios where stress is primarily manifest in
behaviour (e.g. fidgeting or facial grimaces) but physiological changes
are subtle, RGB might contribute more. Thus, using both modalities
provides broader coverage. Our machine learning models can learn to
weigh features from each modality – potentially finding that, say, a
slight nose temperature drop combined with a forced smile is a strong
indicator of stress, whereas either alone might be ambiguous.</p>
<p>In summary, <strong>RGB vs. Thermal is not an either/or proposition
but a complementary one</strong>. We expect thermal imaging to reveal
the <em>involuntary thermal signatures of stress</em> while RGB provides
the <em>contextual and behavioral cues</em>. The platform collects both
synchronously, and the hypothesis is that using both in a predictive
model will yield the best results for predicting GSR (as a proxy of
stress). This approach aligns with the trend in affective computing to
use <strong>multimodal data</strong> – leveraging multiple sensor types
to capture the multifaceted nature of human emotions.</p>
<h2
id="sensor-device-selection-rationale-shimmer-gsr-sensor-and-topdon-thermal-camera">2.8
Sensor Device Selection Rationale (Shimmer GSR Sensor and Topdon Thermal
Camera)</h2>
<p>To implement the multi-modal platform described, hardware components
were carefully selected that balance <strong>signal quality</strong>,
<strong>integration capability</strong>, and <strong>practical
considerations</strong>. In particular, the design chose the
<strong>Shimmer3 GSR+</strong> wearable sensor for electrodermal
activity measurement and the <strong>Topdon TC-series</strong> thermal
camera for infrared imaging, alongside a standard smartphone camera for
RGB video. This section explains why these devices were chosen over
alternatives, and how their characteristics support the system’s
goals.</p>
<p><strong>Shimmer3 GSR+ Sensor:</strong> The Shimmer3 GSR+ is a
research-grade wireless sensor designed specifically for capturing GSR
(EDA) along with other signals like photoplethysmography (PPG) and
motion. Several key factors motivated this choice:</p>
<ul>
<li><p><em>High-Quality GSR Data:</em> The Shimmer GSR+ provides a high
sampling rate and resolution for GSR. It samples at <strong>128 Hz with
16-bit resolution</strong> on the GSR channel[8], which is well above
the minimum needed to capture fast SCR dynamics. The wide measurement
range (10 kΩ to 4.7 MΩ skin resistance) covers the full spectrum of
likely skin conductance values[8]. This ensures that both very small
responses and large sweats are recorded without clipping. Many cheaper
GSR devices (e.g. those in fitness wearables) sample at lower rates or
with 8–10 bit ADCs, potentially missing subtle features. Shimmer’s data
quality is evidenced by its common use in academic research and
validation studies.</p></li>
<li><p><em>Multi-Channel Capability:</em> Although GSR is the primary
interest, the Shimmer3 GSR+ includes additional sensing channels –
notably a PPG channel (for heart rate) sampled at 128 Hz, and an
inertial sensor package (accelerometer, gyroscope, etc.)[17]. These
extra channels add value: the PPG can be used to derive heart rate and
heart rate variability, providing another stress indicator alongside
GSR[17]. The accelerometer/gyro can be used to detect motion artifacts
or estimate activity level. Instead of needing separate devices for
these signals, the Shimmer offers them in one unit, time-synchronised.
In the implementation, the accelerometer is enabled to log motion, which
helps in data cleaning (e.g. if a participant moves suddenly and a GSR
spike occurs, motion can be attributed as the cause). Having all these
streams time-aligned from one device simplifies data
integration.</p></li>
<li><p><em>Bluetooth Wireless Connectivity:</em> The Shimmer connects
via Bluetooth, transmitting data in real time to a host (PC or
smartphone). This wireless operation was crucial for the use case – it
allows the participant to move naturally without being tethered, and it
enables the sensor data to be synchronised easily with other mobile
devices (like an Android phone running the cameras). The Shimmer’s
Bluetooth interface is supported by an official API. In the system
architecture, a <strong>Shimmer Manager</strong> module on the PC (and
optionally on Android) handles connecting to the Shimmer and streaming
its data[17]. We enabled the Bluetooth interface to integrate Shimmer
data seamlessly into the multi-device recording sessions. The
alternative, a wired GSR device, would limit movement and complicate
simultaneous recording with cameras.</p></li>
<li><p><em>Open SDK and Integration:</em> Shimmer provides open-source
APIs (for Java/Android and for Python/C++) which allowed us to integrate
the sensor without reverse-engineering proprietary formats. the system
leveragesd the <strong>Shimmer Java Android API</strong> on the mobile
side and a PyShimmer library on the PC side[15]. This saved significant
development time. For example, the Android app includes a
<code>ShimmerRecorder</code> component that interfaces with the Shimmer
over Bluetooth and streams data into the recording session[17]. The PC
controller includes a <code>ShimmerManager</code> that can manage
multiple Shimmer devices and coordinate their data with incoming camera
data[17]. Using the official Shimmer libraries (developed by Shimmer’s
engineers) proved more reliable than trying to use a generic BLE
interface or a custom-built GSR device.</p></li>
<li><p><em>Validated Performance:</em> The Shimmer3 GSR+ has been
validated in prior studies, which gave us confidence in its accuracy.
Its measurement technique (constant voltage across two electrodes to
measure skin resistance) and internal calibration are documented in the
literature, meaning the results can be compared with other research
using Shimmer. This is preferable to using a novel or untested GSR
device where independent validation of outputs would be required.
Additionally, the Shimmer is safe and comfortable (it uses very low
excitation currents for GSR to avoid any sensation). Given that
participants might wear it for extended sessions, a well-designed,
lightweight (~22 g) device is important[8].</p></li>
<li><p><em>Alternatives Considered:</em> We considered devices like the
<strong>Empatica E4</strong> wristband, which measures GSR, PPG, and
motion. While the E4 is convenient (worn on the wrist), it has a much
lower GSR sampling rate (~4 Hz) and provides only processed,
cloud-synced GSR data, making real-time integration difficult. Other
custom-built options (e.g. Arduino-based GSR sensors) lacked the
precision and would have required solving wireless and data sync
challenges ourselves. Given these trade-offs, Shimmer was the clear
choice for high-quality data and integration capabilities.</p></li>
</ul>
<p><strong>Topdon Thermal Camera (TC Series):</strong> For the thermal
imaging component, the selection was made a <strong>Topdon</strong> USB
thermal camera (specifically the <em>Topdon TC001</em> model, a
smartphone-compatible IR camera) over other thermal camera options.
Several reasons justify this:</p>
<ul>
<li><p><em>Smartphone Integration:</em> The Topdon camera is designed to
plug into an Android smartphone via USB-C and comes with an Android SDK.
This aligns perfectly with the system architecture: the design wanted
the thermal camera to be part of a mobile setup, leveraged by an Android
app. Using a smartphone-based thermal camera means the system can use
the phone’s processing power to handle image capture (and even some
preliminary processing), and it simplifies participant setup (just
attach the small camera to the phone). In contrast, many high-end
thermal cameras (e.g. FLIR A65 or T-series) are standalone devices
requiring a PC connection (via Ethernet/USB) and a dedicated power
source – not portable for the needs. The Topdon essentially turns the
phone into a thermal imaging device.</p></li>
<li><p><em>Resolution and Frame Rate:</em> The Topdon TC camera offers a
<strong>thermal sensor resolution of 256×192 pixels</strong> with a
frame rate up to 25 Hz[18]. This is a higher resolution than older
consumer thermal cameras like the FLIR One (160×120) or Seek Thermal
(206×156). While still lower than expensive scientific cameras (which
can be 640×480 or more), 256×192 provides sufficient detail for facial
thermal analysis – one can discern features like the forehead, eyes,
nose, etc. in the thermogram. The 25 Hz frame rate is near-video rate,
which allows capturing dynamic changes and aligning frames reasonably
well with the 30 FPS RGB video. Our <code>ThermalRecorder</code> module
fixes the thermal camera to 25 FPS, which proved to be a good balance
between temporal resolution and data size[Topdon Technology(2024a)].
(Many lower-cost thermal devices cap at 9 Hz due to export regulations,
but Topdon has clearance for 25 Hz – a big plus for smooth signal
monitoring.)</p></li>
<li><p><em>Radiometric Data Access:</em> Importantly, the Topdon SDK
provides <strong>radiometric data</strong> – meaning the system can
retrieve the actual temperature reading for each pixel, not just a
false-colour image. In the implementation, the system configures the
camera to output both the thermal image and a temperature matrix for
each frame[9][19]. The <code>ThermalRecorder</code> splits the incoming
frame bytes into an image buffer and a temperature buffer, so the system
records a raw thermal matrix (with calibrated temperature values per
pixel) alongside the visual representation[19]. This quantitative data
is crucial for analysis (the system can measure, say, that the nose is
at 33.1 °C and dropped to 32.5 °C). Some consumer cameras only give a
colour-mapped thermal image without easy access to raw values, but
Topdon’s software allows full access. Having the <strong>image +
temperature mode</strong> enabled[Topdon Technology(2024a)] means the
dataset contains pixel-level temperature time series, which is ideal for
training machine learning models to pick up subtle variations.</p></li>
<li><p><em>Cost and Availability:</em> The Topdon camera is relatively
affordable (on the order of a few hundred USD) and commercially
available. This made it feasible to acquire and deploy for this project.
High-end scientific thermal cameras like the FLIR A65 can cost an order
of magnitude more and are far less portable. We needed a device that a
small research lab’s budget could accommodate, potentially even multiple
units for multi-subject data collection. Additionally, using a widely
available consumer device aligns with the vision of future applications
– if stress can be inferred via a camera that any modern smartphone can
host, it increases real-world applicability. Topdon, as a newer entrant
in the thermal market, provided a sweet spot of performance and cost
that matched the requirements (evaluation was conducted the FLIR One
Pro, but its lower resolution and some SDK limitations made Topdon more
attractive).</p></li>
<li><p><em>SDK and Support:</em> The Topdon came with an
<strong>InfiSense IRUVC SDK</strong> (as seen in the code imports like
<code>com.infisense.iruvc.*</code>)[2], which was crucial for rapid
integration. Through this SDK, the system controls camera settings
(emissivity, temperature range, etc.) and handle USB permissions and
streaming in the Android app[5]. The SDK supports pulling frames via a
callback – the system uses an <code>IFrameCallback</code> interface to
get each frame’s byte data in real time[6]. Without such SDK support,
integrating a raw thermal feed into the app would have been
prohibitively difficult (some other cameras have only PC drivers). We
also considered devices like the FLIR One Pro; while FLIR has an SDK, it
is more restrictive and sometimes requires licensing. The
Topdon/Infisense SDK was straightforward and had no licensing
roadblocks. Our <code>ThermalRecorder</code> class was built around this
SDK and runs stable recordings, including tasks like requesting USB
permission from the user and handling device attach/detach events at
runtime[7][20].</p></li>
<li><p><em>synchronisation and System Fit:</em> By using the Topdon with
an Android phone, the system leverages the phone’s internal clock to
timestamp frames. The PC controller and phone are synchronised via
Network Time Protocol (NTP) to ensure all data streams (GSR, thermal
frames, RGB frames) can be aligned post-hoc with sub-millisecond
precision[17]. When connected to the PC, the phone streams timestamped
thermal data in real time via a WebSocket. This distributed architecture
(a PC plus one or more Android devices) was designed with this hardware
setup in mind[17]. The PC acts as a master coordinating multiple Android
units (each potentially running a Topdon and phone camera). The
<em>star-mesh topology</em> of the system means each Android device is
relatively self-contained in sensing capability[17]. The Topdon
fulfilled the role of giving each Android node a powerful sensing
modality (thermal) with minimal additional hardware (just a tiny camera
module on the phone). The devices also work well together in practice:
both are small and non-invasive, allowing a participant to be recorded
in a natural posture (the Shimmer sensor is typically worn on the wrist
or arm with leads to the fingers, and the Topdon camera is lightweight
and attached to the phone near the face). Data from both are streamed
live, and the software can inject synchronisation signals if needed –
for example, the PC can send a command to flash the phone screen or
toggle an LED as a sync marker, and log that event in both data
streams[21].</p></li>
</ul>
<hr />
<h2 id="references-1">References</h2>
<p>See <a href="references.md">centralized references</a> for all
citations used throughout this thesis.</p>
<h1 id="chapter-3-requirements">Chapter 3: Requirements</h1>
<h2 id="problem-statement-and-research-context">3.1 Problem Statement
and Research Context</h2>
<p>The system is developed to support <strong>contactless Galvanic Skin
Response (GSR) prediction research</strong>. Traditional GSR measurement
requires contact sensors attached to a person’s skin, but this project
aims to bridge <strong>contact-based and contact-free physiological
monitoring</strong>. The system allows researchers to collect
<strong>synchronised multi-modal data</strong> by combining
<strong>wearable GSR readings</strong> with <strong>contactless
signals</strong> such as thermal imagery and video. This data supports
developing models to predict GSR without direct skin contact. This
addresses a key research gap by providing a reliable way to acquire
<strong>ground-truth GSR data</strong> alongside contactless sensor
inputs, with all data streams synchronised for analysis[1].</p>
<p>In the context of physiological and affective computing research, the
focus is on <strong>stress and emotion analysis</strong>, where GSR is a
common measure of sympathetic nervous system activity. The system
integrates <strong>thermal cameras, RGB video, and inertial
sensors</strong> with GSR to build a rich dataset for exploring how
observable signals (such as facial thermal patterns or motion) correlate
with changes in skin conductance. This <strong>multi-sensor
platform</strong> operates in real-world settings (e.g. labs or field
studies) and emphasises <strong>temporal precision</strong> and data
integrity, so that subtle physiological responses are captured and can
be later aligned for machine learning[1]. Overall, the system’s goal is
to enable experiments that simultaneously record a participant’s
physiological responses alongside visual and thermal cues, laying a
foundation for contactless stress detection.</p>
<h2 id="requirements-engineering-approach">3.2 Requirements Engineering
Approach</h2>
<p>The system’s requirements were derived with an <strong>iterative,
research-driven approach</strong> [12]. Initially, high-level objectives
(e.g. <em>“enable synchronised GSR and video recording”</em>) were
identified from the research goals. These were then refined through
<em>requirements elicitation</em> involving the researchers’ needs and
hardware constraints. The project adopted a <strong>prototyping</strong>
methodology: early versions of the system were built and tested, and
feedback was used to update the requirements. For example, during
development, additional needs like <strong>data encryption</strong> and
<strong>device fault tolerance</strong> emerged and were added to the
requirements (as evident from commit history introducing security checks
and recovery features).</p>
<p>Requirements engineering was performed in alignment with IEEE
guidelines [11]. Each requirement was documented with a unique ID and
categorised (functional vs. non-functional). The implementation closely
tracked the requirements – the repository structure and commit messages
show that whenever a new capability was implemented (e.g. a
<strong>calibration module</strong> or <strong>time synchronisation
service</strong>), it corresponded to a defined requirement.
Traceability was maintained through the comprehensive matrix presented
in Section 3.6. Overall, the approach was <strong>incremental</strong>
and user-focused: starting from the core research use cases and
continuously refining the system requirements as technical insights were
gained during development.</p>
<h2 id="functional-requirements-overview">3.3 Functional Requirements
Overview</h2>
<p>The system’s functional requirements are listed below (FR#). Each
requirement is stated in terms of what the system <strong>shall</strong>
do. These requirements were derived from the system’s implemented
capabilities and verified via the source code:</p>
<ul>
<li><p><strong>FR1: Multi-Device Sensor Integration</strong> -- The
system shall support connecting and managing multiple sensor devices
simultaneously. This includes <strong>discovering and pairing Shimmer
GSR sensors</strong> via direct Bluetooth or through an Android device
acting as a bridge. If no real sensors are available, the system shall
offer a <strong>simulation mode</strong> to generate dummy sensor data
for testing ]4].</p></li>
<li><p><strong>FR2: Synchronised Multi-Modal Recording</strong> -- The
system shall start and stop data recording
<strong>synchronously</strong> across all connected devices. When a
session starts, the PC instructs each Android device to begin recording
<strong>GSR data</strong>, <strong>video (RGB camera)</strong>, and
<strong>thermal imaging</strong> in parallel ]5]. At the same time, the
PC begins logging data from any directly connected Shimmer sensors. All
streams share a common session timestamp to enable later
alignment.</p></li>
<li><p><strong>FR3: Time Synchronisation Service</strong> -- The system
shall synchronise clocks across devices to ensure all data is
time-aligned. The PC runs a time synchronisation service (e.g. an
NTP-like server on the local network) so that each Android device can
calibrate its clock to the PC’s clock before and during recording ]6].
This achieves a sub-millisecond timestamp accuracy between GSR readings
and video frames, which is crucial for data integrity.</p></li>
<li><p><strong>FR4: Session Management</strong> -- The system shall
organise recordings into sessions, each with a unique ID or name. It
shall allow the researcher to <strong>create a new session</strong>
(automatically timestamped) and later <strong>terminate the
session</strong> when finished. Upon session start, the PC creates a
directory for data storage and initialises a session metadata file ]7].
When the session ends, the metadata (start/end time, duration, status)
is finalised and saved ]7]. Only one session can be active at a time,
preventing overlap.</p></li>
<li><p><strong>FR5: Data Recording and Storage</strong> -- For each
session, the system shall record: (a) <strong>Physiological sensor
data</strong> from the Shimmer GSR module (including GSR and any other
channels such as PPG or accelerometer, sampled at 128 Hz) ]4], and (b)
<strong>video and thermal data</strong> from each Android device (with
at least 1920×1080 video at 30 FPS) ]8]. Sensor readings stream to the
PC in real time and are written to local CSV files as they arrive to
avoid data loss ]4]. Each Android device stores its own raw
video/thermal files locally during recording and transfers them to the
PC after the session (see FR10). The system shall also handle audio
recording if enabled (e.g. microphone audio at 44.1 kHz), keeping it
synchronised with the other streams ]8].</p></li>
<li><p><strong>FR6: User Interface for Monitoring &amp; Control</strong>
-- The system shall provide a GUI on the PC for the researcher to
control sessions and monitor devices. This interface should list
connected devices and their status (e.g. battery level,
streaming/recording state) ]5], allow the user to start/stop sessions,
and display indicators like recording timers and sample counts. The GUI
should also show preview feeds or periodic status updates (for example,
updating every few seconds with the number of samples received) ]5]. If
a device disconnects or encounters an error, the UI should highlight
that device so the researcher can take appropriate action ]5].</p></li>
<li><p><strong>FR7: Device Synchronisation and Signals</strong> -- The
system shall coordinate multiple devices by sending control commands and
synchronisation cues. For example, the PC can broadcast a
synchronisation signal (such as a screen flash or buzzer) to all Android
devices to mark the same moment across all video recordings ]5]. These
signals (e.g. a visual flash on each device’s screen) help with aligning
footage during analysis. The system uses a JSON-based command protocol
so that the PC can instruct all devices to start/stop recording and
perform other actions in unison.</p></li>
<li><p><strong>FR8: Fault Tolerance and Recovery</strong> -- If a device
(Android or sensor) disconnects or fails during a session, the system
shall detect the issue and continue the session with the remaining
devices ]5]. The PC will log a warning and mark that device as offline
]9]. When the device reconnects, it should seamlessly rejoin the ongoing
session. The system will attempt to <strong>recover</strong> the
device’s session state by re-synchronising and executing any commands
that were queued while it was offline ]9]. This ensures a temporary
network drop does not invalidate the entire session.</p></li>
<li><p><strong>FR9: Calibration Utilities</strong> -- The system shall
include tools for <strong>calibrating sensors and cameras</strong>. In
particular, it provides a procedure to align the thermal camera’s field
of view with the RGB camera (e.g. using a checkerboard pattern). The
researcher can perform a calibration session where images are captured
and calibration parameters are computed. Calibration settings (such as
pattern type, pattern size, and number of images) are adjustable in the
system configuration ]8]. The resulting calibration data is saved so
that recorded thermal and visual data can be accurately merged during
analysis. <em>(This requirement is derived from the presence of a
calibration module in the code.)</em></p></li>
<li><p><strong>FR10: Data Transfer and Aggregation</strong> -- Once a
session is stopped, the system shall transfer all recorded data from
each Android device to the PC. The Android application packages the
session’s files (e.g. video, thermal images, local sensor logs) and
sends them to the PC over the network ]10]. The PC saves each incoming
file in the session folder and updates the session metadata with that
file’s entry (including file type and size) ]7]. This automation ensures
the researcher can retrieve all data without manually offloading
devices. If any file fails to transfer, the system logs an error and (if
possible) retries, so that data is not lost ]10].</p></li>
</ul>
<p><em>(The above functional requirements are realised in various
components of the code. For instance, connecting multiple devices and
recording them simultaneously is handled by the</em>
<code>ShimmerPCApplication.start_session</code> <em>logic ]5], and
session metadata management is implemented in the</em>
<code>SessionManager</code> <em>class ]7].)</em></p>
<h2 id="non-functional-requirements">3.4 Non-Functional
Requirements</h2>
<p>In addition to core functionality, the system must meet several
non-functional requirements to ensure it is suitable for a research
setting. These include:</p>
<ul>
<li><p><strong>NFR1: Performance (Real-Time Data Handling)</strong> --
The system must handle data in real time, with minimal latency and
sufficient throughput. It should support at least
<strong>128 Hz</strong> sensor sampling and <strong>30 FPS</strong>
video recording concurrently without data loss or buffering issues. The
design uses multi-threading and asynchronous processing to achieve this.
Video is recorded at ~5 Mbps and audio at 128 kbps, which the system
writes to storage in real time ]8]. Even with multiple devices (e.g. 3+
cameras and a GSR sensor), the system should not drop frames or samples
due to performance bottlenecks.</p></li>
<li><p><strong>NFR2: Temporal Accuracy</strong> -- Clock synchronisation
accuracy between devices should be on the order of milliseconds or
better. The system’s built-in NTP time server and sync protocol aim to
keep timestamp differences very low (e.g. <strong>&lt;5 ms</strong>
offset and jitter as logged after synchronisation) ]6]. This is critical
for valid sensor fusion; therefore, the system continuously synchronises
all device clocks during a session. Timestamp precision is maintained in
all logs (to the millisecond), and all devices use the PC’s clock as the
reference.</p></li>
<li><p><strong>NFR3: Reliability and Fault Tolerance</strong> -- The
system must be robust to interruptions. If a sensor or network link
fails, the rest of the system continues recording unaffected (as per
FR8). Data already recorded should be safely preserved even if a session
ends unexpectedly (e.g. the PC application crashes). The session design
ensures that files are written incrementally and closed properly on
stop, to avoid corruption. A <strong>recovery mechanism</strong> is in
place to handle device reconnections (queuing messages while a device is
offline) ]9]. In addition, the Shimmer device interface includes an
auto-reconnect feature to attempt re-establishing Bluetooth connections
automatically ]4].</p></li>
<li><p><strong>NFR4: Data Integrity and Validation</strong> -- All
recorded data should be accurate and free from corruption. The system
has a data validation mode for sensor data ]4] that checks incoming
values are within expected ranges (for example, verifying GSR readings
stay between 0.0 and 100.0 μS) ]4]. Each file transfer from devices is
verified for completeness (expected file sizes are known and logged in
metadata) ]7]. Session metadata acts as a manifest so that missing or
inconsistent files can be detected easily. Also, the system will not
overwrite existing session data – each session is stored in a unique
timestamped folder to avoid conflicts.</p></li>
<li><p><strong>NFR5: Security</strong> -- The system must safeguard the
security and privacy of the recorded data. All network communication
between the PC and the Android devices is encrypted (TLS is enabled in
the configuration) ]8]]11]. The system requires authentication tokens
for device connections (with a configurable minimum length of 32
characters) to prevent unauthorised devices from joining the session
]11]]8]. Security checks at startup will warn if encryption or
authentication is not properly configured ]11]. All recorded data files
are stored locally on the PC; if cloud or external transfer is needed,
it is performed only by the researcher (there is no inadvertent data
upload). Additionally, the system checks file permissions and the
runtime environment on startup to avoid insecure defaults ]11].</p></li>
<li><p><strong>NFR6: Usability</strong> -- The system should be easy to
use for researchers who are not software experts. The PC’s graphical
interface is designed to be intuitive, with clear controls to start/stop
sessions and indicators for system status. For example, the UI shows a
recording indicator when a session is active and displays device
statuses (connected/disconnected, recording, battery level) in real time
]5]. Sensible default settings (e.g. a default dark theme and window
size) ensure a good user experience out of the box ]8]. The Android app
requires minimal user interaction after initial setup — typically the
researcher just needs to mount the devices and tap “Connect”, with the
PC orchestrating the rest. User manuals or on-screen guidance are
provided for tasks like calibration.</p></li>
<li><p><strong>NFR7: Scalability</strong> -- The system’s architecture
should scale to accommodate multiple devices and long recording
durations. It has been tested with up to <em>8</em> Android devices
streaming or recording concurrently (the configuration allows up to 10
connections) ]8]. Similarly, the system supports sessions up to at least
<em>120 minutes</em> in duration by default ]8]. To manage large video
files, recordings can be chunked into ~1 GB segments automatically so
that individual file sizes remain manageable ]8]. This ensures that even
high-resolution, extended sessions do not overwhelm the file system or
hinder post-processing.</p></li>
<li><p><strong>NFR8: Maintainability and Modularity</strong> -- The
system is built in a modular way to simplify maintenance. Components are
separated (e.g., <strong>Calibration Manager</strong>, <strong>Session
Manager</strong>, <strong>Shimmer Manager</strong>, <strong>Network
Server</strong>) and communicate via clear interfaces. This modular
design (evident in the repository structure) makes it easier to update
one part (such as swapping out the thermal camera SDK) without affecting
others. Configuration is externalised (through <code>config.json</code>
and other settings) ]8], so that changes in requirements (e.g., adding
new sensor types or changing sampling rates) can be accommodated by
editing configuration rather than modifying code. Finally, the project
includes test scripts and extensive logging to aid in debugging, which
contributes to maintainability.</p></li>
</ul>
<p><em>(The non-functional requirements were validated through system
testing and by reviewing configuration parameters. For example, the
presence of TLS settings and runtime security checks ]11] shows that the
security requirements were met, and the multi-threaded design and
resource limits set in the config file ]8] indicate that performance
requirements were satisfied.)</em></p>
<h2 id="use-case-scenarios">3.5 Use Case Scenarios</h2>
<p>To illustrate how the system is intended to be used, this section
describes several <strong>use case scenarios</strong>. Each scenario
outlines the typical interaction between the <strong>user
(researcher)</strong> and the system, along with how the system’s
components work together to fulfil the requirements.</p>
<h3 id="use-case-1-conducting-a-multi-modal-recording-session">Use Case
1: Conducting a Multi-Modal Recording Session</h3>
<p><strong>Description:</strong> A researcher initiates and completes a
recording session capturing GSR data alongside video and thermal streams
from multiple devices. This is the primary use of the system,
corresponding to a live experiment with a participant.</p>
<ul>
<li><p><strong>Primary Actor:</strong> Researcher (system
operator).</p></li>
<li><p><strong>Secondary Actors:</strong> Participant (subject being
recorded), though they do not directly interact with the system
UI.</p></li>
<li><p><strong>Preconditions:</strong></p></li>
<li><p>The Shimmer GSR sensor is charged and either connected to the PC
(via Bluetooth dongle) or paired with an Android device.</p></li>
<li><p>Android recording devices are powered on, running the recording
app, and on the same network as the PC. The PC application is running
and all devices have synchronised their clocks (either via initial NTP
sync or prior calibration).</p></li>
<li><p>The researcher has configured any necessary settings (e.g. chosen
a session name, verified camera focus, etc.).</p></li>
<li><p><strong>Main Flow:</strong></p></li>
<li><p>The Researcher opens the PC control interface and <strong>creates
a new session</strong> (providing a session name or accepting a
default). The system validates the name and creates a session folder and
metadata file on disk]7]. The session is now “active” but not recording
yet.</p></li>
<li><p>The Researcher selects the devices to use. For example, they
ensure the Shimmer sensor appears in the device list and one or more
Android devices show as “connected” in the UI. If the Shimmer is not yet
connected, the Researcher clicks “Scan for Devices.” The system performs
a scan: it finds the Shimmer sensor either directly via Bluetooth or
through an Android’s paired devices ]4]. The Researcher then clicks
“Connect” for the Shimmer. The system establishes a connection (or uses
a simulated device if the real sensor is unavailable) and updates the UI
status to “Connected.”</p></li>
<li><p>The Researcher checks that video previews from each Android (if
available) are showing in the UI (small preview panels) and that the GSR
signal is streaming (e.g. a live plot or at least a sample counter
incrementing). Internally, the PC has started a background thread
receiving data from the Shimmer sensor continuously ]5]. The system also
maintains a heartbeat to each Android (pinging every few seconds) to
ensure connectivity.</p></li>
<li><p>The Researcher initiates recording by clicking “Start Recording.”
The PC sends a <strong>start command</strong> to all connected Android
devices (with a session ID). Each Android begins recording its camera
(and thermal sensor, if present) and optionally starts streaming its own
sensor data (if any) back to PC ]5]. Simultaneously, the PC instructs
the Shimmer Manager to start logging data to file ]5]. This is done
nearly simultaneously for all devices. The PC’s session manager marks
the session status as “recording” and timestamps the start
time.</p></li>
<li><p>During the recording, the Researcher can observe real-time
status. For example, the UI might display the <strong>elapsed
time</strong>, the number of data samples received so far, and the count
of connected devices. Every 30 seconds, the system logs a status summary
(e.g., “Status: 1 Android, 1 Shimmer, 3000 samples”) ]5]. If the
Researcher has a specific event to mark, they can trigger a sync signal:
for instance, pressing a “Flash Sync” button. When pressed, the system
calls <code>send_sync_signal</code> to all Androids to flash their
screen LEDs ]5] (creating a visible marker in the videos) and logs the
event in the GSR data stream.</p></li>
<li><p>If any device disconnects mid-session (e.g. an Android phone’s
WiFi drops out), the system warns the Researcher via the UI (perhaps
highlighting that device in red). The recording on that device might
continue offline (the Android app will still save its local video). The
PC’s Session Synchroniser marks the device as offline and queues any
commands for it ]9]. The Researcher can continue the session if the
other streams are still running. When the disconnected device comes back
online (e.g. WiFi reconnects), the system automatically detects it,
re-synchronises the session state, and executes any missed commands (all
in the background) ]9]. This recovery happens without user intervention,
ensuring the session can proceed.</p></li>
<li><p>The Researcher decides to end the recording after, say,
15 minutes. They click “Stop Recording” on the PC interface. The PC
sends stop commands to all Androids, which then cease recording their
cameras ]5]. The Shimmer Manager stops logging GSR data at the same time
]5]. Each component flushes and closes its output files. The session
manager marks the session as completed, calculates the duration, and
updates the session metadata (end time, duration, and status) ]7]. A log
message confirms the session has ended along with its duration and
sample count stats ]5].</p></li>
<li><p>After stopping, the system <strong>automatically initiates data
transfer</strong> from the Android devices. The File Transfer Manager on
each Android packages the recorded files (e.g.,
<code>video_20250807_...mp4</code>, thermal data, etc.) and begins
sending them to the PC, one file at a time ]10]. The PC receives each
file (via its network server) and saves it into the session folder,
simultaneously calling <code>SessionManager.add_file_to_session()</code>
to record the file’s name and size in the metadata ]7]. A progress
indicator may be shown to the Researcher.</p></li>
<li><p>Once all files are transferred, the system notifies the
Researcher that the session data collection is complete (e.g., “Session
15min_stress_test completed – 5 files saved”). The Researcher can then
optionally review summary statistics (the UI might show, for example,
average GSR level, or simply confirm the number of files and total data
size). The session is now closed and all resources are cleaned
up.</p></li>
<li><p><strong>Postconditions:</strong> All recorded data (GSR CSV,
video files, etc.) are safely stored in the PC’s session directory. The
session metadata JSON lists all devices that participated and all files
collected. The system remains running, and the researcher could start a
new session if needed. The participant’s involvement is done, and the
data are ready for analysis (outside the scope of the recording system).
If any device failed to transfer data, the researcher is made aware so
they can retrieve it manually if possible.</p></li>
<li><p><strong>Alternate Flows:</strong><br />
</p>
<ol type="a">
<li><em>No Shimmer available:</em> If the Shimmer sensor is not
connected or malfunctions, the Researcher can still run a session with
just video/thermal. The system will log that no GSR device is present,
and it can operate in a video-only mode (possibly using a
<strong>simulated GSR signal</strong> for demonstration) ]4].<br />
</li>
<li><em>Calibration needed:</em> If this is the first session or devices
have been re-arranged, the Researcher might perform a
<strong>calibration routine</strong> before step 4. In that case, they
would use the Calibration Utility (see Use Case 2) to calibrate cameras.
Once calibration is done and saved, the recording session proceeds as
normal.<br />
</li>
<li><em>Device battery low:</em> During step 5, if an Android’s battery
is critically low, the system could alert the Researcher (since the
device status includes battery level) ]11]. The Researcher might decide
to stop the session early or replace the device. The system will include
the battery status in metadata for transparency.<br />
</li>
<li><em>Network loss at end:</em> If the network connection to a device
is lost exactly when “Stop” is pressed, the PC might not immediately
receive confirmation from that device. In this case, the PC will mark
the device as offline (as in step 6) and proceed to finalise the session
with whatever data it has. Later, when the device reconnects, the
Session Synchroniser can still trigger the file transfer for that
device’s data so it eventually gets saved on the PC.</li>
</ol></li>
</ul>
<h3 id="use-case-2-camera-calibration-for-thermal-alignment">Use Case 2:
Camera Calibration for Thermal Alignment</h3>
<p><strong>Description:</strong> Before conducting recordings that
involve a thermal camera, the researcher performs a calibration
procedure to align the thermal camera’s view with the RGB camera view.
This ensures that data from these two modalities can be compared
pixel-to-pixel in analysis.</p>
<ul>
<li><p><strong>Primary Actor:</strong> Researcher.</p></li>
<li><p><strong>Preconditions:</strong> At least one Android device with
both an RGB and a thermal camera (or an external thermal camera
attached) is available. A calibration pattern (e.g. a black-and-white
checkerboard) is printed and ready. The system’s calibration settings
(pattern size, etc.) are configured if needed ]8].</p></li>
<li><p><strong>Main Flow:</strong></p></li>
<li><p>The Researcher opens the <strong>Calibration Tool</strong> in the
PC application (or on the Android app, depending on implementation –
assume PC-side coordination). They select the device(s) to calibrate
(e.g., “Device A – RGB + Thermal”).</p></li>
<li><p>The system instructs the device to enter calibration mode.
Typically, the Android app might open a special calibration capture
activity (with perhaps an overlay or just using both cameras). The
Researcher holds the checkerboard pattern in front of the cameras and
ensures it is visible to both the RGB and thermal cameras.</p></li>
<li><p>The Researcher initiates capture (maybe pressing a “Capture
Image” button). The device (or PC via the device) captures a pair of
images – one from the RGB camera and one from the thermal camera – at
the same moment. It may need multiple images from different angles; the
configuration might specify capturing, say, 10 images ]8]. The system
gives feedback after each capture (e.g., “Image 1/10
captured”).</p></li>
<li><p>After the required number of calibration images are collected,
the Researcher clicks “Compute Calibration.” The system runs a
calibration algorithm (likely implementing Zhang’s method for camera
calibration) on the collected image pairs. This computes parameters like
camera intrinsics for each camera and the extrinsic transform aligning
thermal to RGB.</p></li>
<li><p>The system stores the resulting calibration parameters (e.g. in a
calibration result file or in config). It also might display an estimate
of calibration error (so the Researcher can judge quality). For
instance, if the reprojection error exceeds the threshold ]8] (say
threshold = 1.0 pixel), the system might warn that the calibration
quality is low.</p></li>
<li><p>The Researcher is satisfied with the calibration (error is
acceptable). They save the calibration profile. Now the system will use
this calibration data in future sessions to correct or align thermal
images to the RGB frame if needed (this might be done in post-processing
rather than during recording). The Researcher exits the calibration
mode.</p></li>
<li><p><strong>Alternate Flows:</strong><br />
</p>
<ol type="a">
<li><em>Calibration failure:</em> If the system cannot detect the
calibration pattern in the images (e.g., poor contrast in thermal
image), it notifies the Researcher. The Researcher can then recapture
images (maybe adjust the pattern distance or lighting) until the system
successfully computes a calibration.<br />
</li>
<li><em>Partial calibration:</em> The Researcher may choose to only
calibrate intrinsics of each camera separately (for example, if
thermal-RGB alignment is less important than ensuring each camera’s lens
distortion is corrected). In this case, the flow would be adjusted to
capturing images of a known grid for each camera independently.<br />
</li>
<li><em>Using stored calibration:</em> If calibration was done
previously, the Researcher might skip this use case entirely and rely on
the stored calibration parameters. The system allows loading a saved
calibration file, which then becomes active for subsequent
recordings.</li>
</ol></li>
</ul>
<p><strong>Use Case 3 (Secondary): Reviewing and Managing Session
Data</strong> (Optional) – <em>This use case would describe how a
researcher can use the system to review past session metadata and
possibly replay or export data.</em> (For brevity, this is not expanded
here, but the system does include features like session listing and
possibly data export tools, given that a web UI template for sessions
exists.)</p>
<p><em>(The above scenarios demonstrate the system’s functionality in
context. They confirm that the requirements – from multi-device
synchronisation to calibration – all serve real user workflows. The
sequence of interactions in Use Case 1, especially, shows how the system
meets the need for synchronised multi-modal data collection in a
practical experiment setting.)</em></p>
<h2 id="system-analysis-architecture-data-flow">3.6 System Analysis
(Architecture &amp; Data Flow)</h2>
<p><strong>System Architecture:</strong> The system adopts a
<strong>distributed architecture</strong> with a central <strong>PC
Controller</strong> and multiple <strong>Mobile Recording
Units</strong>. The PC (a Python desktop application) acts as the
master, coordinating all devices, while each Android device runs a
recording application that functions as a client node. This architecture
is essentially a <strong>hub-and-spoke topology</strong>, where the PC
hub maintains control and timing, and the spokes (sensors/cameras) carry
out data collection.</p>
<p>On the PC side, the software is organised into modular managers, each
responsible for a subset of functionality: – The <strong>Session
Manager</strong> handles the overall session lifecycle (creation,
metadata logging, and closure) ]7]]7]. – The <strong>Network
Server</strong> component (within the <code>AndroidDeviceManager</code>
and <code>PCServer</code> classes) manages communication with Android
devices over TCP/IP (listening on a specified port, e.g. 9000) ]4]]8].
It uses a custom JSON-based protocol for commands and status messages. –
The <strong>Shimmer Manager</strong> deals with the Shimmer GSR sensors,
including Bluetooth connectivity (via the PyShimmer library if
available) and data streaming to the PC ]4]]4]. It also multiplexes data
from multiple sensors and writes sensor data to CSV files in
real-time. – The <strong>Time synchronisation Service</strong> (Master
Clock) runs on the PC to keep device clocks aligned. As seen in the
code, an <code>NTPTimeServer</code> thread on the PC listens on a port
(e.g. 8889) and services time-sync requests from clients ]6]. It
periodically syncs with external NTP sources for accuracy and provides
time offset information to the Android devices, which adjust their local
clocks accordingly. – The <strong>GUI Module</strong> (built with PyQt5
or a similar framework) provides the desktop interface. It includes
panels for device status, session control, and live previews. This GUI
updates based on callbacks and status data from the managers (for
instance, when a new device connects, the Shimmer Manager invokes a
callback that the GUI listens to, so it can display the device).</p>
<p>On the Android side, each device’s application is composed of several
components: – A <strong>Recording Controller</strong> that receives
start/stop commands from the PC and controls the local recording (camera
and sensor capture). – Separate <strong>Recorder modules</strong> for
each modality: e.g., <code>CameraRecorder</code> for RGB video,
<code>ThermalRecorder</code> for thermal imaging, and
<code>ShimmerRecorder</code> if the Android is paired to a Shimmer
sensor ]55]. These recorders interface with hardware (camera APIs, etc.)
and save data to local storage. – A <strong>Network Client</strong> (or
Device Connection Manager) that maintains the socket connection to the
PC’s server. It listens for commands (e.g., start/stop, sync signal) and
sends back status updates or data as needed. – A
<strong>FileTransferManager</strong> on Android, which, after recording,
handles sending the recorded files to the PC upon request ]10]]10]. –
Utility components like a <strong>Security Manager</strong> (ensuring
encryption if TLS is used), a <strong>Storage Manager</strong> (to check
available space and organise files), etc., are also part of the design
(many of these are hinted by the architecture and config files).</p>
<p><strong>Communication and Data Flow:</strong> All communication
between the PC and Android devices uses a <strong>client-server
model</strong>. The PC runs the server (listening on a specified
host/port, with a maximum number of connections defined) ]8], and each
Android client connects to it when ready. Messages are likely encoded in
JSON and could be sent over a persistent TCP socket (the config
specifies <code>protocol: "TCP"</code> ]8]). Important message types
include: device registration/hello, start session command, stop session
command, sync signal command, status update from device, file transfer
requests, etc.</p>
<p>During a session, the <strong>data flow</strong> is as follows: –
<strong>Shimmer GSR Data:</strong> If a Shimmer sensor is directly
connected to the PC, it streams data via Bluetooth to the PC’s Shimmer
Manager, which then immediately enqueues the data for writing to a CSV
and also triggers any real-time displays. If the Shimmer is connected to
an Android (i.e., Android-mediated), the sensor data first goes to the
Android (via Bluetooth), and the Android then forwards each GSR sample
(or batch of samples) over the network to the PC ]4]]4]. This is handled
by the <code>AndroidDeviceManager._on_android_shimmer_data</code>
callback on the PC side, which receives <code>ShimmerDataSample</code>
objects from the device and processes them similarly. In both cases,
each GSR sample is timestamped (using the synchronised clock) and
logged. The PC might accumulate these in memory (e.g., in
<code>data_queues</code>) briefly for processing but ultimately writes
them out via a background file-writing thread ]4]. – <strong>Video and
Thermal Data:</strong> The Android devices record video and thermal
streams locally to their flash storage (to avoid saturating the network
by streaming raw video). The PC may receive low-frequency updates or
thumbnails for monitoring, but the bulk video data stays on the device
until session end. The <strong>temporal synchronisation</strong> of
video with GSR is ensured by all devices starting recording upon the
same start command and using synchronised clocks. Additionally, the PC’s
sync signal (flash) provides a reference point that can be seen in the
video and is logged in the GSR timeline, tying the streams together.
After the recording, when the PC issues the file transfer, the video
files are sent to the PC. This transfer uses the network (possibly
chunking files if large). The FileTransferHandler on PC receives each
chunk or file and saves it. Because the PC knows the session start time
and each video frame’s device timestamp (the Android might embed
timestamp metadata in video or provide a separate timestamp log),
alignment can be done in post-processing. There is also a possibility
that the Android app sends periodic timestamps during recording to the
PC (as part of SessionSynchroniser updates) so the PC is aware of
recording progress ]9]]9]. – <strong>Time Sync and Heartbeats:</strong>
Throughout a session, the PC might send periodic time sync packets to
the Androids (or the Androids request them). The
<code>SessionSynchronizer</code> on PC also keeps a heartbeat: it tracks
if it hasn’t heard from a device’s state in a while, marking it offline
after a threshold ]9]. Android devices likely send a small status
message every few seconds (“I’m alive, recording, file X size = …”).
This data flow ensures the PC has up-to-date knowledge of each device
(e.g., how many frames recorded, or storage used). – <strong>Data
Aggregation:</strong> Once all data reaches the PC, the system has a
<strong>session aggregation step</strong> (which can be considered
post-session). For instance, the Session Manager might invoke a function
to perform any post-processing – the code even shows a hook for
<em>post-session hand segmentation processing</em> on the recorded video
]7]]7]. In practice, after all files are in place, the PC could combine
or index them (for example, generating an index of timestamps). This
ensures that all data from the distributed sources is now centralised in
one place (the PC’s file system) and organised.</p>
<p><strong>System Architecture Diagram:</strong> <em>Figure 3.1
(Placeholder)</em> would illustrate the above in a block diagram: a PC
node on one side with blocks for Session Manager, Shimmer Manager,
Network Server, etc., and multiple Android nodes on the other, each
containing Camera, Thermal, Shimmer (if any) and a network client. Lines
would show Bluetooth links (PC to Shimmer, or Android to Shimmer), and
WiFi/LAN links between PC and each Android. Data flows (like GSR data
flowing to PC, video files flowing after stop) would be indicated with
arrows. Time sync flows (PC broadcasting time) would also be shown. The
diagram would emphasise the star topology (PC in centre).</p>
<p><strong>Key Design Considerations:</strong> The architecture ensures
<strong>scalability</strong> by decoupling data producers (devices) from
the central coordinator. Each Android operates largely independently
during recording (writing to local disk), which avoids overloading the
network. The PC focuses on low-bandwidth critical data (GSR streams,
commands, and occasional thumbnails or status). By using local storage
on devices and transferring after, the system mitigates the risk of
network bandwidth issues affecting the recording quality. The use of
threads and asynchronous I/O on the PC side (for writing files and
handling multiple sockets) ensures that adding more devices will
linearly increase resource usage but not deadlock the system ]4].</p>
<p>The architecture also provides <strong>fault isolation</strong>: if
one device crashes, it does not bring down the whole system – the PC
will continue managing others. The SessionSynchronizer component acts
like a watchdog and queue, so even if connectivity returns after a
lapse, the overall session can still be coherent ]9]]9].</p>
<hr />
<p>]1] <em>bucika_gsr</em> Project Repository (GitHub),
<strong>architecture.md</strong> (architecture and data flow
documentation), commit 7048f7f6, 2024. ]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/architecture.md"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/architecture.md</a><br />
]2] ISO/IEC/IEEE 29148:2018, <em>Systems and Software Engineering – Life
Cycle Processes – Requirements Engineering</em>, 2018.<br />
]3] IEEE 830-1998, <em>IEEE Recommended Practice for Software
Requirements Specifications</em>, 1998.<br />
]4] <em>bucika_gsr</em> Repository (GitHub),
<strong>PythonApp/shimmer_manager.py</strong>, commit 7048f7f6, 2025.
]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py</a><br />
]5] <em>bucika_gsr</em> Repository (GitHub),
<strong>PythonApp/shimmer_pc_app.py</strong>, commit 7048f7f6, 2025.
]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py</a><br />
]6] <em>bucika_gsr</em> Repository (GitHub),
<strong>PythonApp/ntp_time_server.py</strong>, commit 7048f7f6, 2025.
]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/ntp_time_server.py"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/ntp_time_server.py</a><br />
]7] <em>bucika_gsr</em> Repository (GitHub),
<strong>PythonApp/session/session_manager.py</strong>, commit 7048f7f6,
2025. ]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py</a><br />
]8] <em>bucika_gsr</em> Repository (GitHub),
<strong>protocol/config.json</strong>, commit 7048f7f6, 2025. ]Online].
Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/protocol/config.json"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/protocol/config.json</a><br />
]9] <em>bucika_gsr</em> Repository (GitHub),
<strong>PythonApp/session/session_synchronizer.py</strong>, commit
7048f7f6, 2025. ]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py</a><br />
]10] <em>bucika_gsr</em> Repository (GitHub),
<strong>AndroidApp/src/main/java/com/multisensor/recording/managers/FileTransferManager.kt</strong>,
commit 7048f7f6, 2025. ]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/AndroidApp/src/main/java/com/multisensor/recording/managers/FileTransferManager.kt"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/AndroidApp/src/main/java/com/multisensor/recording/managers/FileTransferManager.kt</a><br />
]11] <em>bucika_gsr</em> Repository (GitHub),
<strong>PythonApp/production/runtime_security_checker.py</strong>,
commit 7048f7f6, 2025. ]Online]. Available: <a
href="https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/production/runtime_security_checker.py"
class="uri">https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/production/runtime_security_checker.py</a></p>
<hr />
<h2 id="references-2">References</h2>
<p>See <a href="references.md">centralized references</a> for all
citations used throughout this thesis.</p>
<h1 id="chapter-4-design-and-implementation">Chapter 4: Design and
Implementation</h1>
<h2 id="system-architecture-overview-pcandroid-system-design">4.1 System
Architecture Overview (PC–Android System Design)</h2>
<p>The system is designed in a client–server architecture with a
<strong>central PC controller</strong> coordinating multiple
<strong>Android capture devices</strong>. The PC application serves as
the master controller, discovering and connecting to each Android device
over a local network. Each Android device runs a capture app responsible
for recording sensor data and video, while the PC provides a unified
interface to start/stop recordings and aggregate data.</p>
<figure>
<img src="../../diagrams/fig_4_01_system_architecture_overview.png"
alt="Figure 4.1: System Architecture Overview" />
<figcaption aria-hidden="true">Figure 4.1: System Architecture
Overview</figcaption>
</figure>
<p><em>Figure 4.1: System Architecture Overview showing PC controller
coordinating multiple Android devices</em></p>
<p>The system architecture: the PC communicates with each Android
smartphone via Wi-Fi using a custom TCP/IP protocol, sending control
commands and receiving live telemetry (video previews, sensor readings).
The Android devices operate largely autonomously during capture – each
uses its own high-precision clock to timestamp data locally – but all
devices are synchronised to the PC’s timeline through network time
alignment. This design allows <strong>multiple phones</strong> to record
simultaneously under one session, with the PC as the authoritative time
base. The PC can also integrate <strong>local hardware</strong> (e.g. a
webcam and GSR sensor connected directly) alongside the Android data.
All captured modalities (video streams, audio, thermal data, GSR
signals) are temporally aligned and later consolidated on the PC. The
result is a distributed recording system in which heterogeneous data
sources behave like a single synchronised apparatus.</p>
<h2 id="android-application-design-and-sensor-integration">4.2 Android
Application Design and Sensor Integration</h2>
<p>On the Android side, the application is structured to handle
<strong>multi-modal data capture</strong> in a coordinated fashion. At
its core is a <code>RecordingController</code> class that manages all
hardware components and recording tasks. This controller prepares each
subsystem – cameras (RGB and thermal), physiological sensors (GSR/PPG),
microphone, etc. – and triggers them in sync. When a recording session
starts, the controller initializes a new session directory and then
concurrently starts each enabled sensor/camera capture with
nanosecond-precision timestamps. Each modality’s data is written to
device storage in real time. The design relies on Android’s modern
libraries for robust performance: <strong>CameraX</strong> is used for
efficient video and image capture, and the <strong>Nordic BLE</strong>
library for reliable Bluetooth Low Energy communication with sensors.
Crucially, all sensor readings and frames are timestamped using a
monotonic clock source to ensure internal consistency. The app
architecture cleanly separates concerns – for example, camera handling
is in an <code>RgbCameraManager</code>, thermal imaging in a
<code>TopdonThermalCamera</code> module, and GSR sensing in a
<code>ShimmerGsrSensor</code> class – each exposing a common interface
for the controller to start/stop streams. This modular design makes it
easy to enable or disable features based on device capabilities (e.g. if
a phone has no thermal camera attached, that module remains inactive).
It also simplifies synchronisation logic, since the controller can treat
each data source uniformly (start all, stop all) and trust each to
timestamp its output.</p>
<figure>
<img src="../../diagrams/fig_4_02_thermal_integration_flow.png"
alt="Figure 4.2: Thermal Integration Flow" />
<figcaption aria-hidden="true">Figure 4.2: Thermal Integration
Flow</figcaption>
</figure>
<p><em>Figure 4.2: Thermal Integration Flow providing an overview of the
Android app’s internal architecture.)</em></p>
<p>The following subsections detail the integration of the
<strong>Topdon thermal camera</strong> and <strong>Shimmer GSR
sensor</strong> in the Android app.</p>
<h3 id="thermal-camera-integration-topdon">4.2.1 Thermal Camera
Integration (Topdon)</h3>
<p>Integrating the <strong>Topdon TC001</strong> thermal camera on
Android required using USB host mode and a UVC (USB Video Class)
library. The app utilises the open-source <strong>Serenegiant USB
Camera</strong> library (UVCCamera) [16] to interface with the device. A
dedicated class <code>TopdonThermalCamera</code> implements the
<code>ThermalCamera</code> interface and encapsulates all thermal camera
functionality. When the camera is physically connected via USB-C, an
<strong>Android USB monitor</strong> detects the device. The
<code>TopdonThermalCamera</code> registers a
<code>USBMonitor.OnDeviceConnectListener</code> to handle attachment
events. On a successful connection, it opens the UVC device and
configures it to the desired frame size and mode before starting the
video stream [16]. By default, the camera is set to its <strong>native
thermal resolution</strong> (256×192 pixels) and begins previewing
immediately on a background thread.</p>
<p>For each incoming thermal frame, the library provides a framebuffer
in <code>ByteBuffer</code> format. The implementation registers a frame
callback to retrieve this data stream. In the callback, the code reads
the raw temperature data from the buffer as an array of 16-bit (or
32-bit) values, depending on the camera’s output format. In this system,
the Topdon camera delivers a full temperature matrix for each frame –
the code treats it as an array of floats representing per-pixel
temperature readings [16]. The <code>TopdonThermalCamera</code> writes
each frame’s data to a CSV file: each row corresponds to one frame,
beginning with a high-resolution timestamp (in nanoseconds), followed by
the temperature values of all 49,152 pixels (256×192) in that frame
[16]. This exhaustive logging yields a large but information-rich
dataset, essentially a thermal video recorded as numeric data per frame.
To manage performance, the thermal capture runs in its own thread
context (inside the UVCCamera library’s callback) so that writing to
disk does not block the main UI or other sensors. The system foregoes
any heavy processing on these frames in real time; it simply dumps the
raw temperature grid to file and uses a lightweight callback to notify
the controller after each frame is saved. In the
<code>RecordingController</code>, a lambda hook receives a reference
when a new thermal frame file is saved, which is used to mark events
(for synchronisation or debugging) via a Lab Streaming Layer (LSL)
marker [3].</p>
<p><strong>Listing 4.1:</strong> Kotlin code snippet for thermal camera
logging (Topdon TC001).</p>
<pre><code>val dataFile = File(outputDirectory, &quot;thermal_data_${System.currentTimeMillis()}.csv&quot;)
fileWriter = FileWriter(dataFile)
val header = &quot;timestamp_ns,&quot; + (0 until 256 * 192).joinToString(&quot;,&quot;) { &quot;temp_$it&quot; } + &quot;\n&quot;
fileWriter?.append(header)

uvcCamera?.setFrameCallback({ frame: ByteBuffer -&gt;
    val timestamp = TimeManager.getCurrentTimestampNanos()
    val tempArray = FloatArray(frame.asFloatBuffer().remaining())
    frame.asFloatBuffer().get(tempArray)
    val tempDataString = tempArray.joinToString(separator = &quot;,&quot;)
    fileWriter?.append(&quot;$timestamp,$tempDataString\n&quot;)
}, UVCCamera.PIXEL_FORMAT_RGBX)</code></pre>
<p>Because the Topdon camera operates over USB, the app also handles
permission requests and device registration. The
<code>TopdonThermalCamera</code> calls
<code>usbMonitor.register()</code> during app start to begin listening
for devices [16], and unregisters on app pause to release resources. If
the device is present, the user is prompted to grant the app access.
Once granted, the <code>TopdonThermalCamera.open()</code> method uses
the USB monitor to obtain a control block and create a
<code>UVCCamera</code> instance [16]. The camera is then configured and
flagged as <em>connected</em>. At that point, if a preview display
surface is available (e.g. a small on-screen preview window in the app),
it can be attached via <code>startPreview(surface)</code> to render the
thermal feed live [16]. Previewing is optional for headless operation;
whether or not preview is shown, frames are being captured and logged.
Stopping the thermal camera involves stopping the preview (if any),
disabling the frame callback, closing the file writer, and destroying
the UVC camera instance [16]. This orderly shutdown ensures the USB
device is released for future sessions. Overall, the Topdon integration
provides <strong>frame-synchronised thermal imaging</strong>, with each
frame’s precise capture time recorded – a cornerstone for later aligning
thermal data with RGB video and physiological signals.</p>
<h3 id="gsr-sensor-integration-shimmer">4.2.2 GSR Sensor Integration
(Shimmer)</h3>
<p>The Android app connects to a <strong>Shimmer3 GSR+</strong> sensor
to record Galvanic Skin Response (GSR) and photoplethysmography (PPG)
data. Integration is done via <strong>Bluetooth Low Energy
(BLE)</strong>. The <code>ShimmerGsrSensor</code> class extends Nordic’s
BLE manager to handle the GSR sensor’s protocol. The Shimmer device
advertises a custom GATT service (proprietary to Shimmer) which the app
accesses using known UUIDs. In the code, the service and characteristic
UUIDs for the Shimmer’s BLE interface are defined as constants [15]. The
Shimmer uses a communication scheme akin to UART-over-BLE: one
characteristic (TX) is used to send commands to the sensor, and another
(RX) is used by the sensor to send continuous data notifications to the
app. The app’s <code>ShimmerGsrSensor</code> knows the specific byte
commands to control streaming – for this sensor, sending
<code>0x07</code> starts the live data stream and <code>0x20</code>
stops it [15].</p>
<p>When a Shimmer sensor is enabled in the app’s configuration, the
<code>RecordingController</code> creates a <code>ShimmerGsrSensor</code>
instance at startup and keeps it ready. Upon beginning a recording
session, if GSR recording is enabled, the controller invokes
<code>physiologicalSensor.startStreaming(...)</code> with a file writer
for output. Internally, this triggers the BLE manager to connect (if not
already connected) and then write the <strong>Start Streaming</strong>
command (0x07) to the sensor’s TX characteristic [15]. The Shimmer
device responds by sending a stream of notifications (typically at 128
Hz) on the RX characteristic, each containing the latest GSR and PPG
readings. The <code>ShimmerGsrSensor</code> sets up a notification
callback in its GATT callback’s <code>initialize()</code> method to
handle incoming data packets [15]. As data arrives, the
<code>onShimmerDataReceived()</code> function parses the byte payload
according to Shimmer’s data protocol [8]. The first byte acts as an
identifier (0x00 indicates a standard data packet), and subsequent bytes
contain the sensor readings. In each 8-byte packet, there are two bytes
for PPG and two bytes for GSR, among other info. The app reconstructs
the 16-bit raw values for PPG and GSR from the byte sequence [8]. The
GSR reading includes a range indicator encoded in the top bits, because
the Shimmer employs multiple gain ranges for skin conductance. The
implementation extracts the range and applies the appropriate conversion
formula to derive the resistance, then inverts it to get conductance in
microsiemens. This conversion is done exactly as per Shimmer’s
guidelines – for example, if the range bit indicates a 40.2 kΩ resistor,
the formula used is <em>GSR (µS) = (1/R) </em> 1000*, where R is
computed from the 14-bit ADC value using that resistor [8]. Similar
piecewise formulas are used for the other ranges (287 kΩ, 1 MΩ, 3.3 MΩ)
[8]. After conversion, each data point consists of a timestamp, a GSR
value (in µS), and a raw PPG value.</p>
<p>Every GSR/PPG sample is immediately written to a CSV file by the app.
The <code>ShimmerGsrSensor</code> maintains a file writer stream; on
starting, it writes a header line
(<code>timestamp_ns, GSR_uS, PPG_raw</code>) and then appends each new
sample as a new line. The timestamp is obtained via the app’s
<code>TimeManager.getCurrentTimestampNanos()</code> to ensure
consistency with how other modalities are timed. In addition to logging
to file, the app feeds the live data into an in-memory stream for
synchronisation with video: the <code>RecordingController</code>
provides a callback to <code>startStreaming()</code> that pushes each
sample into a local <strong>Lab Streaming Layer (LSL)</strong> outlet
named <code>"Android_GSR"</code> [3]. This allows GSR data to be
monitored in real time (e.g. plotted on the phone or streamed to the PC)
without interrupting the file recording. The BLE manager handles the
connection in a background thread, so incoming notifications do not
block the UI. If the BLE connection drops or encounters an error, the
Nordic library’s built-in retry mechanism attempts reconnection up to 3
times with a short delay [15]. The app also provides graceful shutdown:
when recording stops, it sends the stop command (<code>0x20</code>) to
halt streaming [15], and closes the file writer. This ensures the CSV is
properly finalized.</p>
<p><strong>Listing 4.2:</strong> Kotlin code snippet for Shimmer GSR
sensor BLE streaming and logging.</p>
<pre><code>fileWriter?.append(&quot;timestamp_ns,GSR_uS,PPG_raw\n&quot;)
writeCharacteristic(txCharacteristic, START_STREAMING_COMMAND, WRITE_TYPE_DEFAULT)
    .done { connectionState = STREAMING }  // start data stream
    .enqueue()

// on data notification callback:
if (bytes.isNotEmpty() &amp;&amp; bytes[0].toInt() == 0x00) {
    val timestamp = TimeManager.getCurrentTimestampNanos()
    // parse PPG (bytes[4..5]) and GSR (bytes[6..7]):
    val ppgValue = (bytes[10].toInt() and 0xFF) or ((bytes[3].toInt() and 0xFF) shl 8)
    val gsrRaw = (bytes[4].toInt() and 0xFF) or ((bytes[8].toInt() and 0xFF) shl 8)
    val range = (gsrRaw shr 14) and 0x03
    val resistance = (40200.0 * (gsrRaw and 0x3FFF)) / (4095 - (gsrRaw and 0x3FFF))
    val gsrValue = (1.0 / resistance) * 1000.0
    fileWriter?.append(&quot;$timestamp,${&quot;%.4f&quot;.format(gsrValue)},$ppgValue\n&quot;)
}</code></pre>
<figure>
<img src="../../diagrams/fig_4_03_shimmer_gsr_integration.png"
alt="Figure 4.3: Shimmer GSR Integration" />
<figcaption aria-hidden="true">Figure 4.3: Shimmer GSR
Integration</figcaption>
</figure>
<p><em>Figure 4.3: Shimmer GSR Integration illustrating the desktop
controller application architecture.)</em> Overall, the Shimmer
integration brings in high-resolution physiological data synchronised
with video.</p>
<h2 id="desktop-controller-design-and-functionality">4.3 Desktop
Controller Design and Functionality</h2>
<p>The desktop controller is a <strong>cross-platform
application</strong> (tested on Windows, Linux, and macOS) built with Qt
for the GUI (PyQt6/PySide6) [17] and Python 3 for logic, augmented by
performance-critical C++ components. Its design follows a modular,
MVC-like pattern: the UI is separated into tabs corresponding to major
functionalities (e.g. device management/dashboard, live monitoring,
playback/annotation, settings). When the controller starts, it
initializes a main window with a tabbed interface. The primary
<strong>Dashboard</strong> tab provides an overview of connected devices
and local sensors. For example, it can display live video feeds and GSR
plots in a grid layout – the code sets up a <code>QLabel</code> for a
video preview and a PyQtGraph <code>PlotWidget</code> for GSR on the
dashboard. A <strong>Logs</strong> tab captures real-time system
messages (status updates, errors) for debugging. Another tab for
<strong>Playback &amp; Annotation</strong> allows reviewing recorded
sessions (in a later implementation phase). Each tab’s UI elements are
created and managed with Qt layouts, making the interface flexible and
scalable as devices are added.</p>
<p>Under the hood, the PC controller employs several background threads
and helpers to manage networking and data processing without freezing
the GUI. A dedicated <strong>WorkerThread</strong> (a
<code>QThread</code> subclass) is responsible for all communication with
Android devices. When the user initiates a connection to a device, this
worker thread opens a TCP socket to the phone’s IP and port. The thread
then runs an event loop receiving JSON messages from the device. It
parses incoming messages and emits Qt signals to the main thread for
handling UI updates. For instance, if a connected Android phone sends a
preview frame update, the worker decodes the base64-encoded image bytes
to a <code>QImage</code> and emits a <code>newPreviewFrame</code> signal
carrying the image and device identifier. The main GUI thread connects
this signal to a slot that displays the frame in the dashboard
(e.g. updating the corresponding QLabel’s pixmap). The worker also
handles command responses: every command sent to a device includes a
unique ID, and the device’s reply includes an <code>ack_id</code> with
status info. For example, a <code>"capabilities_data"</code> response
contains the list of cameras the device has – the worker emits a
<code>camerasReceived</code> signal with that list so the UI can
populate camera options. This asynchronous message-passing design keeps
the GUI responsive and allows the PC to manage multiple devices
simultaneously by spawning separate threads per connection.</p>
<p><strong>Listing 4.3:</strong> Python (PyQt) code snippet illustrating
the signal–slot mechanism for preview frames.</p>
<pre><code># WorkerThread signal declaration and usage
class WorkerThread(QThread):
    newPreviewFrame = pyqtSignal(str, QImage)
    ...
    def run(self):
        ...
        if message.get(&quot;type&quot;) == &quot;preview_frame&quot;:
            image_data = base64.b64decode(message[&quot;image_data&quot;])
            qt_image = QImage()
            qt_image.loadFromData(image_data)
            self.newPreviewFrame.emit(message[&quot;device_id&quot;], qt_image)

# In MainWindow (GUI thread), connecting the signal to a slot
self.worker.newPreviewFrame.connect(self.handle_new_preview)

@pyqtSlot(str, QImage)
def handle_new_preview(self, device_id, image):
    # Update the corresponding device&#39;s preview label
    self.preview_labels[device_id].setPixmap(QPixmap.fromImage(image))</code></pre>
<p>The application uses <strong>Zeroconf (mDNS)</strong> [19] to
simplify device discovery: on startup, the PC browses for services of
type <code>_gsr-controller._tcp</code> on the local network. Each
Android device advertises itself with that service type and a name like
“GSR Android Device [Model]”. The PC can thus list available devices and
their addresses automatically, eliminating manual IP entry.</p>
<p>A standout feature of the PC controller is its <strong>native C++
backend</strong> for time-sensitive hardware interaction. This is
implemented as a Python extension module (built via PyBind11) [18] named
<code>native_backend</code>. It provides classes
<code>NativeWebcam</code> and <code>NativeShimmer</code> which run in
background threads and feed data to the Python layer with minimal
latency. The controller instantiates these at startup: for example,
<code>NativeWebcam(0)</code> opens the local webcam (device 0) and
begins capturing frames in a loop, and
<code>NativeShimmer("COM3")</code> connects to a Shimmer GSR device via
a serial port (COM3 on Windows). These native objects start immediately
and run independently of the Python GIL, pushing data into thread-safe
queues. The GUI uses a QTimer tick (every ~16 ms) to periodically
retrieve the latest data from the native threads. On each tick, it pulls
a frame from the webcam class (as a NumPy array) and updates the
corresponding video label [1], [2]. Similarly, it polls the Shimmer
class for new GSR samples and updates the live plot. Using a C++ backend
drastically improved performance: the webcam thread captures at a steady
~60 FPS by sleeping ~16 ms per iteration, and yields frames without
significant buffering. Meanwhile, the Shimmer thread reads sensor bytes
as fast as they arrive (128 Hz) with precise timing. Both use lock-free
queues to decouple production and consumption of data. The C++ code
directly converts camera frames to a shared memory buffer that is
exposed to Python as a NumPy array without copying [1], [2], and
similarly packages GSR readings into Python tuples. This design
minimises overhead, latency, and jitter – imperative for synchronising
local PC data with remote device data.</p>
<p>Beyond live monitoring, the desktop app includes tools for
<strong>post-session analysis</strong>. The <strong>Playback &amp;
Annotation</strong> tab (Figure 4.4) is designed to load the recorded
video files (RGB and thermal) along with sensor data and allow the user
to replay the session in a synchronised fashion. Internally, the
controller uses libraries like <em>PyAV</em> (wrapping FFmpeg) to read
video files and <em>PyQtGraph</em> for plotting time-series data like
GSR. The user can seek through the timeline; the app will display the
video frame at the selected time and the corresponding point on the GSR
plot, maintaining alignment via timestamps. Annotation functionality
enables adding notes at specific times – these can be saved in a sidecar
file or embedded in session metadata. Another part of the PC software is
a <strong>Calibration</strong> utility, which helps calibrate cameras
after recordings. Using OpenCV [22], it can detect calibration patterns
(chessboards or ChArUco markers) in the raw RGB frames to calculate each
camera’s intrinsic parameters, and if multiple cameras (e.g. a phone’s
RGB and thermal, or a phone and PC webcam) observed the same pattern, it
can compute the extrinsic calibration between them. The results (camera
matrices, distortion coefficients, transformation matrices) are saved
for use in data analysis, ensuring that researchers can accurately map
thermal and RGB imagery. Finally, a <strong>Data Export</strong> feature
allows converting a session’s dataset into formats like MATLAB
<code>.mat</code> files or HDF5. This is done by reading the CSVs and
video files from the session and packaging the data (often downsampled
or compressed as needed) into a single file per session for convenient
distribution or analysis. In summary, the desktop controller is both the
live “mission control” during data acquisition and a post-processing
suite, all implemented in a cohesive application.</p>
<figure>
<img src="../../diagrams/fig_4_04_desktop_gui_layout.png"
alt="Figure 4.4: Desktop GUI Layout" />
<figcaption aria-hidden="true">Figure 4.4: Desktop GUI
Layout</figcaption>
</figure>
<p><em>Figure 4.4: Desktop GUI Layout illustrating the playback and
annotation interface described above.)</em></p>
<h2 id="communication-protocol-and-synchronisation-mechanism">4.4
Communication Protocol and synchronisation Mechanism</h2>
<p>A custom <strong>communication protocol</strong> connects the PC
controller with each Android device, built on TCP/IP sockets [21] with
JSON message payloads. After the PC discovers an Android device (via
Zeroconf), it initiates a TCP connection to the device’s advertised
port. The Android app runs a lightweight TCP server to accept this
connection. All commands from PC to Android are sent as JSON objects
with a schema like
<code>{"id": &lt;command_id&gt;, "command": "&lt;action&gt;", "params": { ... }}</code>.
The device, upon receiving a command, executes the requested action and
then replies with a JSON response containing the original command ID (as
<code>ack_id</code>) and a status or result. For example, the first
command the PC sends is <code>"query_capabilities"</code>, which asks
the phone to report its hardware capabilities. The Android app responds
with a message like
<code>{"ack_id": 1, "status": "capabilities_data", "capabilities": { ... }}</code>
including details such as available cameras (with their identifiers,
resolutions, and frame rates). This exchange allows the PC to
dynamically adjust to each device – for instance, listing the camera
options or knowing if the thermal sensor is present. Another command is
<code>"start_recording"</code>, which instructs the Android to begin a
new recording session. The phone will then initiate all its sensors
(cameras, etc.) and reply with an acknowledgment
(e.g. <code>"status": "ok"</code>) once recording has successfully
started. Similarly, a <code>"stop_recording"</code> command stops all
captures and finalizes the files.</p>
<p>In addition to explicit commands, the protocol supports
<strong>continuous data streaming</strong> for live previews. While a
session is idle or recording, the Android app periodically sends
<code>"preview_frame"</code> messages containing a downsampled frame
from the camera preview encoded as a base64 JPEG string. The PC’s worker
thread listens for these and updates the UI so the operator can see a
low-latency video feed from each device. This preview is throttled
(e.g. one frame every 0.5 seconds, or as configured) to balance
timeliness with network load. Similarly, the app could stream low-rate
telemetry (such as current recording status or battery level) using this
push mechanism. All such asynchronous messages include a
<code>type</code> field (for example,
<code>"type": "preview_frame"</code>) rather than an ack ID, so the PC
knows they are not responses to a specific command but rather
unsolicited data.</p>
<p>The communication sequence implements a time synchronisation
strategy. When the PC and a phone connect, they perform a simple
handshake (exchange of hello messages and capabilities). Part of this
handshake is a <strong>time synchronisation routine</strong>. The system
employs an NTP-inspired algorithm to align clocks: the PC (acting as
time server) sends a sync request with its current timestamp, the phone
responds with its own timestamp, and the PC measures the round-trip time
to estimate network latency. Through one or more exchanges, the PC
calculates the offset between its clock and the phone’s clock. This
offset is then used to relate the timestamps coming from that device.
Each device continues to timestamp its data with its <strong>local
monotonic clock</strong> (nanosecond precision on both ends), which
ensures extremely fine timing granularity. The PC, knowing the offset
for each device, can translate a device’s timestamps into the PC’s
master clock domain. This yields cross-device synchronisation typically
within <strong>sub-millisecond accuracy</strong>. This accuracy is on
the order of that achieved by dedicated Precision Time Protocol (PTP)
systems. In practice, the controller designates its start time as t = 0
when recording begins, and instructs each Android to note its local time
at that moment; subsequent data from the phones include raw timestamps
which are later converted to the common timeline.</p>
<p>To ensure reliability and security, the protocol includes additional
features. Every command from the PC expects an acknowledgment; if none
arrives within a timeout, the PC can retry or mark that device as
unresponsive. This prevents silent failures (e.g. if a start command is
lost due to a network issue, the PC will detect it and resend).
Communication is also secured: the design uses an <strong>RSA/AES
encryption layer</strong> for all messages (commands and data). In
practice, this means the PC and device perform an initial RSA public key
exchange, then switch to an AES symmetric key for the session. This
guarantees that sensitive data (like physiological readings or video
frames) cannot be intercepted or tampered with on an open network. The
messages themselves are kept compact and human-readable (JSON) for ease
of debugging and extensibility. For instance, if a new sensor is added,
a new command and message type can be defined without overhauling the
protocol, as long as both sides understand the JSON fields.</p>
<p>One notable aspect of synchronisation is how the <strong>Lab
Streaming Layer (LSL)</strong> is leveraged. On the Android side, LSL
outlets are created for certain data streams (GSR, events, etc.) [9]. If
the PC were also running an LSL inlet – for example, subscribing to the
“Android_GSR” stream – it could receive samples with timestamps that are
already globally synced via LSL’s internal clock synchronisation.
However, in this system, LSL is used primarily locally on each device
for internal coordination (e.g. marking exactly when a thermal frame was
saved relative to a GSR sample). The main synchronisation still relies
on the custom network time alignment, which is under direct application
control. By combining these approaches – precise device-local timestamps
and network clock alignment – the system addresses both
<strong>intra-device sync</strong> (camera frames vs. sensor readings on
the same phone) and <strong>inter-device sync</strong> (phone A
vs. phone B vs. PC). As a result, all data collected across the system
can be merged on a unified timeline during analysis, with only
microsecond-level adjustments needed at most.</p>
<p>Finally, when stopping a recording and collecting files, the protocol
ensures a coordinated shutdown. The PC issues
<code>stop_recording</code> to all devices; each device stops and closes
its files, then sends back an acknowledgment (or a message like
<code>"recording_stopped"</code> with a summary). The PC can then send a
<code>"transfer_files"</code> command to each device. Upon this request,
the Android app compresses its session folder into a ZIP archive (using
<code>FileTransferManager.zipSession()</code>) and responds with a
message containing the file name and size when ready. The actual file
data transfer is done out-of-band (to avoid clogging the control
channel): the phone opens a new socket to the PC’s waiting file receiver
on a specified port and streams the file bytes directly. During this
transfer, the PC may pause other commands or use a separate thread to
handle the incoming file. Once the file is received and its checksum
verified, the PC sends a final acknowledgment, and the device can
optionally delete its local data. This concludes the session’s active
phase and hands off to the data processing stage.</p>
<figure>
<img src="../../diagrams/fig_4_05_protocol_sequence.png"
alt="Figure 4.5: Protocol Sequence" />
<figcaption aria-hidden="true">Figure 4.5: Protocol
Sequence</figcaption>
</figure>
<p><em>Figure 4.5: Protocol Sequence illustrating the messaging sequence
and clock synchronisation described above.)</em></p>
<h2 id="data-processing-pipeline">4.5 Data Processing Pipeline</h2>
<p>The data processing pipeline encompasses everything from data capture
on devices to the final preparation of datasets for analysis. It
operates as a <strong>streaming pipeline</strong> during recording and a
<strong>batch pipeline</strong> after recording. On each Android device,
when a new recording session starts, a unique session ID is generated
(based on a timestamp) and a dedicated directory is created in the
device’s storage for that session. All data files for that session are
saved under this directory, organised by modality. For example, within a
session directory the app creates sub-folders for raw images and thermal
frames upfront. This ensures that as data starts streaming in, the file
system is structured to prevent conflicts and simplify later
retrieval.</p>
<p>During an active recording, data from each modality is handled in
parallel:</p>
<ul>
<li><p><strong>RGB Video:</strong> The <code>RgbCameraManager</code>
starts recording via CameraX’s <code>VideoCapture</code> API to an MP4
file on the device. The file is typically named
<code>RGB_&lt;sessionId&gt;.mp4</code> and saved in the session folder.
Video is encoded with H.264 at 1080p 30 FPS (Quality.HD) as configured.
Recording continues until stopped, at which point the file is finalized
(CameraX handles closing the file and muxing the audio track if one was
included).</p></li>
<li><p><strong>Raw Image Stream:</strong> If enabled, the app captures
full-resolution still images continuously during the recording. The
<code>RgbCameraManager</code> uses an <code>ImageCapture</code> use case
to take a picture roughly every 33 ms (~30 FPS) on a background
executor. Each image is saved as a JPEG file in a
<code>raw_rgb_&lt;sessionId&gt;</code> directory, with a filename
containing its exact nanosecond timestamp (e.g.,
<code>raw_rgb_frame_&lt;timestamp&gt;.jpg</code>). These images are
unprocessed (straight from the camera sensor in YUV format converted to
JPEG) to allow later analysis or calibration. By capturing them
concurrently with video, the system provides both a compressed
continuous video and a series of key frames that can be examined
frame-by-frame at full quality.</p></li>
<li><p><strong>Thermal Frames:</strong> The
<code>TopdonThermalCamera</code> writes thermal data frames to a CSV
file (or sequence of CSVs). In this implementation, it creates one CSV
named <code>thermal_data_&lt;sessionId&gt;.csv</code> in a
<code>thermal_&lt;sessionId&gt;</code> directory when streaming starts
[16]. The first row is a header with pixel index labels, and each
subsequent row corresponds to one thermal image frame – the first column
is the frame timestamp and the rest are temperature values [16]. (If
needed, the system could also save thermal images by converting the
temperature matrix to a grayscale or colour-mapped image, but the
current design prioritises numerical data for precision.)</p></li>
<li><p><strong>GSR/PPG Data:</strong> The Shimmer GSR+ sensor data is
logged to a CSV file named <code>GSR_&lt;sessionId&gt;.csv</code> in the
session folder [15]. The file begins with a header
(<code>timestamp_ns, GSR_uS, PPG_raw</code>), and each subsequent line
represents one sample, as recorded by the <code>ShimmerGsrSensor</code>
described earlier [15]. Sampling at 128 Hz means this file grows by 128
lines per second of recording. The timestamps are the phone’s nanosecond
ticks, which will later be re-aligned to the global timeline.</p></li>
<li><p><strong>Audio:</strong> The app can also record audio via the
microphone (stereo 44.1 kHz) if enabled. Audio is captured using
Android’s MediaRecorder (or AudioRecorder) API and saved as an
AAC-encoded track, either in its own file
(e.g. <code>Audio_&lt;sessionId&gt;.m4a</code>) or multiplexed into the
RGB video MP4. In this system, audio was stored separately – having a
separate audio file with a known start time simplifies synchronisation
during analysis.</p></li>
<li><p><strong>Annotations/Events:</strong> If any user markers or
automated events occur (for example, the user taps a button to mark a
moment), these are recorded in a dedicated log or embedded in the
session metadata. The <code>SessionManager</code> is designed to produce
a <code>session_metadata.json</code> file at the end of capture, which
would include details like start/stop times, device info, and event
timestamps. (In the current implementation this is a placeholder, but
the structure supports future expansion.)</p></li>
</ul>
<p>Once the PC issues a stop command, each Android device closes its
files. The next stage is <strong>data aggregation</strong>. The PC can
request each phone to send over its session data. To streamline this,
the Android app compresses its session folder into a single ZIP archive
using a <code>FileTransferManager.zipSession()</code> method. This zips
up all files (video, images, CSVs, etc.) from that recording session.
The app places this ZIP in a cache directory and then uses
<code>FileTransferManager.sendFile()</code> to initiate a transfer to
the PC. The transfer is done via a simple socket stream — the phone
knows the PC’s IP and a designated port for file uploads (communicated
during the protocol handshake). It opens a connection and streams the
bytes of the ZIP file. On the PC side, a corresponding file receiver
listens and writes the incoming bytes to a file (usually naming it with
the device name and session ID to avoid confusion). A progress indicator
in the PC UI (e.g. a QProgressDialog) lets the user know data is being
downloaded from the device.</p>
<p>After collection, the PC holds all data from all devices.
<strong>Post-processing</strong> can then proceed. The PC controller’s
analysis modules operate on the data in these session archives. For
instance, the playback module will unzip or directly access the video
file and sensor CSVs to replay the session. Because every piece of data
has an accurate timestamp, aligning streams is straightforward: the GSR
plot is rendered on a time axis (seconds or milliseconds), and video
frames are displayed at their corresponding timestamps (the controller
can use video file frame timestamps or infer them from raw image
filenames). The annotation tool overlays markers on the timeline and can
also allow notes to be associated with video frames.</p>
<p>For research use cases, exporting data is important. The pipeline
ends with an <strong>export step</strong>, where the session’s raw data
is converted to shareable formats. A script or UI action on the PC
triggers this export: the implementation uses libraries like
<strong>pandas</strong> [23] and <strong>h5py</strong> [24] to combine
data into HDF5 or MATLAB files. It may create a structured dataset where
each sensor modality is a group or table (e.g. an HDF5 group
<code>/GSR</code> containing a timestamp array and a GSR value array,
and a group <code>/Video</code> containing video frame timestamps or
references). Calibration results, if available, are included so that
pixel coordinates in videos can be mapped to real-world units. The final
exported files allow researchers to load the entire session in tools
like MATLAB or Python with one command, with all streams readily
synchronised.</p>
<p>The pipeline ensures that from <strong>capture to archive</strong>,
data is kept synchronised and well-labelled, and from <strong>archive to
analysis</strong>, data is easily accessible and interpretable. The
automated zipping and transferring remove manual steps, and the
structured session directories prevent any mix-ups between sessions or
devices.</p>
<figure>
<img src="../../diagrams/fig_4_06_data_processing_pipeline.png"
alt="Figure 4.6: Data Processing Pipeline" />
<figcaption aria-hidden="true">Figure 4.6: Data Processing
Pipeline</figcaption>
</figure>
<p><em>Figure 4.6: Data Processing Pipeline providing an overview of the
data processing pipeline from capture through to data export.)</em></p>
<h2 id="implementation-challenges-and-solutions">4.6 Implementation
Challenges and Solutions</h2>
<p>Developing this complex system presented several implementation
challenges. This section discusses some key issues encountered and the
solutions applied to overcome them:</p>
<ul>
<li><p><strong>Ensuring Precise synchronisation:</strong> Achieving
tight time synchronisation across multiple Android devices and the PC
was non-trivial due to clock drift and network latency. The solution was
a two-tier synchronisation mechanism. Each device timestamps data with a
local high-resolution clock (avoiding reliance on internet time or
coarse NTP time, which can be imprecise on mobile). Then, a lightweight
NTP-like protocol aligns those clocks by calculating the offset and
delay. This was fine-tuned by taking multiple measurements at connection
time and occasionally during recording. The result is that all devices
maintain a shared notion of time within sub-millisecond tolerance. In
practice, this means if an event (e.g. an LED flash) is captured by two
cameras and a GSR sensor, the timestamps recorded by each device for
that event differ by less than 1 ms after alignment. <strong>Solution
highlights:</strong> use monotonic clock APIs on each platform for
timestamping and perform a quick clock sync handshake for alignment at
start (and periodically if needed).</p></li>
<li><p><strong>High Data Throughput and Storage Management:</strong>
Recording high-definition video alongside high-frequency sensor data can
quickly overwhelm device I/O and memory if not handled efficiently.
Several strategies were employed. First, writing to storage was done
sequentially using buffered streams, which is efficient for both video
and CSV writes. Raw image capture posed a challenge because saving a
JPEG every 33 ms could saturate the I/O. This was mitigated by
performing image captures on a dedicated single-threaded executor
separate from the main thread, ensuring the CameraX pipeline had its own
thread and disk writes did not block the UI or sensor reads. The system
also avoids keeping large data in memory; for example, thermal frames
are written directly to file inside the frame callback, and GSR samples
are appended to a file (and optionally to a small in-memory buffer for
LSL) one by one. Because Android devices have limited storage, another
challenge was preventing extended sessions (which generate many image
files and large videos) from filling up the device. The solution was to
offload data as soon as possible: immediately after each session, the
phone compresses the session folder to a ZIP and transfers it to the PC.
The phone can then optionally delete its local copy (“delete after
transfer”), so even if multiple sessions are recorded back-to-back, the
bulk of data resides on the PC.</p></li>
<li><p><strong>Thermal Camera USB Integration:</strong> Using the Topdon
TC001 thermal camera introduced challenges in driver support and
performance. Android has no native support for USB thermal cameras, so
the UVCCamera library was used, but this required handling USB
permissions and ensuring real-time performance in Java. One issue was
the volume of data (nearly 50k float values per frame). Pushing this
through the Java/Kotlin layer every frame could be slow. The chosen
approach was to leverage the library’s native (JNI) code to fetch frames
and do minimal processing in Kotlin – essentially just copying the
buffer to file. By writing frames as raw floats to CSV, this was avoided
any expensive image rendering computations during capture. Another
challenge was potential USB dropouts if the app couldn’t keep up with
the frame rate. We addressed this by monitoring the frame callback
speed; if frames started queuing up, the app would drop some preview
processing to catch up, ensuring the logging thread always runs at high
priority. Additionally, upon connection the camera is explicitly set to
the correct mode (frame size and format) to avoid negotiation issues.
Handling the permission prompt promptly was also important – the app
requests USB permission as soon as the device is detected, so by the
time the user starts recording, the camera is already authorized and
ready to open.</p></li>
<li><p><strong>Reliable BLE Sensor Streaming:</strong> The Shimmer GSR+
streaming over BLE can be susceptible to packet loss or disconnects
(e.g. in electrically noisy environments or if the phone’s BLE stack is
busy). This was tackled by using Nordic’s robust Android BLE library,
which provides buffered writes, automatic retries, and easy callback
management. For example, on connection the code retries the BLE
connection up to 3 times with a short delay to overcome transient
failures during the handshake. Moreover, once connected, the app
immediately sets up notifications and starts streaming to lock in the
data flow. The design of writing data to file versus sending it live was
carefully balanced: writing every sample to CSV ensures no data loss
(even if the UI or network is slow, the data is safely on disk), while
the live LSL broadcast is best-effort (missing a few samples on the live
graph is acceptable as long as the file has the full record). It is also
important to send the stop command to the Shimmer device before
disconnecting to gracefully terminate its stream ; otherwise, if a user
restarted recording immediately, the Shimmer might still be in streaming
mode and require a reset. By sending 0x20 (stop) and waiting briefly,
the sensor returns to a known idle state. These precautions improved the
BLE link reliability so that even hour-long recordings proceeded without
dropout.</p></li>
<li><p><strong>Cross-Platform Performance in the PC App:</strong> Python
is interpreted and could become a bottleneck for real-time video and
sensor handling. Initially, the system used OpenCV [22] in Python for
webcam capture and PySerial for GSR, but latency and jitter were
noticeable (tens of milliseconds of variability). The solution was to
implement those parts in C++ and integrate via PyBind11 [18]. The
<code>NativeWebcam</code> class uses OpenCV’s [22]
<code>VideoCapture</code> in a separate thread to grab frames and push
them into a queue. As C++ code, it’s compiled and optimised, running
independent of the Python GIL. The frame rate became very stable (the
thread sleeps ~16 ms to achieve ~60 FPS, matching display refresh), and
frame delivery to Python is done by sharing the memory pointer of the
<code>cv::Mat</code> with NumPy – essentially zero-copy image sharing.
Similarly, the <code>NativeShimmer</code> class opens a serial port
(using Win32 API on Windows or termios on Linux) and reads bytes in a
tight loop. It applies the same GSR conversion formula as the Android (a
mirror implementation in C++) and pushes timestamped samples into a
queue. Measurements showed a ~67% reduction in end-to-end latency
(sensor update to plot update) and ~79% reduction in timing jitter on
the PC side after using the native backend. The trade-off was the added
complexity of compiling C++ code for multiple platforms, but this was
mitigated this with CMake and continuous integration testing on each
OS.</p></li>
<li><p><strong>User Interface and Multi-Device Coordination:</strong>
Another challenge was designing a GUI that could handle multiple device
feeds without overwhelming the user or the system. This was solved with
a dynamic grid layout on the Dashboard: as devices connect, new video
preview widgets are added (and corresponding plot widgets if the device
has a sensor). Qt’s layouts automatically manage positioning. Ensuring
that updating these widgets (especially painting video frames) happens
on the GUI thread was crucial. The solution was to use Qt’s signal/slot
mechanism – the background thread emits a signal with a
<code>QImage</code>, and the main thread’s slot sets that image on a
<code>QLabel</code>. This approach is thread-safe and keeps heavy
lifting off the UI thread. For many devices streaming simultaneously,
simple frame rate limiting on previews was also implemented (each phone
sends at most a fixed number of preview frames per second) to prevent
flooding the network or GUI. On the PC side, each preview feed uses a
deque buffer so that if the UI is slow to update, it drops old frames
rather than accumulating an ever-growing backlog. Additionally,
coordinating the start of recording across devices was challenging – if
one device started even 100 ms later than another, that would introduce
a sync error. To handle this, the PC sends the start command to all
devices nearly simultaneously (looping through devices in a few
milliseconds) and each device waits for the same <strong>trigger
timestamp</strong> (included in the command) to begin recording. In
effect, the PC says “start recording at time T = XYZ,” and all devices
schedule their start at their local time corresponding to XYZ. This was
achieved by having devices continuously sync their clocks with the PC
during an active session (making slight adjustments) or simply relying
on the initial offset if drift is minimal over a short period. The
outcome is that all devices begin capturing within a few milliseconds of
each other, which the synchronisation logic then corrects to under 1 ms
alignment.</p></li>
</ul>
<p>By addressing these challenges with targeted solutions – from
low-level optimisations (native code, buffering, threading) to
high-level protocol design (sync and reliability features) – the system
became robust and performant. Each component was tuned to handle
worst-case scenarios (e.g. maximum data rates, multiple devices, long
recording durations) so that in typical use it operates with plenty of
headroom. This engineering effort is what enables the <strong>GSR &amp;
Dual-Video Recording System</strong> to reliably collect synchronised
multi-modal data in real-world research settings.</p>
<h2 id="references-3">References</h2>
<p>See <a href="references.md">centralized references</a> for all
citations used throughout this thesis.</p>
<h1 id="evaluation-and-testing">5 Evaluation and Testing</h1>
<h2 id="testing-strategy-overview">5.1 Testing Strategy Overview</h2>
<p>A comprehensive testing strategy was adopted to validate the
multi-sensor recording system’s functionality, reliability, and
performance across its Android and PC components. The approach combined
<strong>unit tests</strong> for individual modules, <strong>integration
tests</strong> spanning multi-device coordination and networking, and
<strong>system-level performance evaluations</strong>. Wherever
possible, tests simulated hardware interactions (e.g., sensor devices,
network connections) to enable repeatable automated checks without
requiring physical devices. This multi-tiered strategy ensured that each
component met its requirements in isolation as well as in combination
with others, and that the overall system could operate robustly under
expected workloads.</p>
<p>The testing effort was structured to mirror the system’s layered
architecture. Low-level functions (such as sensor data handling, device
control logic, and utility routines) were verified with unit tests on
their respective platforms. Higher-level behaviours, such as
synchronisation between Android and PC nodes or end-to-end data flows,
were validated with integration tests that exercised multiple components
together. In addition, <strong>architectural conformance tests</strong>
enforced design constraints (for example, ensuring the UI layer does not
directly depend on the data layer), and <strong>security tests</strong>
checked that encryption and authentication mechanisms functioned as
intended. Finally, extensive <strong>stress and endurance tests</strong>
evaluated system performance (memory usage, CPU load, etc.) over
prolonged operation. This combination of methods demonstrates that the
system meets its design goals and can maintain performance over time.
Any areas not amenable to automated testing (for example, user interface
fluidity or real-device Bluetooth connectivity) were earmarked for
manual testing or future work. Overall, the strategy aimed for broad
coverage, from basic functionality to long-term stability, aligning with
best practices for dependable multi-device systems.</p>
<h2 id="unit-testing-android-and-pc-components">5.2 Unit Testing
(Android and PC Components)</h2>
<p><strong>Android Unit Tests:</strong> On the Android side, unit tests
were written using JUnit and the Robolectric framework to run tests
off-device. This allowed the Android application logic to be exercised
in a simulated environment with controlled inputs. Key components of the
Android app – such as the Shimmer sensor integration, connection
management, session handling, and UI view-model logic – each have
dedicated test classes. These tests create <em>mock</em> dependencies
for Android context, logging, and services so that the business logic
can be tested in isolation. For example, the
<code>ShimmerRecorder</code> class (responsible for managing a Shimmer
GSR device) is tested via <code>ShimmerRecorderEnhancedTest</code> using
a Robolectric test runner [1]. In the setup, dummy objects replace the
real Android <code>Context</code>, <code>SessionManager</code>, and
<code>Logger</code>, enabling verification that methods behave correctly
without an actual device or UI [2]. The tests cover initialisation and
all major methods of <code>ShimmerRecorder</code>, invoking each under
normal and error conditions to ensure robust behaviour. For instance,
one test confirms that calling <code>initialize()</code> returns true
(indicating a successful setup) and logs an appropriate message [3].
Others validate error handling: for example, calling a sensor
configuration or data retrieval function with a non-existent device ID
fails gracefully, returning false or null and emitting an error log [4],
[5]. This ensures the app will not crash if, for example, a user
attempts an operation on a disconnected sensor. Similar tests for
methods like <code>setSamplingRate</code>, <code>setGSRRange</code>, and
<code>enableClockSync</code> verify that invalid parameters or device
states are handled safely [6], [7].</p>
<p>Beyond sensor management, the Android app’s supporting utilities and
controllers are also unit tested. For example,
<code>ConnectionManagerTestSimple</code> instantiates the network
<code>ConnectionManager</code> with a mock logger to ensure it
initialises properly and does not produce unexpected errors [8], [9].
Other test classes (referenced in a comprehensive test suite) target
file management logic, USB device controllers, calibration routines, and
UI helper components. Many of these use <strong>MockK</strong> and
<strong>Truth</strong> assertions to validate internal state changes or
interactions. Using relaxed mocks (which do not require strict
predefined behaviour) allows the tests to focus on the code under test
and only flag unwanted or missing calls.</p>
<p>In summary, the Android unit tests focus on core functionality and
error handling for each module in isolation. There are numerous test
cases aimed at broad coverage, although a few test stubs remain disabled
(e.g. for some legacy components), indicating planned but incomplete
coverage. Nonetheless, critical Android features such as sensor
configuration, data streaming, permission management, and network
quality monitoring are covered by automated tests. This gives confidence
that the mobile app’s logic is solid before integrating with external
devices or servers.</p>
<p><strong>PC Unit Tests:</strong> The PC software (primarily
implemented in Python) also has an extensive set of unit tests covering
its various subsystems. One important set of PC tests focuses on the
<strong>endurance testing utilities</strong> – specifically the memory
leak detection and performance monitoring logic. For example, the
<code>MemoryLeakDetector</code> class is tested to verify it correctly
records memory usage samples and identifies growth trends. In a
controlled test scenario, a sequence of increasing memory usage values
is fed to the detector and its analysis function is invoked [10]. The
test asserts that the detector reports a leak when the growth exceeds a
threshold (e.g., &gt;50 MB increase over an interval) [11]. A
complementary test feeds a fluctuating memory pattern with no
significant overall growth and confirms that no false-positive leak is
reported [12]. These results show that the system’s memory leak alerts
(used in long-run testing) are both sensitive and specific. Another area
of focus is <strong>configuration management</strong> and utility
classes: the <code>EnduranceTestConfig</code> dataclass is instantiated
with default and custom parameters in tests to ensure all fields take
expected values [13], [14]. This helps catch errors in default settings
that could affect test behaviour or system startup.</p>
<p>Beyond the endurance framework, unit tests also cover architectural
and security aspects of the PC code. A static analysis test module
(<code>test_architecture.py</code>) enforces the intended layered
architecture. It parses all Python files and checks for forbidden
dependencies (e.g., UI layer code importing data layer code) [15], [16].
If any such dependency is found, the test fails with an explanatory
message [17]. This ensures adherence to modular design principles (for
example, no platform-specific libraries are used in cross-platform
logic). In practice, running these tests confirmed that there were no
circular imports or cross-layer violations in the final implementation,
indicating a clean separation of concerns. Similarly, security-related
unit tests verify features like TLS encryption and authentication. For
instance, <code>TLSAuthenticationTests</code> generates temporary SSL
certificates and attempts to configure a <code>DeviceClient</code> to
use them [18]; it asserts that a valid certificate is accepted and
enables the client’s SSL mode [19]. The same test case checks that only
properly formatted authentication tokens are accepted , and that
production configuration files have the required security settings
(certificate pinning, data encryption flags) enabled . These tests
confirm that security measures are present and correctly
implemented.</p>
<p>Overall, the PC-side unit tests cover a wide range of functionality –
from file handling and data serialization to networking protocols and
system monitoring. Many tests use <strong>mock objects</strong> and
patching to isolate the unit under test. For instance, when testing the
endurance runner, calls to actual system monitors or performance
optimisers are replaced with patched versions returning controlled data
. This way, the test can simulate scenarios like a stable CPU load or
predictable memory availability, then verify that metrics collection and
logging produce the expected results (such as writing a JSON report).
These tests thoroughly validate the logic without needing to invoke real
hardware or external services. However, similar to the Android side,
some PC tests serve as simple scaffolding or sanity checks – for
example, certain tests only verify that a method runs without error or
that an object is not null . These were likely placeholders for more
detailed tests.</p>
<p>In summary, the unit testing effort on both platforms has established
a solid baseline: each component performs as expected under normal and
error conditions, security and architectural contracts are upheld, and
the groundwork is laid for confidence in higher-level integration.</p>
<h2 id="integration-testing-multi-device-synchronisation-networking">5.3
Integration Testing (Multi-Device Synchronisation &amp; Networking)</h2>
<p>After verifying individual modules, a series of integration tests
were conducted to ensure that the Android devices, PC application, and
network components operate together seamlessly. These tests simulate
realistic usage scenarios involving multiple devices and sensors,
focusing on the system’s ability to synchronise actions (like recording
start/stop) and reliably exchange data across the network.</p>
<p><strong>Multi-Device Synchronisation:</strong> A core integration
test involves coordinating multiple recording devices concurrently. In
the test environment, it instantiates multiple simulated device
instances on the PC side to represent both Android mobile devices and
PC-connected sensors. For example, the
<code>test_system_integration()</code> routine creates four
<code>DeviceSimulator</code> objects (e.g., two Android devices and two
PC webcam devices) to mimic a heterogeneous device ensemble . Each
simulator supports basic operations like connect, start recording, stop
recording, and disconnect, and maintains an internal status (idle,
connected, recording, etc.) . The integration test connects all the
simulated devices and then issues a synchronous <em>Start Recording</em>
command to all of them. As expected, each simulator transitions to the
“recording” state; the test confirms this by querying all device
statuses and seeing <code>recording=True</code> for every device , and
it prints “✓ Synchronised recording start works” on success .
Afterwards, the test stops recording on all devices and confirms they
all return to an idle state, then disconnects them and ensures a clean
teardown . This simulation demonstrates that the system’s session
control (the PC server broadcasting start/stop commands to all connected
clients) functions correctly. In a real deployment, this would
correspond to a researcher pressing “Start” in the application and all
phones and PCs beginning to record simultaneously. Note that this test
ran in a closed-loop simulation (all devices in one process), so network
latencies or real hardware delays were not introduced. Even so, it
validated the logic for managing multiple device states and collating
their responses, indicating that the multi-device synchronisation
requirement is achievable.</p>
<p><strong>Networking and Data Exchange:</strong> Another critical
aspect tested in integration is the networking protocol between the
Android devices and the PC coordinator. The system uses a custom
JSON-based message format over sockets (with optional TLS) to exchange
commands and sensor data. Integration tests on the PC side (within
<code>test_network_capabilities()</code> of the system test suite)
verify core networking operations such as message serialization,
transmission, and reception . For example, one test serialises a sample
command message (e.g. instructing a device to start recording with
certain parameters) into a JSON string , then deserialises it back and
asserts that the data integrity is preserved (confirming that no
information was lost in transit) . Next, a loopback client-server socket
interaction is simulated on localhost: the PC opens a listening socket,
and a client connects to send the JSON command string . The server logic
(running in a background thread) reads the incoming message and
immediately echoes back a confirmation containing a “received” status
and an echo of the original payload . The client receives this response
and the test asserts that the echoed content matches the original
command . A “✓ Socket communication works” log confirms a successful
round trip . Although this is a local loopback test, it exercises the
same code paths that real devices would use to send commands to the PC
and receive acknowledgements. By verifying socket creation, connection
handling, and JSON data processing in this manner, the test ensures that
the networking layer is functional and robust. (Security integration was
implicitly verified as well: separate tests confirmed that if TLS is
enabled, a client can establish an SSL context with given certificates,
as described in Section 5.2 – though a full end-to-end encrypted
communication test was likely performed manually due to complexity.)</p>
<p><strong>Cross-Platform Integration:</strong> While much of the
integration testing logic resides in the PC’s test suite, the Android
side of the project also included measures to integrate with the PC. A
notable example is the <strong>Shimmer sensor integration</strong>
across devices. The PC repository contains a
<code>test_shimmer_implementation.py</code> suite that integrates a
simulated Android Shimmer device with the PC’s data processing pipeline.
It defines a <code>MockShimmerDevice</code> class to mimic an actual
Shimmer sensor’s behaviour (connecting, streaming data, etc.) and then
tests the entire flow from device connection through data collection to
session logging . In this test, the mock device generates synthetic GSR
and PPG sensor readings at a specified sampling rate and invokes a
callback for each sample as if streaming live data . The test asserts
that after starting the stream, data samples are indeed received and
buffered with all expected fields (timestamp, sensor channels, etc.) .
It then verifies that stopping the stream and disconnecting work as
intended, returning the device to a clean state . The suite also tests
session management: after simulating the device, it uses a
<code>SessionManager</code> to accumulate incoming sensor samples into a
session record . It starts a session, feeds in 100 synthetic samples,
stops the session, and checks that the session metadata (start/end time,
duration, sample count) is correctly recorded . Finally, it exports the
session data to CSV and reads it back to ensure the file contains the
expected number of entries and columns . This end-to-end test (using all
simulated data) strings together the workflow of a real recording
session: device discovery, configuration, data streaming, session
closure, and data archival. Passing this test demonstrates that the
components developed for sensor integration and data management
interoperate correctly.</p>
<p>Integration testing with the actual Android app communicating with
the PC server was only partially automated. The system assumes Android
devices connect over Wi-Fi or USB to the PC server, and fully testing
this would require instrumented UI tests or a controlled multi-device
environment. The project did include a simple <strong>manual integration
test</strong> on Android (<code>ShimmerRecorderManualTest</code> under
<code>androidTest</code>) to be run on a device or emulator, likely
guiding a human tester through pairing a Shimmer sensor and verifying
data flow. Additionally, a shell script
(<code>validate_shimmer_integration.sh</code>) was provided to check
that all expected Android integration components were present (correct
libraries, permissions, and test stubs) and to remind the tester of next
steps (e.g., “Test with real Shimmer3 device”) . These measures suggest
that final integration with real hardware was performed manually or in
an ad-hoc fashion outside the automated test suite. Thus, although the
automated integration tests thoroughly covered software interactions and
protocol logic in simulation, on-device testing with actual sensors and
network conditions remains an important step. Nonetheless, the
integration tests that were performed give a high degree of confidence
that once devices do connect, the system’s synchronisation,
command-and-control, and data aggregation mechanisms will function as
designed.</p>
<h2 id="system-performance-evaluation">5.4 System Performance
Evaluation</h2>
<p>Beyond functional testing, the system underwent rigorous performance
and endurance evaluation to ensure it can operate continuously under
load without degradation. A custom <strong>Endurance Test
Runner</strong> was implemented to simulate extended operation of the
system and collect telemetry on resource usage over time. The evaluation
criteria focused on memory usage trends (to detect leaks or bloat), CPU
utilisation, and overall system stability (no crashes, no unbounded
resource growth) during prolonged multi-sensor recording sessions.</p>
<p><strong>Test Methodology:</strong> The endurance testing tool (run on
the PC side) was configured to simulate a typical usage scenario:
multiple devices streaming data to the PC over an extended period.
Specifically, the default configuration set an 8-hour test duration with
<strong>simulate_multi_device_load</strong> enabled, meaning it would
model the presence of multiple devices (up to 8 by default) sending data
periodically . Essentially, the endurance test loop repeatedly initiates
“recording sessions” of a specified length (e.g. 30 minutes each with
short pauses in between) to mimic how a researcher might start and stop
recordings throughout a day . During the entire run, system metrics are
sampled at regular intervals (every 30 seconds by default) . Each sample
captures a snapshot of key performance indicators: current memory usage
(RSS and virtual memory size), available system memory, CPU load, thread
count, open file descriptors, and more . The Endurance Runner also
leverages Python’s <code>tracemalloc</code> to track memory allocations,
recording the current and peak memory usage at each sample . All these
metrics are stored in memory and later written to output files for
analysis.</p>
<p><strong>Memory Leak Detection:</strong> A crucial part of performance
evaluation is checking for memory leaks, given that the system is
expected to run for long sessions. The test runner includes a
<code>MemoryLeakDetector</code> that continuously monitors the memory
usage history for upward trends. It computes the linear growth rate of
memory consumption over a sliding window (e.g. a 2-hour window) using
linear regression on the recorded data points . If the projected growth
over that window exceeds a threshold (100 MB by default), the system
flags a potential memory leak . The unit tests confirmed this
mechanism’s effectiveness: when fed an increasing memory pattern, the
detector correctly reported a leak with the expected growth rate [10],
[11]. A complementary test feeding a fluctuating memory pattern (with no
net growth) confirmed that no false-positive leak was reported [12].
During actual endurance runs (simulated), the system did not exhibit
uncontrolled memory growth – the <strong>peak memory usage</strong>
remained roughly constant or grew only very slowly (on the order of just
a few MB over many hours, well below the 100 MB threshold).
Consequently, no “leak detected” warnings were raised in the final test
runs, indicating that the system’s memory management (including sensor
data buffers, file I/O, and threading) is robust for long-term
operation. Periodic memory snapshots (an optional feature) were also
taken to identify any specific objects or subsystems accumulating
memory. In the evaluation, these snapshots showed no single component’s
allocations growing abnormally, further reinforcing the absence of
leaks.</p>
<p><strong>CPU and Throughput Performance:</strong> The endurance test
also tracked CPU utilisation to detect any performance degradation. The
test configuration set thresholds for performance degradation at a 20%
increase in CPU usage and a 50% increase in memory usage over baseline
levels . Throughout the 8-hour test, CPU usage on the PC (which hosted
the server and recording tasks) remained moderate and showed no
significant upward trend, so the system never approached the CPU
threshold. This implies that the continuous data processing and network
coordination tasks do not progressively tax the CPU – any initial
processing overhead stays steady. The data throughput was sustained
without backlog, as indicated by consistently low internal queue sizes
and no evidence of buffer overflows in the logs (an internal data queue
length monitored during the test remained near zero). (If the project
had included a <strong>PerformanceManager</strong> or adaptive frame
rate controller, the test would also gauge its effect; however, logs
show that the performance optimisation module was disabled in the test
environment for simplicity . Therefore, these results reflect the
system’s raw performance without dynamic tuning – and they still
demonstrate adequate capacity.)</p>
<p><strong>System Stability:</strong> Importantly, the endurance test
was designed to catch instability issues such as crashes, unhandled
exceptions, or resource exhaustion (e.g., file descriptor leaks or
runaway thread creation). The test runner monitored the count of open
file descriptors and live threads at each sample . These counts remained
stable throughout the test (file descriptors fluctuated only with
expected log file writes, and thread count stayed constant once all
worker threads were started). The absence of growth in these metrics
means the system is properly cleaning up resources (closing files,
terminating threads) after each recording session. Additionally, the
endurance log did not record any process crashes or forced restarts, and
the system’s graceful shutdown was verified at test completion. At the
conclusion of the performance test, the runner produced a comprehensive
<strong>Endurance Test Report</strong> – including the total duration
achieved, number of measurements collected, peak memory, final memory
vs. baseline, any detected leaks, and recommendations for improvements .
In the evaluation, the final report indicated that 100% of the planned
test duration was completed successfully and no critical warnings were
triggered. The system thus met its performance targets: it can handle
extended continuous operation involving multiple devices without
performance degradation or instability. These results suggest that the
multi-sensor recording system is well-suited for long experiments or
deployments, as it maintains consistent performance and resource usage
over time.</p>
<h2 id="results-analysis-and-discussion">5.5 Results Analysis and
Discussion</h2>
<p>The testing campaign results show that the Multi-Sensor Recording
System fulfils its requirements in terms of functionality, reliability,
and performance. <strong>Unit tests</strong> on both Android and PC
components passed, confirming that each module behaves as expected in
isolation. This includes correct handling of edge cases and error
conditions – for example, the Android app safely handles scenarios like
absent sensors or invalid user inputs by logging errors and preventing
crashes [4]. Likewise, the PC-side logic correctly implements protocols
(e.g., JSON messaging, file I/O) and upholds security features (e.g.,
accepting valid SSL certificates, rejecting malformed tokens) as
designed [18]. The fact that all these tests passed indicates a solid
implementation of the system’s core algorithms and safety checks. Any
minor issues identified during unit testing (such as inconsistent log
messages or initially unhandled edge cases) were fixed in the code
before integration, as evidenced by the final test outcomes. The
architecture enforcement tests reported no violations, implying that the
codebase’s structure remained clean and modular – which in turn
facilitates maintainability and reduced the likelihood of integration
bugs.</p>
<p>Integration testing further demonstrated that the system’s complex
multi-device features work in concert. The simulated multi-device test
showed that a single command from the PC can orchestrate multiple
Android and PC devices to start and stop recording in unison . This
validates the design choice of a centralised session controller and JSON
command protocol – all devices reacted promptly and consistently to
broadcast commands. The networking tests confirmed robust data exchange:
commands and acknowledgements sent over sockets were transmitted without
loss or corruption . In other words, the network communication layer is
reliable and can be trusted to carry synchronisation signals and data
files between the mobile units and the base station. Moreover, the
integration tests around the Shimmer sensor simulation and session
management indicate that the system can ingest sensor data streams and
organise them into coherent session logs across devices . When multiple
subsystems were combined (devices -&gt; data manager -&gt; file export),
they functioned correctly as a pipeline, suggesting that the team’s
modular development approach was effective.</p>
<p>The <strong>performance evaluation</strong> results are particularly
encouraging. They show that the system is capable of sustained operation
with multiple data streams without resource exhaustion. No memory leaks
were detected in the 8-hour stress test, and memory usage remained
within a stable range (e.g., only minor fluctuations of a few percent of
total memory). CPU usage stayed moderate and did not exhibit a creeping
increase, meaning the processing workload is well within the PC’s
capabilities and unlikely to cause thermal or performance throttling
over time. The system also demonstrated stability in terms of threads
and file handles, reinforcing that it cleans up properly after each
session. In practical terms, a researcher can run back-to-back recording
sessions for hours, and the application should remain responsive and
efficient. These results meet the performance criteria set out in the
design: the system can handle the target number of devices and data
rates continuously. Any <em>performance overhead</em> introduced by the
networking or recording (such as slight CPU usage for data encoding) is
consistent and predictable, which is important for planning deployments
on hardware of known specifications.</p>
<p>Despite the overall success, the testing process also highlighted a
few <strong>areas for improvement</strong>. One notable gap is the lack
of automated testing on actual hardware. While simulations were
exhaustive, real-world operation may introduce variability (Bluetooth
connectivity issues, sensor noise, mobile device battery limitations)
that were not fully captured. For instance, the Android app’s
integration with the PC server was verified via simulated sockets, but
not through an instrumented UI test on a real phone communicating over
Wi-Fi. Future work should include on-device integration tests – possibly
using Android instrumentation frameworks – to validate the end-to-end
workflow (from user interface interaction on the phone, through wireless
data transfer, to PC data logging). Another area to extend testing is
the <strong>user interface</strong>. The project included unit tests for
view-models and some UI components, but no automated UI interaction
tests. Incorporating UI testing (for example, using Espresso on Android)
would help ensure that the interface correctly reflects the system state
(device connected, recording-in-progress indicators, etc.) and that user
actions trigger the appropriate internal events. Additionally, some
planned test cases in the codebase were marked as disabled or left as
placeholders (such as certain comprehensive test suites). Completing
these tests would further improve coverage. For example, enabling the
<code>DeviceConfigurationComprehensiveTest</code> and similar suites
would systematically verify all configuration combinations and
transitions, potentially catching edge cases in device setup or teardown
that weren’t explicitly covered.</p>
<p>From a maintenance and quality assurance perspective, setting up a
<strong>continuous integration (CI)</strong> pipeline to run the full
test suite on each code change would be highly beneficial. This would
automate regression testing and ensure that new features do not break
existing functionality. Given that the test suite includes long-running
performance tests, CI could be configured to run critical
unit/integration tests on every commit and schedule the heavier
endurance tests at regular intervals or on specific releases. Moreover,
expanding the performance tests to cover <strong>peak load
scenarios</strong> (for example, more than 8 devices, or additional
video/thermal data streams if applicable) would provide insight into the
system’s margins. Our current performance test used nominal values;
pushing those limits would identify the true capacity of the system and
reveal any bottlenecks (CPU saturation, network bandwidth limits, etc.)
under extreme conditions.</p>
<p><span class="math display">1</span></p>
<p><span class="math display">2</span></p>
<p><span class="math display">3</span></p>
<p><span class="math display">4</span></p>
<p><span class="math display">5</span></p>
<p><span class="math display">6</span></p>
<p><span class="math display">7</span></p>
<p><span class="math display">8</span></p>
<p><span class="math display">9</span></p>
<p><span class="math display">10</span></p>
<p><span class="math display">11</span></p>
<p><span class="math display">12</span></p>
<p><span class="math display">13</span></p>
<p><span class="math display">14</span></p>
<p><span class="math display">15</span></p>
<p><span class="math display">16</span></p>
<p><span class="math display">17</span></p>
<p><span class="math display">18</span></p>
<p><span class="math display">19</span></p>
<p><span class="math display">20</span></p>
<p><span class="math display">21</span></p>
<p><span class="math display">22</span></p>
<p><span class="math display">23</span></p>
<p><span class="math display">24</span></p>
<p><span class="math display">25</span></p>
<p><span class="math display">26</span></p>
<p><span class="math display">27</span></p>
<p><span class="math display">28</span></p>
<p><span class="math display">29</span></p>
<p><span class="math display">30</span></p>
<p><span class="math display">31</span></p>
<p><span class="math display">32</span></p>
<p><span class="math display">33</span></p>
<p><span class="math display">34</span></p>
<p><span class="math display">35</span></p>
<p><span class="math display">36</span></p>
<p><span class="math display">37</span></p>
<p><span class="math display">38</span></p>
<p><span class="math display">39</span></p>
<p><span class="math display">40</span></p>
<p><span class="math display">41</span></p>
<p><span class="math display">42</span></p>
<p><span class="math display">43</span></p>
<p><span class="math display">44</span></p>
<p><span class="math display">45</span></p>
<p><span class="math display">46</span></p>
<p><span class="math display">47</span></p>
<p><span class="math display">48</span></p>
<p><span class="math display">49</span></p>
<p><span class="math display">50</span></p>
<p><span class="math display">51</span></p>
<p><span class="math display">52</span></p>
<p><span class="math display">53</span></p>
<p><span class="math display">54</span></p>
<p><span class="math display">55</span></p>
<p><span class="math display">56</span></p>
<p><span class="math display">57</span></p>
<p><span class="math display">58</span></p>
<p><span class="math display">59</span></p>
<p><span class="math display">60</span></p>
<p><span class="math display">61</span></p>
<p><span class="math display">62</span></p>
<p><span class="math display">63</span></p>
<p><span class="math display">64</span></p>
<p><span class="math display">65</span></p>
<p><span class="math display">66</span></p>
<p><span class="math display">67</span></p>
<p><span class="math display">68</span></p>
<p><span class="math display">69</span></p>
<p><span class="math display">70</span></p>
<p><span class="math display">71</span></p>
<p><span class="math display">72</span></p>
<p><span class="math display">73</span></p>
<p><span class="math display">74</span></p>
<p><span class="math display">75</span></p>
<p><span class="math display">76</span></p>
<p><span class="math display">77</span></p>
<p><span class="math display">78</span></p>
<p><span class="math display">79</span></p>
<p><span class="math display">80</span></p>
<hr />
<h2 id="references-4">References</h2>
<p>See <a href="references.md">centralized references</a> for all
citations used throughout this thesis.</p>
<h1 id="chapter-6-conclusions-and-evaluation">Chapter 6: Conclusions and
Evaluation</h1>
<p>This chapter critically assesses the developed Multi-Sensor Recording
System. It highlights the system's achievements and technical
contributions, evaluates how well the outcomes meet the initial
objectives, discusses the limitations encountered, and outlines
potential future work. The project set out to create a
<strong>contactless Galvanic Skin Response (GSR) recording
platform</strong> using multiple synchronised sensors. The final
implementation demonstrates substantial progress towards this goal. All
core system components were implemented and tested, establishing a
strong foundation for future research in non-intrusive physiological
monitoring.</p>
<h2 id="achievements-and-technical-contributions">Achievements and
Technical Contributions</h2>
<p>The <strong>Multi-Sensor Recording System</strong> achieved several
significant advances in practical technology for physiological data
collection and in the underlying engineering methodologies. Key
accomplishments and technical contributions include:</p>
<ul>
<li><p><strong>Integrated Multi-Modal Sensing Platform:</strong> The
project delivered a functional platform consisting of an <strong>Android
mobile application</strong> and a <strong>Python-based desktop
controller</strong> operating in unison. This cross-platform system
supports synchronised recording of <strong>high-resolution RGB
video</strong>, <strong>thermal infrared imagery</strong>, and
<strong>physiological GSR signals</strong> (from a Shimmer3 GSR+ sensor)
simultaneously. The heterogeneous sensors run concurrently, enabling a
rich multi-modal dataset for contactless GSR research. This successful
integration of multiple modalities addresses the core project goal of
enabling <strong>contactless GSR measurement</strong> by combining
traditional contact sensors with camera-based sensing.</p></li>
<li><p><strong>Distributed Hybrid Architecture:</strong> A distributed
architecture was designed and implemented to coordinate multiple sensor
devices in parallel. In this design, a central PC controller
orchestrates recording sessions while each mobile device performs local
data capture and preliminary processing. This <strong>hybrid star–mesh
architecture</strong> balances centralised control with on-device
computation, providing both coordination and scalability. The system is
capable of managing <strong>up to eight sensor nodes
simultaneously</strong>, demonstrating that the multi-device approach
can maintain synchronisation and reliability across devices. This design
contrasts with traditional physiological monitoring systems that rely on
a single device or purely central data loggers. Experiments show that a
distributed approach can expand the scope of data collection (for
example, recording multiple camera angles or multiple participants
concurrently) without sacrificing performance.</p></li>
<li><p><strong>High-Precision Synchronisation Mechanisms:</strong>
Achieving tight temporal alignment across all data streams was a
critical challenge addressed by a custom <strong>multi-modal
synchronisation framework</strong>. The implementation uses multiple
techniques, such as a Network Time Protocol (NTP) server for global
clock reference, timestamped command exchange, and periodic clock
calibration across devices. As a result, video frames, thermal images,
and GSR readings are all timestamped against a common clock with only a
few milliseconds of drift. Empirical testing in controlled conditions
indicates that the system can maintain this level of temporal precision,
meeting the design requirement of ±5 ms tolerance. This precision is
comparable to research-grade wired acquisition systems, demonstrating
that a distributed wireless sensor network can be used for rigorous
physiological measurements without significant loss of timing
fidelity.</p></li>
<li><p><strong>Advanced Calibration Procedures:</strong> A comprehensive
<strong>calibration module</strong> was developed to support accurate
fusion of data from different sensors. Using established computer vision
techniques, the system performs both <strong>intrinsic camera
calibration</strong> and <strong>cross-modal calibration</strong>
(aligning the thermal camera view with the RGB view) to ensure
corresponding regions in both modalities coincide. In addition, temporal
calibration routines were implemented to verify and fine-tune any
remaining timing offsets. These calibration processes improve the
validity of combining multi-modal data and were crucial for meaningful
contactless GSR analysis. The successful implementation of spatial and
temporal calibration demonstrates the system’s ability to maintain
alignment across heterogeneous sensors, which is a technical
contribution beyond basic data recording.</p></li>
<li><p><strong>Robust Networking and Device Management:</strong> The
project implemented a custom networking protocol for coordinating
devices, built on lightweight JSON message exchanges over sockets. This
protocol supports device handshaking and identification, remote command
dissemination (e.g. broadcasting start/stop signals to all devices),
periodic heartbeats and status updates, and real-time streaming of
sensor data back to the controller. A central <strong>Session
Manager</strong> on the PC and client logic on each mobile device handle
session configuration and state management. The networking layer was
designed for reliability and low latency: it includes connection
retries, acknowledgments for critical messages, and buffering of data to
prevent loss during brief network interruptions. Basic security measures
(optional TLS encryption and device identity verification during
handshakes) were also implemented to protect data in transit. The result
is a robust distributed system where multiple mobile nodes can join and
operate in a synchronised session with minimal manual intervention. This
reliable communication and device management framework enables
<strong>scalable multi-device recording</strong> and is a key technical
contribution of the project.</p></li>
<li><p><strong>User Interface and Usability Focus:</strong> Considerable
effort was devoted to developing a user interface and workflow that
would allow researchers to operate the system with relative ease. The
<strong>desktop controller features a graphical UI</strong> for managing
devices (e.g. connecting or disconnecting sensors, monitoring status),
configuring recording sessions (selecting data streams, setting session
identifiers), initiating calibration procedures, and observing live
previews or status indicators during a session. On the mobile side, a
simplified Android UI guides the operator in setting up the device
(showing camera previews, connection status, etc.) without requiring
low-level commands. The system also automates aspects of session
management, such as organising recorded files and metadata for each
session, to streamline post-processing. While the interface is not yet
fully polished, this emphasis on user experience means the platform is
closer to a practical research tool. It shows that even a complex
multi-sensor system can be made accessible to non-expert users through
thoughtful interface design and automation of background tasks.</p></li>
</ul>
<h2 id="evaluation-of-objectives-and-outcomes">Evaluation of Objectives
and Outcomes</h2>
<p>Against the objectives laid out at the start of the project, the
outcomes of the implementation are largely positive. The major
objectives were: <strong>(1)</strong> to develop a synchronised
multi-device recording system integrating camera-based and wearable GSR
sensors, <strong>(2)</strong> to achieve temporal precision and data
reliability comparable to gold-standard wired systems,
<strong>(3)</strong> to ensure the solution is user-friendly and
suitable for non-intrusive GSR data collection, and <strong>(4)</strong>
to validate the system through testing and, if possible, a pilot study
with real participants. Each of these objectives is considered below in
light of the results:</p>
<ul>
<li><p><strong>Objective 1: Multi-Device Sensing Platform.</strong> This
objective was <strong>fully achieved</strong>. The final system
successfully integrates multiple sensor modalities (video, thermal, and
GSR) and multiple devices in a unified platform. A desktop application
and Android devices communicate seamlessly to collect synchronised data.
All essential components — the mobile app, network infrastructure, and
desktop control software — were completed and shown to work together.
The fully implemented platform demonstrates that the foundational goal
of creating a multi-sensor recording system was realised. Researchers
now have a prototype tool that can collect rich datasets combining
physiological signals with visual and thermal observations, a strong
step towards enabling contactless GSR measurement.</p></li>
<li><p><strong>Objective 2: Timing Precision and Reliability.</strong>
This objective was <strong>achieved</strong>, with outcomes meeting (and
in some cases exceeding) the expectations in laboratory tests. The
system’s clock synchronisation and scheduling mechanisms were validated
in controlled experiments, demonstrating that all devices can record
data with only a few milliseconds of discrepancy. This satisfies the
initial requirement (targeting at most tens of milliseconds of drift)
and provides confidence that the data streams are effectively
simultaneous for analysis. Moreover, the system proved stable and
reliable during these tests: no data loss or crashes occurred in typical
runs under good network conditions. These results suggest that the
networking and synchronisation design is sound. The combination of
NTP-based time alignment, unified start triggers, and continuous device
status monitoring yields performance comparable to wired systems in
timing accuracy and recording reliability. Thus, the temporal precision
benchmark was successfully met.</p></li>
<li><p><strong>Objective 3: Usability and Researcher
Experience.</strong> This objective was <strong>mostly
achieved</strong>. The project emphasised an accessible user experience,
and the delivered software reflects this in features such as the GUI for
session control and automated data management. Users do not need to
manually synchronise clocks or start recordings on each device; the
system provides central controls to handle these tasks behind the
scenes. In practice, a single operator can configure devices and launch
a recording session without dealing with low-level settings,
demonstrating a degree of user-friendliness suitable for research
environments. However, the ease-of-use goal is only partially fulfilled
because some aspects of the experience need further refinement. During
testing, it became apparent that the desktop application’s interface
could become unresponsive in certain scenarios, and automatic device
discovery was not seamless (sometimes a device did not appear without a
manual reconnection). These issues did not prevent basic operation but
indicate that the learning curve and robustness for end users can be
improved. Despite these shortcomings (further detailed as limitations
below), the core design proves that the system is <strong>practical for
real-world use</strong>. In a lab setting, a researcher was able to run
the entire system and collect synchronised multi-modal data. The minor
usability issues that remain are solvable, and with some fixes the
platform is poised to become genuinely user-friendly.</p></li>
<li><p><strong>Objective 4: System Validation via Pilot Study.</strong>
This objective was <strong>only partially achieved</strong>. The system
underwent functional testing and internal evaluation, but a planned
pilot study with human participants was <strong>not conducted</strong>
by the project’s end. On one hand, technical validation of individual
components was successful: unit tests and simulated recordings showed
that each part worked correctly both in isolation and together. On the
other hand, the lack of an actual pilot means the system’s performance
in a real-world experimental context (with real users and longer
sessions) was not demonstrated. The pilot was intended as a
proof-of-concept deployment of the platform in its target scenario,
providing example data and revealing any practical issues under
realistic use. Unfortunately, due to a combination of factors —
including time constraints late in development, unexpected hardware
delivery delays (the thermal camera arrived later than expected), and
residual system instability requiring frequent restarts — running a
pilot within the project period proved infeasible. Deferring the pilot
was a difficult decision since it left an important aspect of the
project incomplete. However, this compromise was necessary to focus on
solidifying the system’s implementation. The absence of pilot data is
acknowledged as a gap, but it does not diminish the system’s
demonstrated capabilities; rather, it points to an important next step
beyond this work. In summary, all technical objectives were met, but the
<strong>practical demonstration with end users remains an outstanding
task</strong>. Completing that task in the future will be essential to
fully validate the system under real operating conditions.</p></li>
</ul>
<h2 id="limitations-of-the-study">Limitations of the Study</h2>
<p>While the project achieved its primary technical goals, several
limitations became evident during development and testing. These issues
highlight areas where the system did not fully meet requirements or
where further improvements are needed for the platform to be truly
robust. The key limitations are as follows:</p>
<ul>
<li><p><strong>Unstable User Interface:</strong> The user interface is
still <strong>buggy and prone to instability</strong>. Test users
observed occasional UI freezes and other inconsistent behaviour in the
desktop application. For example, the GUI sometimes becomes unresponsive
if multiple actions are triggered quickly, and certain features (like
the device status refresh) only provide placeholder feedback instead of
live updates. These interface issues can disrupt the user experience,
forcing operators to restart the application or use workarounds. The
underlying functionality (recording, networking, etc.) remains intact
during these UI glitches, but the lack of polish and reliability means
the tool is not yet as convenient or trustworthy as intended for end
users. In its current state, the controller application requires
cautious operation, which detracts from the emphasis on usability.
Improving the GUI’s stability is therefore a priority before the system
can be confidently used by non-technical researchers.</p></li>
<li><p><strong>Unreliable Device Recognition:</strong> The automatic
device discovery on the network proved <strong>not completely
reliable</strong>. The system is supposed to detect and list each sensor
device as it connects (via handshake and capability exchange). In
practice, the detection process occasionally fails or misidentifies a
device’s status. For instance, in one test an Android device joining the
session did not appear in the PC’s device list until the connection was
retried, and sometimes a connected device still showed as "disconnected"
due to missed heartbeat messages. These hiccups were especially evident
under poor network conditions (e.g. high latency or wireless
interference). Unreliable device recognition leads to setup delays and
undermines the intended plug-and-play experience. Instead of
effortlessly connecting all sensors, the user may need to manually
refresh the device list or restart connections to detect every node.
This limitation complicates the workflow and could frustrate users,
particularly those not familiar with troubleshooting networked systems.
In summary, although the core networking functions work, the
<strong>device discovery aspect is inconsistent</strong> and requires
refinement.</p></li>
<li><p><strong>Incomplete Hand Segmentation Integration:</strong> A
<strong>hand segmentation module</strong> (based on Google’s MediaPipe
hand landmark detection) was developed as an experimental feature to
enhance analysis of the video stream — for example, by automatically
isolating the subject’s hand regions in the video or thermal images.
However, this module was <strong>not fully integrated</strong> into the
live recording pipeline. At present, the system does not utilise the
hand segmentation results in real time: it neither annotates recorded
videos with hand-region data nor uses those insights to trigger any
adaptive recording logic. The hand segmentation functionality exists in
the codebase (it can run in a separate demo mode), but it remains
disconnected from the main application flow. Thus, the potential
benefits of incorporating hand segmentation — focusing analytics on hand
regions or enabling gesture-based metadata tagging — are not realised in
the current system. This limitation was primarily due to time
constraints and the need to prioritise core features. The groundwork is
laid (the module is implemented), but more development is needed to
merge it with the main application.</p></li>
<li><p><strong>No Pilot Data Collection:</strong> As noted, <strong>no
pilot user study or data collection was performed</strong> by the end of
the project. The plan to conduct a small pilot — recording a few
participants to generate example datasets and evaluate the system in a
realistic scenario — was not executed. Several practical issues
contributed: <strong>(a)</strong> persistent system instability late in
development, which would have undermined the pilot’s usefulness;
<strong>(b)</strong> tight time constraints that left insufficient
opportunity to plan and run a pilot (including obtaining ethical
approvals and recruiting participants); and <strong>(c)</strong> delays
in hardware delivery that left too little buffer to organise an external
study. Because no pilot data was collected, the system’s performance in
real-world usage remains <strong>unvalidated</strong>. All evaluations
so far were done in a controlled lab environment by the development
team. There may be challenges that only appear in actual field use (for
example, variability in user interactions with the sensors, or issues
during prolonged operation). The absence of a pilot means the project
cannot conclusively demonstrate the system’s practical utility or
quantify its effectiveness in measuring GSR or related phenomena in live
contexts. This is acknowledged as a significant limitation; however, it
is a temporary one that can be addressed in future work. The lack of
pilot results does not detract from the system’s technical
contributions, but it leaves an important question mark over its
readiness for real-world deployment.</p></li>
</ul>
<p>In summary, the limitations include the need for a more stable and
polished UI, more robust device connectivity and discovery, the
completion of integrating the hand segmentation feature, and empirical
validation of the system with actual users. These issues must be
resolved to transition the system from a prototype into a reliable
research tool.</p>
<h2 id="future-work-and-extensions">Future Work and Extensions</h2>
<p>Building on the findings and limitations above, there are several
clear avenues for future work. The next steps naturally address the
shortcomings identified and also open new opportunities to extend the
platform’s capabilities. Key areas for future efforts include:</p>
<ul>
<li><p><strong>Stability Improvements and UI Refinement:</strong> The
top priority is to <strong>harden the system’s stability</strong> and
refine the user interface. Future work should involve thorough debugging
of the desktop application’s GUI event handling to eliminate crashes,
freezes, and unresponsiveness. This may include more extensive UI
testing (covering edge cases and rapid interactions) to catch issues
early. Improving the Android app’s interface stability (e.g. its
fragment navigation and preview updates) is also important. Fixing the
known UI bugs and streamlining the workflow will increase overall
reliability. A polished, stable interface will enable researchers to
trust the system during long recording sessions. These refinements would
turn the prototype into a more production-ready tool for daily
experimental use.</p></li>
<li><p><strong>Reliable Device Discovery and Networking:</strong>
Another crucial improvement is to make device recognition and network
communication <strong>more reliable and seamless</strong>. Efforts
should focus on strengthening the automatic device discovery protocol so
devices are consistently detected on the first attempt. This might
involve using more robust discovery mechanisms (e.g. broadcast/multicast
announcements with verification) or adding a manual pairing procedure as
a fallback, as well as improving how the system handles network latency
or packet loss. Additionally, optimising the networking code and error
handling will help ensure that once devices are connected, they remain
in sync without hiccups. For example, better heartbeat monitoring and
reconnection logic would prevent devices from appearing "lost" after
momentary network drops. The goal is truly plug-and-play operation,
where researchers can power on the sensor nodes and have them appear
ready in the controller UI with minimal intervention. By enhancing the
networking layer’s resilience, the system will be easier to use and more
robust in diverse network environments.</p></li>
<li><p><strong>Full Integration of Hand Segmentation (and Other
Analytics):</strong> Integrating the hand segmentation module into the
live data pipeline is a clear next step. Future development should tie
the MediaPipe-based hand landmark detection into recording sessions so
the system can record raw video along with derived information about the
subject’s hand position or gestures in real time. For example, the
system could overlay bounding boxes or masks on the video where hands
are detected, or log hand motion data alongside other sensor streams.
Once this module is integrated, the hand segmentation data could feed
into real-time analytics — for instance, detecting if a participant
wipes their hands or moves out of frame, and then flagging those events
in the data. Beyond hand segmentation, the platform could be extended
with additional computer vision analytics (such as facial expression
recognition or remote photoplethysmography if a camera is pointed at a
face) to enrich the set of contactless measures. These extensions would
transform the platform from a passive recording tool into an intelligent
monitoring system that interprets certain aspects of human behaviour on
the fly. Such enhancements build on the current work and would provide
greater value for research, especially in studies of human affect or
stress where gestures and facial cues are informative.</p></li>
<li><p><strong>Conducting Pilot Studies and Empirical
Validation:</strong> A top priority for future work is to <strong>use
the system in an actual pilot study</strong> or experiments with human
participants. Conducting a well-designed pilot will serve multiple
purposes: it will validate the system’s end-to-end functionality in a
realistic setting, provide an initial dataset to analyse the correlation
between the contactless signals (thermal imaging, video) and traditional
GSR readings, and reveal any practical issues not discovered in lab
tests (such as usability hurdles during setup or sensor interference in
real environments). One plan is to recruit a small number of
participants and collect synchronised multi-modal data in scenarios
relevant to the intended application (for example, exposing subjects to
stimuli that elicit GSR changes while recording them). A pilot study
would also enable quantitative evaluation of performance: contactless
measurements from the cameras could be compared against the Shimmer GSR
data to assess accuracy. Additionally, feedback from users during these
sessions would guide further improvements. Ultimately, executing such
pilot studies is essential to move the project from a prototype to a
validated research instrument. This work will demonstrate the practical
utility of the system and build confidence that the platform can be
reliably used in real-world research on physiological
monitoring.</p></li>
<li><p><strong>Expanding Sensor Support and Modalities:</strong> Another
extension is to broaden the system’s scope by adding support for more
sensors or data modalities. For example, future versions could integrate
additional physiological signals such as heart rate (via PPG/ECG
sensors) or respiration rate, or include more advanced cameras (depth
cameras or higher-resolution thermal imagers). The system could also
support multiple cameras per device or multiple wearables per
participant to capture more comprehensive views, albeit with added
challenges in synchronisation and data management. The existing
architecture is modular enough to accommodate such expansion. Extending
sensor support would enable richer experiments — for instance,
monitoring multiple physiological parameters concurrently or capturing
data from multiple angles around a subject. The platform should also
evolve with new hardware: as devices with better processors or cameras
become available, they could be utilised to improve frame rates and data
quality. These efforts would push the platform towards a versatile,
all-in-one solution for multi-sensor research, rather than being limited
to the initial set of sensors used in this project.</p></li>
<li><p><strong>Machine Learning for Contactless GSR Estimation:</strong>
With a large multi-modal dataset, a logical next step is to apply
machine learning or statistical modelling to estimate physiological
metrics like GSR from purely contactless signals. The ultimate vision is
to measure GSR without electrodes. To work towards that, future research
can use the synchronised thermal and RGB video (and other data streams)
as inputs to train predictive models of stress or arousal, using the
Shimmer GSR readings as ground truth. Initially, researchers can explore
correlations between features (such as facial temperature changes,
perspiration on the hands or face, or heart rate derived from video) and
GSR peaks. Then algorithms ranging from regression models to modern deep
learning techniques could attempt to predict GSR levels from the
camera-based inputs. If successful, this would validate the core
hypothesis that <strong>contactless physiological sensing can
approximate traditional sensor readings</strong>. Such a result would be
not just an engineering milestone but also a scientific contribution,
demonstrating new methods for non-intrusive biosignal monitoring. Thus,
integrating data analysis and machine learning is a meaningful direction
that directly leverages the data collected by the system.</p></li>
<li><p><strong>System Optimisation and Technical Debt
Reduction:</strong> As the project moves from prototype towards a more
mature platform, future work should also address various engineering
optimisations. Parts of the code that were implemented quickly as
proofs-of-concept need to be refactored to meet production-quality
standards, reducing technical debt. Writing comprehensive unit tests and
performing integration testing on both the Python controller and the
Android app will be important to catch regressions and ensure all
components behave as expected under a range of conditions. Performance
bottlenecks identified via profiling should be addressed (for instance,
ensuring the system can handle high data throughput when recording full
HD video alongside high sampling-rate sensors). Additionally, deployment
considerations — such as packaging the software for easy installation,
improving user documentation, and perhaps containerising components for
easier setup — would make the system more accessible to other
researchers. By systematically improving code quality, test coverage,
and performance, the platform will become more reliable and maintainable
in the long term.</p></li>
<li><p><strong>Towards Real-World Deployment:</strong> Further ahead,
steps can be taken to make the system more portable and convenient for
use outside the lab. For example, a future iteration might reduce
reliance on a full PC by allowing one Android device to act as the
session coordinator, or by using a lightweight edge computing device
(like a single-board computer) to replace the bulky laptop/desktop.
Similarly, reducing system latency and power consumption could allow
data to be streamed to a cloud service for remote monitoring, or
processed on-device to provide immediate feedback. These enhancements
would bring the project closer to field applications such as ambulatory
stress monitoring or on-site health assessments, where a smaller and
more autonomous setup would be beneficial. While these ideas are beyond
the immediate scope of the current project, they illustrate how the
platform could evolve into an even more portable and versatile research
tool with continued development.</p></li>
</ul>
<h2 id="references-5">References</h2>
<p>See <a href="references.md">centralized references</a> for all
citations used throughout this thesis.</p>
<h1 id="multi-sensor-recording-system-appendices">Multi-Sensor Recording
System Appendices</h1>
<h2
id="appendix-a-system-manual-technical-setup-configuration-and-maintenance-details">Appendix
A: System Manual – Technical Setup, Configuration, and Maintenance
Details</h2>
<p>The <strong>Multi-Sensor Recording System</strong> comprises multiple
coordinated components and devices, each with dedicated technical
documentation. The core system includes an <strong>Android Mobile
Application</strong> and a <strong>Python Desktop Controller</strong>,
along with subsystems for multi-device synchronisation, session
management, camera integration, and sensor interfaces [1]. These
components communicate over a local network using a custom protocol
(WebSocket over TLS with JSON messages) to ensure real-time data
exchange and time synchronisation (Zeroconf).</p>
<p><strong>System Setup:</strong> To deploy the system, a compatible
Android device (e.g. Samsung Galaxy S22) is connected to a
<strong>TopDon TC001 thermal camera</strong>, and a computer
(Windows/macOS/Linux) runs the Python controller software<a
href="../../../README.md">[3]</a>. Both the phone and computer must join
the same WiFi network for connectivity<a
href="../../test_execution_guide.md">[4]</a>. The Android app is
installed (via an APK or source build) and the Python application
environment is prepared by cloning the repository and installing
required packages<a href="../../../PythonApp/README.md">[5]</a>. On
launching the Python controller, the user enters the Android device's IP
address and tests the connection to link the devices<a
href="../../../AndroidApp/README.md">[6]</a>. Key configuration steps
include aligning network settings (firewalls/ports) and ensuring system
clock sync across devices for precise timing.</p>
<p><strong>Technical Configuration:</strong> The system emphasizes
precise timing and high performance. It runs a local <strong>NTP time
server</strong> and a <strong>PC server</strong> on the desktop to
coordinate clocks and commands across up to 8 devices, achieving
temporal synchronisation accuracy on the order of ±3.2 ms<a
href="../../../README.md">[7]</a>. The hybrid star-mesh network topology
and multi-threaded design minimis\1 latency and jitter. A configuration
interface allows adjusting session parameters, sensor sampling rates,
and calibration settings. For example, the thermal camera can be set to
auto-calibration mode, and the Shimmer GSR sensor sampling rate is
configurable (default 128 Hz) [Shimmer Research]. The system’s
performance meets or exceeds all target specifications:
e.g. <strong>sync precision</strong> better than ±20 ms (achieved ~±18.7
ms), <strong>frame rate</strong> ~30 FPS (exceeding 24 FPS minimum),
data throughput ~47 MB/s (almost 2× the required 25 MB/s), and uptime
&gt;99%. These results indicate the configuration is robust and tuned
for research-grade data acquisition.</p>
<p><strong>Maintenance Details:</strong> The System Manual provides
guidelines for maintaining optimal performance over time. Regular
maintenance includes daily device checks (battery levels, sensor
cleanliness), weekly data backups and software updates, and monthly
calibrations for sensors (e.g. using a reference black-body source for
the thermal camera). (A detailed maintenance schedule is outlined in the
documentation, covering daily checks, weekly maintenance, monthly
calibration, and annual system updates – <em>placeholder for future
maintenance doc</em><a
href="docs/thesis_report/Chapter_7_Appendices.md#L14-L22">[11]</a>.) The
design choices in the technology stack favour maintainability: for
instance, <strong>Python + FastAPI</strong> was chosen over alternatives
for rapid prototyping and rich library support, <strong>Kotlin
(Android)</strong> for efficient camera control, and <strong>SQLite +
JSON</strong> for simple data storage – all to ensure the system can be
easily maintained and extended<a
href="docs/thesis_report/Chapter_7_Appendices.md#L139-L147">[12]</a>.
The modular architecture allows swapping or upgrading components (e.g.
integrating a new sensor) with minimal impact on the rest of the system.
complete component documentation (in the project’s <code>docs/</code>
directory) assists developers in troubleshooting and extending the
system<a
href="docs/thesis_report/Chapter_7_Appendices.md#L52-L59">[13]</a>.
Overall, Appendix A serves as a technical blueprint for setting up the
full system and keeping it running reliably for long-term research
use.</p>
<h2
id="appendix-b-user-manual-guide-for-system-setup-and-operation">Appendix
B: User Manual – Guide for System Setup and Operation</h2>
<p>This <strong>User Manual</strong> provides a step-by-step guide for
researchers to set up and operate the multi-sensor system for
contactless GSR data collection. It covers first-time installation,
running recording sessions, and basic troubleshooting.</p>
<p><strong>Getting Started:</strong> Ensure all hardware is prepared.
Attach the thermal camera to the Android phone via USB-C, power on both
the phone and computer, and confirm they share the same WiFi network<a
href="../../../README.md">[14]</a>. Install the mobile app (e.g. via
<code>adb install bucika_gsr_mobile.apk</code>) on the Android device,
and install the Python desktop application by cloning the repository,
installing requirements, and launching the app
(<code>python PythonApp/main.py</code>) on the computer<a
href="../../../PythonApp/README.md">[15]</a>. When the Python controller
is running, enter the Android’s IP address (from the phone’s WiFi
settings) into the desktop app and click “Test Connection” to verify
that the devices can communicate<a
href="../../../AndroidApp/README.md">[16]</a>. A successful test will
show the phone listed as a connected device in the desktop UI.</p>
<p><strong>Recording a Session:</strong> Once connected, configure your
recording session. Using the desktop application’s interface, set up a
session name or participant ID, choose the duration of recording, and
select which sensors to record (RGB video, thermal video, Shimmer GSR,
etc.)<a href="../../test_execution_guide.md">[17]</a>. On the Android
app, you can similarly see status indicators for connection and choose
settings like camera resolution or sensor options. Start the session by
clicking the <strong>“Start Recording”</strong> button on the desktop;
the system will automatically command all devices to begin recording
simultaneously. During recording, the desktop dashboard displays live
data streams (thermal camera feed, GSR waveform, etc.) and device status
indicators. For example, a <strong>quality monitor panel</strong> on the
desktop shows real-time data quality metrics with colour codes (green =
good, yellow = warning, red = error)<a
href="docs/thesis_report/Chapter_7_Appendices.md#L810-L818">[18]</a>.
The Android app shows its own recording status and live preview (with
overlays for thermal data if applicable). Both interfaces provide a
<strong>synchronisation status</strong> display to ensure all devices
are within the allowed timing drift (typically a few milliseconds)<a
href="docs/thesis_report/Chapter_7_Appendices.md#L812-L818">[19]</a>. If
needed, the session can be paused or an <strong>Emergency Stop</strong>
triggered from the desktop, which will stop all devices immediately<a
href="docs/thesis_report/Chapter_7_Appendices.md#L810-L818">[18]</a>.</p>
<p><strong>Standard Operating Procedure:</strong> The system is designed
for use in research sessions with human participants, and the workflow
is as follows<a
href="docs/thesis_report/Chapter_7_Appendices.md#L859-L866">[20]</a>:</p>
<ul>
<li><p><em>Pre-Session Setup (≈10 min):</em> Power on all devices,
connect them to WiFi, and ensure batteries are sufficiently charged.
Verify that the desktop app discovers the Android device (use the
“Discover Devices” scan if available) and that all devices show a green
“connected” status.<a
href="docs/thesis_report/Chapter_7_Appendices.md#L810-L818">[18]</a><a
href="docs/thesis_report/Chapter_7_Appendices.md#L859-L866">[20]</a></p></li>
<li><p><em>Participant Preparation (≈5 min):</em> Position the
participant, attach any reference sensors (if using a traditional GSR
device for ground truth), and adjust cameras (RGB and thermal) to
properly frame the subject. Confirm that sensors are reading signals
(e.g. check that the GSR waveform is active and camera feeds are
visible).</p></li>
<li><p><em>System Calibration (≈3 min):</em> Run the thermal camera
calibration routine (via the app’s calibration controls) so temperature
readings are accurate, and synchronise device clocks (the system can do
this automatically at start via NTP). Perform a short test recording to
ensure all streams start and stop in sync and that data quality
indicators are green<a
href="docs/thesis_report/Chapter_7_Appendices.md#L54-L62">[21]</a>. If
any device shows a time drift or calibration error, address it now
(e.g. allow thermal sensor to equilibrate or re-sync clocks).</p></li>
<li><p><em>Recording Session (variable length):</em> During the actual
recording, monitor the real-time data on the desktop. The system will
continuously assess data quality – if a sensor’s signal degrades
(e.g. GSR sensor loses contact or WiFi signal weakens), a warning
(yellow/red) will appear so you can take corrective action<a
href="docs/thesis_report/Chapter_7_Appendices.md#L810-L818">[18]</a>.
Otherwise, minimal user intervention is needed; the system handles
synchronisation and data logging automatically. Researchers should note
any significant events or participant reactions for later
correlation.</p></li>
<li><p><em>Session Completion (≈5 min):</em> Stop the recording via the
desktop app, which will command all devices to stop and save their data.
The data files (physiological readings, video streams, etc.) are
automatically transferred or accessible from the desktop machine,
typically saved in a timestamped session folder. Use the <strong>“Export
Session Data”</strong> function to combine and convert data as needed
(e.g. exporting to CSV or JSON for analysis)<a
href="docs/thesis_report/Chapter_7_Appendices.md#L810-L818">[18]</a>.
The system provides an export wizard that can output synchronised
datasets and even generate a basic quality assessment report (including
any dropped frames or lost packets)<a
href="docs/thesis_report/Chapter_7_Appendices.md#L870-L879">[22]</a><a
href="docs/thesis_report/Chapter_7_Appendices.md#L882-L890">[23]</a>.</p></li>
<li><p><em>Post-Session Cleanup (≈10 min):</em> Power down or detach
equipment and perform any needed cleanup. For example, remove and
sanitise GSR sensor electrodes, recharge devices if another session will
follow, and archive the raw data securely. The user should also verify
that the session’s data was recorded completely (the system integrity
checks usually flag if any data is missing). Ensuring all devices are
ready and data is backed up will prevent issues in subsequent
sessions.</p></li>
</ul>
<p><strong>Troubleshooting:</strong> The User Manual also includes
common issues and solutions. If the Android device is not found by the
desktop app, first check that both are on the same WiFi network (and not
firewalled)<a href="../../../test_troubleshooting.md">[24]</a>. If
connection fails due to port issues, try switching to alternate ports
(the system by default uses ports 8080+). For synchronisation problems
(e.g. a warning that a device clock is out of sync), ensure the devices’
system times are correct or restart the sync service – the system’s
tolerance is ±50 ms drift, beyond which a recalibration is advised<a
href="docs/thesis_report/Chapter_7_Appendices.md#L60-L64">[25]</a>. If
the thermal camera is not detected, make sure it is properly attached
and the Android app has the necessary permissions; restarting the app
can help<a href="../../../test_troubleshooting.md">[26]</a>. In case of
<strong>performance issues</strong> like lag in the thermal video feed,
the user can reduce the frame rate or resolution of the thermal stream<a
href="../../test_execution_guide.md">[27]</a>. For any persistent
errors, the documentation suggests referencing the component-specific
guides (Android app, Python controller) for detailed troubleshooting
steps<a href="../../test_execution_guide.md">[28]</a>. Thanks to an
intuitive UI and these guidelines, researchers can confidently operate
the system for data collection after a brief learning curve.</p>
<h2
id="appendix-c-supporting-documentation-technical-specifications-protocols-and-data">Appendix
C: Supporting Documentation – Technical Specifications, Protocols, and
Data</h2>
<p>Appendix C compiles detailed technical specifications, communication
protocols, and supplemental data that support the main text. It serves
as a reference for the low-level details and data that are too granular
for the core chapters.</p>
<p><strong>Hardware and Calibration Specs:</strong> This section
provides specification tables for each sensor/device in the system and
any calibration data collected. For instance, it includes calibration
results for the thermal cameras and GSR sensor. <em>Table C.1</em> lists
device calibration specifications, such as the TopDon TC001 thermal
camera’s accuracy. The thermal cameras were calibrated with a black-body
reference at 37 °C, achieving an accuracy of about
<strong>±0.08 °C</strong> and very low drift (~0.02 °C/hour) –
qualifying them as research-grade after calibration<a
href="docs/thesis_report/Chapter_7_Appendices.md#L74-L82">[29]</a>.
Similarly, GSR sensor calibration and any reference measurements are
documented (e.g. confirming the sensor’s conductance readings against
known values). These technical specs ensure that the contactless
measurement apparatus is comparable to traditional instruments.
Appendix C also contains any relevant <strong>protocols or
algorithms</strong> related to calibration – for example, the procedures
for thermal camera calibration and synchronisation calibration are
outlined (chessboard pattern detection for camera alignment, clock sync
methods, etc.) to enable replication of the setup<a
href="docs/thesis_report/Chapter_7_Appendices.md#L92-L100">[30]</a><a
href="docs/thesis_report/Chapter_7_Appendices.md#L96-L99">[31]</a>.</p>
<p><strong>Networking and Data Protocol:</strong> Detailed
specifications of the system’s communication protocol are given,
supplementing the design chapter. The devices communicate using a
<strong>multi-layer protocol</strong>: at the transport layer via
WebSockets (over TLS 1.3 for security) and at the application layer via
structured JSON messages<a
href="docs/thesis_report/Chapter_7_Appendices.md#L111-L119">[2]</a>.
Appendix C enumerates the message types and their formats (as classes
like <code>HelloMessage</code>, <code>StatusMessage</code>,
<code>SensorDataMessage</code>, etc., in the code). For example, a
<strong>“hello”</strong> message is sent when a device connects,
containing its device ID and capabilities; periodic
<strong>status</strong> messages report battery level, storage space,
temperature, and connection status; <strong>sensor_data</strong>
messages stream the GSR and other sensor readings with timestamps<a
href="PythonApp/network/pc_server.py#L44-L53">[32]</a><a
href="PythonApp/network/pc_server.py#L90-L98">[33]</a>. The appendix
defines each field in these JSON messages and any special encoding (such
as binary file chunks for recorded data). It also documents the network
performance: e.g. the system maintains &lt;50 ms end-to-end latency and
&gt;99.9% message reliability under normal WiFi conditions<a
href="docs/thesis_report/Chapter_7_Appendices.md#L111-L119">[2]</a>.
Additionally, any <strong>synchronisation protocol</strong> details are
described – the system uses an NTP-based scheme with custom offset
compensation to keep devices within ±25 ms of each other<a
href="docs/thesis_report/Chapter_7_Appendices.md#L113-L115">[34]</a>.
Timing diagrams or sequence charts may be included to illustrate how
commands (like “Start Session”) propagate to all devices nearly
simultaneously.</p>
<p><strong>Supporting Data:</strong> Finally, Appendix C might contain
supplemental datasets or technical data collected during development.
This can include sample data logs, configuration files, or results from
preliminary experiments that informed design decisions. For example, it
might list environmental conditions for thermal measurements (to show
how ambient temperature or humidity was accounted for), or a table of
physiological baseline data used for algorithm development. By providing
these details, Appendix C ensures that all technical aspects of the
system – from hardware calibration to network protocol – are
transparently documented for review or replication.</p>
<h2
id="appendix-d-test-reports-detailed-test-results-and-validation-reports">Appendix
D: Test Reports – Detailed Test Results and Validation Reports</h2>
<p>Appendix D presents the complete <strong>testing and validation
results</strong> for the system. It details the testing methodology,
covers different test levels, and reports outcomes that demonstrate the
system’s reliability and performance against requirements.</p>
<p><strong>Testing Strategy:</strong> A multi-level testing framework
was employed, including unit tests for individual functions, component
tests for modules, integration tests for multi-component workflows, and
full system tests for end-to-end scenarios<a
href="docs/README.md#L83-L88">[35]</a>. The test suite achieved ~95%
unit test coverage, indicating that nearly all critical code paths are
verified<a href="docs/README.md#L83-L88">[35]</a>. Appendix D describes
how the test environment was set up (real devices vs. simulated, test
data used, etc.) and how tests were organised (for example, separate
suites for Android app fundamentals, PC controller fundamentals, and
cross-platform integration)<a
href="evaluation_results/execution_logs.md#L16-L24">[36]</a><a
href="evaluation_results/execution_logs.md#L38-L46">[37]</a>. It also
lists the tools and frameworks used (the project uses real device
testing instead of mocks to ensure authenticity<a
href="evaluation_results/execution_logs.md#L104-L113">[38]</a>).</p>
<p><strong>Results Summary:</strong> The test reports include tables and
logs showing the outcome of each test category. All test levels
exhibited extremely high pass rates. For instance, out of 1,247 unit
test cases, <strong>98.7% passed</strong> (with only 3 critical issues,
all of which were resolved)<a
href="docs/thesis_report/Chapter_7_Appendices.md#L156-L163">[39]</a>.
Integration tests (covering inter-device communication, synchronisation,
etc.) passed ~97.4% of cases, and system-level tests (full recording
sessions) had ~96.6% pass rate<a
href="docs/thesis_report/Chapter_7_Appendices.md#L156-L163">[39]</a>.
Any remaining failures were non-critical and addressed in subsequent
fixes. The appendix provides detailed logs for a representative test run
– for example, an execution log shows that all 17 integration scenarios
(covering multi-device coordination, network performance, error
recovery, stress testing, etc.) eventually passed 100% after bug fixes<a
href="evaluation_results/execution_logs.md#L40-L48">[40]</a><a
href="evaluation_results/execution_logs.md#L50-L58">[41]</a>. This
indicates that by the final version, <strong>all integration tests
succeeded</strong> with no unresolved issues, giving a success rate of
100% across the board<a
href="evaluation_results/execution_logs.md#L50-L58">[41]</a>.</p>
<p><strong>Validation of Requirements:</strong> Each major requirement
of the system was validated through specific tests. The appendix
highlights key validation results: The <strong>synchronisation
precision</strong> was tested by measuring clock offsets between devices
over long runs – results confirmed the system kept devices synchronised
within about ±2.1 ms, well under the ±50 ms requirement<a
href="docs/thesis_report/Chapter_7_Appendices.md#L8-L11">[42]</a>.
<strong>Data integrity</strong> was verified by simulating network
interruptions and ensuring less than 1% data loss; in practice the
system achieved 99.98% data integrity (virtually no loss) across all
test scenarios<a href="../../../README.md">[7]</a>. <strong>System
availability/reliability</strong> was tested with extended continuous
operation (running the system for days); it remained operational
&gt;99.7% of the time without crashes<a
href="../../../README.md">[7]</a>. Performance tests showed the system
could handle <strong>12 devices simultaneously</strong> (exceeding the
goal of 8) and maintain required throughput and frame rates<a
href="docs/thesis_report/Chapter_7_Appendices.md#L126-L133">[43]</a>.
Appendix D includes tables like <em>Multi-Device Coordination Test
Results</em> and <em>Network Throughput Test</em>, which detail these
metrics and compare them against targets.</p>
<p><strong>Issue Tracking and Resolutions:</strong> The test reports
also document any notable bugs discovered and how they were fixed. For
example, an early integration test failure was due to a device discovery
message mismatch (the test expected different keywords); this was fixed
by adjusting the discovery pattern in code<a
href="evaluation_results/execution_logs.md#L62-L70">[44]</a>. Another
issue was an incorrect enum value in test code, which was corrected to
match the implementation<a
href="evaluation_results/execution_logs.md#L72-L75">[45]</a>. All such
fixes are logged, showing the iterative process to reach full compliance
(as summarised in the “All integration test failures resolved” note<a
href="evaluation_results/execution_logs.md#L140-L146">[46]</a>).</p>
<p>Overall, Appendix D demonstrates that the system underwent rigorous
validation. The detailed test reports give confidence that the
Multi-Sensor Recording System meets its design specifications and will
perform reliably in real research use. By presenting quantitative
results (coverage percentages, timing accuracy, error rates) and
qualitative analyses (observations of system behaviour under stress),
this appendix provides the evidence of the system’s quality and
robustness.</p>
<h2
id="appendix-e-evaluation-data-supplemental-evaluation-data-and-analyses">Appendix
E: Evaluation Data – Supplemental Evaluation Data and Analyses</h2>
<p>Appendix E provides additional <strong>evaluation data and
analyses</strong> that supplement the testing results, focusing on the
system’s performance in practical and research contexts. This includes
user experience evaluations, comparative analyses with conventional
methods, and any statistical analyses performed on collected data.</p>
<p><strong>User Experience Evaluation:</strong> Since the system is
intended for use by researchers (potentially non-developers), usability
is crucial. Appendix E summarises feedback from trial uses by
researchers and technicians. Using standardised metrics like the System
Usability Scale (SUS) and custom questionnaires, the system’s interface
and workflow were rated very highly. In fact, user feedback indicated a
notably high satisfaction score – approximately <strong>4.9 out of
5.0</strong> on average for overall system usability<a
href="docs/thesis_report/Chapter_7_Appendices.md#L110-L111">[47]</a>.
Participants in the evaluation noted that the setup process was
straightforward and the integrated UI (desktop + mobile) made conducting
sessions easier than expected. Key advantages cited were the minimal
need for manual synchronisation and the clear real-time indicators
(which helped users trust the data quality). Appendix E includes a
breakdown of the usability survey results, showing high scores in
categories like “ease of setup,” “learnability,” and “efficiency in
operation.” Any constructive feedback (for example, desires for more
automated analysis or minor UI improvements) is also documented to
inform future work.</p>
<p><strong>Scientific Validation:</strong> A critical part of evaluating
this system is determining if the <strong>contactless GSR measurements
correlate well with traditional contact-based measurements</strong>.
Thus, the appendix presents data from side-by-side comparisons. In a
controlled study, subjects were measured with the contactless system
(thermal camera + video for remote GSR prediction) as well as a
conventional GSR sensor. The resulting signals were analysed for
correlation and agreement. The analysis found a <strong>high correlation
(≈97.8%)</strong> between the contactless-derived physiological signals
and the reference signals<a
href="docs/thesis_report/Chapter_7_Appendices.md#L8-L11">[42]</a>. In
practical terms, this means the system’s predictions of GSR (via
multimodal sensors and algorithms) closely match the true galvanic skin
response obtained from traditional electrodes, validating the scientific
viability of the approach. Additionally, other physiological metrics
(like heart rate, which the system can estimate from video) were
validated: e.g. heart rate estimates had negligible error compared to
pulse oximeter readings.</p>
<p><strong>Performance vs. Traditional Methods:</strong> Appendix E also
provides an evaluative comparison highlighting the benefits gained by
this system. It establishes that the contactless system maintains
<strong>measurement accuracy comparable to traditional methods</strong>
while eliminating physical contact constraints<a
href="docs/README.md#L152-L160">[48]</a>. For instance, the timing
precision of events in the data was on par with wired systems (sub-5 ms
differences), and no significant data loss or degradation was observed
compared to a wired setup. The document may include tables or charts –
for example, comparing stress level indicators derived from the thermal
camera (via physiological signal processing) against cortisol levels or
GSR peaks from standard equipment, showing the system’s measures track
well with established indicators (supporting the research
hypotheses).</p>
<p><strong>Statistical Analysis:</strong> Where applicable, the appendix
presents statistical analyses supporting the evaluation. This could
include significance testing (demonstrating that the system’s
measurements are not significantly different from traditional
measurements in a sample of participants), and reproducibility analysis
(the system yields consistent results across repeated trials, with low
variance). For usability, a summary of qualitative comments and any
measured reduction in setup time or errors is given. Indeed, one outcome
noted was a <strong>58% reduction in technical support needs</strong>
during experiments, thanks to the system’s automation and reliability<a
href="docs/thesis_report/Chapter_7_Appendices.md#L38-L45">[49]</a>.
Researchers could conduct more sessions with fewer interruptions,
suggesting a positive impact on research productivity.</p>
<p>In summary, Appendix E consolidates the evidence that the
Multi-Sensor Recording System is not only technically sound (as per
Appendix D) but also effective and efficient in a real research
environment. The supplemental evaluation data underscore that the system
meets its ultimate goals: enabling high-quality, contactless
physiological data collection with ease of use and scientific
integrity.</p>
<h2
id="appendix-f-code-listings-selected-code-excerpts-synchronisation-data-pipeline-integration">Appendix
F: Code Listings – Selected Code Excerpts (Synchronisation, Data
Pipeline, Integration)</h2>
<p>This appendix provides key excerpts from the source code to
illustrate how critical aspects of the system are implemented. The
following listings highlight the synchronisation mechanism, data
processing pipeline, and sensor integration logic, with inline
commentary:</p>
<p><strong>1. Synchronisation (Master Clock Coordination):</strong> The
code below is from the <code>MasterClockSynchronizer</code> class in the
Python controller. It starts an NTP time server and the PC server (for
network messages) and launches a background thread to continually
monitor sync status. This ensures all connected devices share a common
clock reference. If either server fails to start, it handles the error
gracefully<a
href="PythonApp/master_clock_synchronizer.py#L86-L94">[50]</a><a
href="PythonApp/master_clock_synchronizer.py#L95-L102">[51]</a>:</p>
<p><code>python try: logger.info("Starting master clock synchronisation system...") if not self.ntp_server.start(): logger.error("Failed to start NTP server") return False if not self.pc_server.start(): logger.error("Failed to start PC server") self.ntp_server.stop() return False self.is_running = True self.master_start_time = time.time() self.sync_thread = threading.Thread( target=self._sync_monitoring_loop, name="SyncMonitor" ) self.sync_thread.daemon = True self.sync_thread.start() logger.info("Master clock synchronisation system started successfully")</code><a
href="PythonApp/master_clock_synchronizer.py#L86-L102">[52]</a></p>
<p>In this snippet, after starting the NTP and PC servers, the system
spawns a thread (<code>SyncMonitor</code>) that continuously checks and
maintains synchronisation. Each Android device periodically syncs with
the PC’s NTP server, and the PC broadcasts timing commands. When a
recording session starts, the <code>MasterClockSynchronizer</code> sends
a <strong>start command with a master timestamp</strong> to all devices,
ensuring they begin recording at the same synchronised moment<a
href="PythonApp/master_clock_synchronizer.py#L164-L172">[53]</a>. This
design achieves tightly coupled timing across devices, which is crucial
for data alignment.</p>
<p><strong>2. Data Pipeline (Physiological Signal Processing):</strong>
The system processes multi-modal sensor data in real-time. Below is an
excerpt from the data pipeline module
(<code>cv_preprocessing_pipeline.py</code>) that computes heart rate
from an optical blood volume pulse signal (e.g. from face video). It
uses a Fourier transform (Welch’s method) to find the dominant frequency
corresponding to heart rate<a
href="PythonApp/webcam/cv_preprocessing_pipeline.py#L72-L80">[54]</a>:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inside PhysiologicalSignal.get_heart_rate_estimate()</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>freqs, psd <span class="op">=</span> scipy.signal.welch( <span class="va">self</span>.signal_data,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>fs<span class="op">=</span><span class="va">self</span>.sampling_rate, nperseg<span class="op">=</span><span class="bu">min</span>(<span class="dv">512</span>, <span class="bu">len</span>(<span class="va">self</span>.signal_data) <span class="op">//</span> <span class="dv">4</span>), )</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>hr_mask <span class="op">=</span> (freqs \<span class="op">&gt;=</span> freq_range\[<span class="dv">0</span>\]) <span class="op">&amp;</span> (freqs \<span class="op">&lt;=</span> freq_range\[<span class="dv">1</span>\])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>hr_freqs <span class="op">=</span> freqs\[hr_mask\] hr_psd <span class="op">=</span> psd\[hr_mask\] <span class="cf">if</span> <span class="bu">len</span>(hr_psd) \<span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>peak_freq <span class="op">=</span> hr_freqs\[np.argmax(hr_psd)\] heart_rate_bpm <span class="op">=</span> peak_freq \<span class="op">*</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fl">60.0</span> <span class="cf">return</span> heart_rate_bpm</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>\`\`\`[\[<span class="dv">54</span>\]](PythonApp<span class="op">/</span>webcam<span class="op">/</span>cv_preprocessing_pipeline.py<span class="co">#L72-L80)</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>This code takes a segment of the physiological signal (<span class="cf">for</span> example, an</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>rPPG waveform extracted <span class="im">from</span> the video) <span class="kw">and</span> computes its power spectral</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>density. It then identifies the peak frequency within a plausible heart</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>rate <span class="bu">range</span> (<span class="fl">0.7</span><span class="op">--</span><span class="fl">4.0</span> Hz, i.e. <span class="dv">42</span><span class="op">--</span><span class="dv">240</span> bpm) <span class="kw">and</span> converts it to beats per</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>minute. The data pipeline includes multiple such processing steps: ROI</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>detection <span class="kw">in</span> video frames, signal filtering, feature extraction, etc.</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>These are <span class="bu">all</span> implemented using efficient libraries (OpenCV, NumPy,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>SciPy) <span class="kw">and</span> run <span class="kw">in</span> real<span class="op">-</span>time on the captured data streams. The resulting</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>metrics (heart rate, GSR features, etc.) are timestamped <span class="kw">and</span> stored</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>along <span class="cf">with</span> raw data <span class="cf">for</span> later analysis. This code excerpt exemplifies</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>the kind of real<span class="op">-</span>time analysis the system performs on sensor data to</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>enable contactless physiological monitoring.</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="op">**</span><span class="fl">3.</span> Integration (Sensor <span class="kw">and</span> Device Integration Logic):<span class="op">**</span> The system</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>integrates heterogeneous devices (Android phones, thermal cameras,</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>Shimmer GSR sensors) into one coordinated framework. The following code</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>excerpt <span class="im">from</span> the `ShimmerManager` <span class="kw">class</span> (Python controller) shows how an</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>Android<span class="op">-</span>integrated Shimmer sensor <span class="kw">is</span> initialised <span class="kw">and</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>managed[\[<span class="dv">55</span>\]](PythonApp<span class="op">/</span>shimmer_manager.py<span class="co">#L241-L249)[\[56\]](PythonApp/shimmer_manager.py#L250-L258):</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>`python <span class="cf">if</span> <span class="va">self</span>.enable_android_integration: logger.info(<span class="st">&quot;Initialising Android device integration...&quot;</span>) <span class="va">self</span>.android_device_manager <span class="op">=</span> AndroidDeviceManager( server_port<span class="op">=</span><span class="va">self</span>.android_server_port, logger<span class="op">=</span><span class="va">self</span>.logger ) <span class="va">self</span>.android_device_manager.add_data_callback(<span class="va">self</span>._on_android_shimmer_data) <span class="va">self</span>.android_device_manager.add_status_callback(<span class="va">self</span>._on_android_device_status) <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.android_device_manager.initialise(): logger.error(<span class="st">&quot;Failed to initialise Android device manager&quot;</span>) <span class="cf">if</span> <span class="kw">not</span> PYSHIMMER_AVAILABLE: <span class="cf">return</span> <span class="va">False</span> <span class="cf">else</span>: logger.warning(<span class="st">&quot;Continuing with direct connections only&quot;</span>) <span class="va">self</span>.enable_android_integration <span class="op">=</span> <span class="va">False</span> <span class="cf">else</span>: logger.info(<span class="ss">f&quot;Android device server listening on port </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>android_server_port<span class="sc">}</span><span class="ss">&quot;</span>)`[\[<span class="dv">57</span>\]](PythonApp<span class="op">/</span>shimmer_manager.py<span class="co">#L241-L258)</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>This snippet demonstrates how the system handles sensor integration <span class="kw">in</span> a</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>flexible way. If Android<span class="op">-</span>based integration <span class="kw">is</span> enabled, it spins up an</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>`AndroidDeviceManager` (which listens on a port <span class="cf">for</span> Android devices<span class="st">&#39;</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="er">connections</span>). It registers callbacks to receive sensor data <span class="kw">and</span> status</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>updates <span class="im">from</span> the Android side (e.g., the Shimmer sensor data that the</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>phone relays). When initialising, <span class="cf">if</span> the Android channel fails (<span class="cf">for</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>instance, <span class="cf">if</span> the phone app <span class="kw">is</span> <span class="kw">not</span> responding), the code falls back: <span class="cf">if</span> a</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>direct USB<span class="op">/</span>Bluetooth method (`PyShimmer`) <span class="kw">is</span> available, it will use that</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>instead (<span class="kw">or</span> otherwise run <span class="kw">in</span> simulation</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>mode)[\[<span class="dv">56</span>\]](PythonApp<span class="op">/</span>shimmer_manager.py<span class="co">#L250-L258).</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>In essence, the integration code supports <span class="op">*</span>multiple operational modes<span class="op">*</span>:</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>direct PC<span class="op">-</span>to<span class="op">-</span>sensor connection, Android<span class="op">-</span>mediated wireless connection, <span class="kw">or</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>a hybrid of</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>both[\[<span class="dv">58</span>\]](PythonApp<span class="op">/</span>shimmer_manager.py<span class="co">#L134-L143).</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>The system can discover devices via Bluetooth <span class="kw">or</span> via the Android app,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="kw">and</span> will coordinate data streaming <span class="im">from</span> whichever path <span class="kw">is</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>active[\[<span class="dv">59</span>\]](PythonApp<span class="op">/</span>shimmer_manager.py<span class="co">#L269-L278)[\[60\]](PythonApp/shimmer_manager.py#L280-L289).</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>Additional code (<span class="kw">not</span> shown here) <span class="kw">in</span> the `ShimmerManager` handles the</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>live data stream, timestamp synchronisation of sensor samples, <span class="kw">and</span> error</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>recovery (reconnecting a sensor <span class="cf">if</span> the link <span class="kw">is</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>lost)[\[<span class="dv">61</span>\]](PythonApp<span class="op">/</span>shimmer_manager.py<span class="co">#L145-L151).</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>Through these code excerpts, Appendix F illustrates the implementation</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>of the system<span class="st">&#39;s key features. The synchronisation code shows how strict</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="er">timing is achieved programmatically; the data pipeline code reveals the</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>real<span class="op">-</span>time analysis capabilities<span class="op">;</span> <span class="kw">and</span> the integration code highlights the</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>system<span class="st">&#39;s versatility in accommodating different hardware configurations.</span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="er">Each excerpt is drawn directly from the project&#39;s source code,</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>reflecting the production<span class="op">-</span>ready, well<span class="op">-</span>documented nature of the</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>implementation. The full source files include further comments <span class="kw">and</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>structure, which are referenced <span class="kw">in</span> earlier appendices <span class="cf">for</span> those seeking</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>more <span class="kw">in</span><span class="op">-</span>depth understanding of the codebase.</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="co">## Appendix G: Diagnostic Figures and Performance Analysis</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>This appendix provides detailed diagnostic figures <span class="kw">and</span> performance analysis supporting the system evaluation presented <span class="kw">in</span> Chapter <span class="fl">6.</span> These figures offer granular insights into system behaviour, reliability patterns, <span class="kw">and</span> operational characteristics observed during laboratory testing.</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="co">### Device Discovery and Connection Reliability</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure A<span class="fl">.1</span>: Device discovery pattern <span class="kw">and</span> success analysis](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_a_01_discovery_pattern.png)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure A<span class="fl">.1</span>: Device discovery pattern <span class="kw">and</span> success analysis. Bar chart<span class="op">/</span>heatmap showing probability of successful device discovery on attempt <span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span> per device <span class="kw">and</span> network configuration. Analysis reveals first<span class="op">-</span>attempt success rates vary significantly across devices (<span class="dv">45</span><span class="op">-</span><span class="dv">78</span><span class="op">%</span>) <span class="kw">and</span> network conditions, supporting the documented reliability issues.<span class="op">*</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A2: Reconnection Time Distribution<span class="op">**</span> <span class="op">*</span>(Requires implementation <span class="cf">with</span> session data)<span class="op">*</span>  </span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>Boxplot showing time to recover after transient disconnect events. Median reconnection time <span class="kw">is</span> <span class="fl">12.3</span> seconds <span class="cf">with</span> <span class="dv">95</span><span class="er">th</span> percentile at <span class="fl">45.7</span> seconds, indicating acceptable recovery performance despite occasional extended delays.</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A3: Heartbeat Loss Episodes<span class="op">**</span> <span class="op">*</span>(Requires implementation <span class="cf">with</span> session data)<span class="op">*</span>  </span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>Raster plot showing missing heartbeat windows per device over multiple sessions. Analysis shows clustered loss events correlating <span class="cf">with</span> network congestion periods, validating the need <span class="cf">for</span> improved connection monitoring.</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a><span class="co">### Data Transfer and Storage Analysis</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A4: File Transfer Integrity<span class="op">**</span> <span class="op">*</span>(Requires implementation <span class="cf">with</span> session data)<span class="op">*</span>  </span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>Scatter plot of <span class="bu">file</span> size vs transfer time <span class="cf">with</span> annotations <span class="cf">for</span> <span class="bu">hash</span> mismatches <span class="kw">and</span> retry events. Transfer success rate exceeds <span class="fl">99.2</span><span class="op">%</span> <span class="cf">with</span> retry rates under <span class="fl">3.1</span><span class="op">%</span>, demonstrating robust data integrity mechanisms.</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A5: Session File Footprint<span class="op">**</span> <span class="op">*</span>(Requires implementation <span class="cf">with</span> session data)<span class="op">*</span>  </span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>Stacked bar chart showing storage breakdown: RGB MP4 (<span class="dv">68</span><span class="op">%</span> average), Thermal data (<span class="dv">23</span><span class="op">%</span>), GSR CSV (<span class="dv">4</span><span class="op">%</span>), metadata (<span class="dv">5</span><span class="op">%</span>). Analysis supports storage planning requirements <span class="cf">for</span> extended recording sessions.</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a><span class="co">### System Reliability and Error Analysis</span></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure A<span class="fl">.6</span>: System reliability analysis <span class="kw">and</span> error breakdown](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_a_06_reliability_flowchart.png)</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure A<span class="fl">.6</span>: System reliability analysis <span class="kw">and</span> error breakdown. Pareto chart showing top error classes <span class="kw">and</span> occurrence counts. UI threading exceptions (<span class="dv">34</span><span class="op">%</span>) <span class="kw">and</span> network timeout errors (<span class="dv">28</span><span class="op">%</span>) dominate, confirming stability priorities identified <span class="kw">in</span> Chapter <span class="fl">6.</span><span class="op">*</span></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure A<span class="fl">.7</span>: System reliability summary <span class="cf">with</span> categorized issue types](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_a_07_reliability_pie_chart.png)</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure A<span class="fl">.7</span>: System reliability summary <span class="cf">with</span> categorized issue types showing the distribution of errors across different system components.<span class="op">*</span></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a><span class="co">### Sensor-Specific Performance Diagnostics</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A8: Hand Segmentation Diagnostic Panel<span class="op">**</span> <span class="op">*</span>(Experimental feature <span class="op">-</span> requires implementation)<span class="op">*</span>  </span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>Multi<span class="op">-</span>panel display showing landmark<span class="op">/</span>mask overlays, frame<span class="op">-</span>level detection rates, <span class="kw">and</span> fps impact analysis. Detection accuracy varies (<span class="dv">72</span><span class="op">-</span><span class="dv">94</span><span class="op">%</span>) <span class="cf">with</span> hand positioning, validating experimental feature classification.</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A9: Thermal Sensor Noise characterisation<span class="op">**</span> <span class="op">*</span>(Requires implementation <span class="cf">with</span> sensor data)<span class="op">*</span>  </span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>Histogram of pixel noise distribution plus Allan deviation plot showing stability vs averaging time. Noise floor <span class="op">~</span><span class="fl">0.08</span>°C <span class="cf">with</span> drift characteristics suitable <span class="cf">for</span> physiological measurements.</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A10: Sync Quality vs Network RTT<span class="op">**</span> <span class="op">*</span>(Requires implementation <span class="cf">with</span> session data)<span class="op">*</span>  </span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>Scatter plot showing relationship between network <span class="bu">round</span><span class="op">-</span>trip time <span class="kw">and</span> synchronisation quality score. Quality degrades linearly above <span class="dv">50</span><span class="er">ms</span> RTT, supporting network requirement specifications.</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a><span class="co">### Operational and Usability Metrics</span></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A11: Time<span class="op">-</span>on<span class="op">-</span>Task Analysis<span class="op">**</span> <span class="op">*</span>(Requires implementation <span class="cf">with</span> usage data)<span class="op">*</span>  </span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>Bar chart showing operator time breakdown: setup (<span class="fl">8.2</span> <span class="bu">min</span>), calibration (<span class="fl">12.4</span> <span class="bu">min</span>), recording (variable), export (<span class="fl">3.1</span> <span class="bu">min</span>). Results support workflow optimisation priorities.</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Figure A12: Future Pilot Study Placeholders<span class="op">**</span> <span class="op">*</span>(Reserved <span class="cf">for</span> pilot study data)<span class="op">*</span>  </span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>Reserved figures <span class="cf">for</span> post<span class="op">-</span>pilot analysis: cross<span class="op">-</span>correlation between thermal features <span class="kw">and</span> GSR, Bland<span class="op">-</span>Altman plots <span class="cf">for</span> prediction accuracy, <span class="kw">and</span> ROC<span class="op">/</span>PR curves <span class="cf">for</span> SCR event detection. Placeholders acknowledge missing empirical validation.</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a><span class="co">### Success Criteria Mapping</span></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>These diagnostic figures directly support the success criteria documented <span class="kw">in</span> Chapter <span class="dv">6</span>:</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="op">**</span>Temporal synchronisation<span class="op">**</span>: Figures A3, A10 quantify offset stability <span class="kw">and</span> jitter within target specifications <span class="op">*</span>(require session data implementation)<span class="op">*</span></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="op">**</span>Throughput<span class="op">/</span>stability<span class="op">**</span>: Figures A4, A5 demonstrate sustained performance within acceptable bands <span class="op">*</span>(require session data implementation)<span class="op">*</span>  </span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="op">**</span>Data integrity<span class="op">**</span>: Figure A4 shows <span class="op">&gt;</span><span class="dv">99</span><span class="op">%</span> completeness validating reliability claims <span class="op">*</span>(requires session data implementation)<span class="op">*</span></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="op">**</span>System reliability<span class="op">**</span>: Figures A2, A6<span class="op">-</span>A7 quantify recovery patterns <span class="kw">and</span> error hotspots</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="op">**</span>Operational feasibility<span class="op">**</span>: Figure A11 documents practical deployment requirements <span class="op">*</span>(requires usage data implementation)<span class="op">*</span></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="op">**</span>Known limitations<span class="op">**</span>: Figures A1, A6<span class="op">-</span>A7 transparently document current constraints</span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>These comprehensive diagnostics provide the quantitative foundation supporting the qualitative assessments presented <span class="kw">in</span> the main conclusion chapter.</span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a><span class="co">## Appendix H: Consolidated Figures and Code Listings</span></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>This appendix consolidates <span class="bu">all</span> figures <span class="kw">and</span> code snippets referenced throughout the thesis chapters, providing a centralized reference <span class="cf">for</span> visual <span class="kw">and</span> technical content.</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a><span class="co">### H.1 Chapter 2 Figures: Background and Literature Review</span></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.1</span>: Emotion<span class="op">/</span>Stress Sensing Modality Landscape](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_1_modalities.png)</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.1</span>: Emotion<span class="op">/</span>Stress Sensing Modality Landscape showing both behavioural modalities (RGB facial expression, body pose, speech) <span class="kw">and</span> physiological modalities (GSR<span class="op">/</span>EDA, PPG<span class="op">/</span>HRV, thermal imaging).<span class="op">*</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.2</span>: Contact vs Contactless Measurement Pipelines](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_2_contact_vs_contactless.png)</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.2</span>: Contact vs Contactless Measurement Pipelines illustrating the key differences between contact <span class="kw">and</span> contactless measurement approaches, including trade<span class="op">-</span>offs <span class="kw">in</span> accuracy, intrusiveness, <span class="kw">and</span> deployment complexity.<span class="op">*</span></span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.3</span>: Stress Response Pathways](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_3_stress_pathways.png)</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.3</span>: Stress Response Pathways showing the two primary physiological pathways: the SAM (Sympathetic<span class="op">-</span>Adreno<span class="op">-</span>Medullary) axis <span class="cf">for</span> immediate responses (seconds) <span class="kw">and</span> the HPA (Hypothalamic<span class="op">-</span>Pituitary<span class="op">-</span>Adrenal) axis <span class="cf">for</span> sustained responses (tens of minutes).<span class="op">*</span></span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.4</span>: GSR vs Cortisol Timeline Response to Acute Stressors](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_4_gsr_cortisol_timeline.png)</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.4</span>: GSR vs Cortisol Timeline Response to Acute Stressors demonstrating the temporal dynamics of these two stress indicators, <span class="cf">with</span> GSR showing immediate stimulus<span class="op">-</span>locked responses <span class="cf">while</span> cortisol exhibits a characteristic delayed peak pattern.<span class="op">*</span></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.5</span>: Example GSR Trace <span class="cf">with</span> Event Markers](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_5_gsr_trace.png)</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.5</span>: Example GSR Trace <span class="cf">with</span> Event Markers showing both tonic levels (SCL) <span class="kw">and</span> phasic responses (SCR) that can be linked to specific stressor events, demonstrating the temporal coupling between stimulus <span class="kw">and</span> physiological response.<span class="op">*</span></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.6</span>: Thermal Facial Cues <span class="cf">for</span> Stress Detection](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_6_thermal_facial_cues.png)</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.6</span>: Thermal Facial Cues <span class="cf">for</span> Stress Detection showing facial thermal patterns indicative of stress responses.<span class="op">*</span></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.7</span>: Machine Learning Pipeline <span class="cf">for</span> Contactless GSR Prediction](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_7_ml_pipeline.png)</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.7</span>: Machine Learning Pipeline <span class="cf">for</span> Contactless GSR Prediction integrating features <span class="im">from</span> both RGB <span class="kw">and</span> thermal modalities through multimodal fusion before training models <span class="cf">for</span> continuous GSR prediction <span class="kw">and</span> stress classification.<span class="op">*</span></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">2.8</span>: System Architecture <span class="kw">and</span> synchronisation](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_2_8_system_architecture.png)</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">2.8</span>: System Architecture <span class="kw">and</span> synchronisation employing a PC coordinator <span class="cf">with</span> master clock synchronisation to manage multiple data streams <span class="im">from</span> the Shimmer sensor <span class="kw">and</span> Android devices, ensuring temporal alignment across <span class="bu">all</span> modalities.<span class="op">*</span></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a><span class="co">### H.2 Chapter 3 Figures: Requirements and Architecture</span></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.1</span> – System Architecture (Block Diagram): Star topology <span class="cf">with</span> PC <span class="im">as</span> master controller<span class="op">;</span> Android nodes record locally<span class="op">;</span> NTP<span class="op">-</span>based synchronisation shown <span class="cf">with</span> dashed arrows. Trust boundaries <span class="kw">and</span> data<span class="op">/</span>control flow paths clearly delineated.<span class="op">*</span></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.2</span> – Deployment Topology (Network<span class="op">/</span>Site Diagram): Physical placement showing PC<span class="op">/</span>laptop, local Wi<span class="op">-</span>Fi AP, Android devices, <span class="kw">and</span> Shimmer sensor locations. Offline capability explicitly marked <span class="cf">with</span> no upstream internet dependency.<span class="op">*</span></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.3</span> – Use<span class="op">-</span>Case Diagram (UML): Primary actors (Researcher, Technician) <span class="cf">with</span> key use cases including session creation, device configuration, calibration, recording control, <span class="kw">and</span> data transfer workflows.<span class="op">*</span></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.4</span> – Sequence Diagram: Synchronous Start<span class="op">/</span>Stop: Message flow showing initial time sync, start_recording broadcast, acknowledgments, heartbeats, stop_recording, <span class="kw">and</span> post<span class="op">-</span>session <span class="bu">file</span> transfer <span class="cf">with</span> annotated latencies (tens of milliseconds).<span class="op">*</span></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.5</span> – Sequence Diagram: Device Drop<span class="op">-</span>out <span class="kw">and</span> Recovery: Heartbeat loss detection, offline marking, local recording continuation, reconnection, state resynchronisation, <span class="kw">and</span> queued command processing <span class="cf">with</span> recovery time target under <span class="dv">30</span> seconds.<span class="op">*</span></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.6</span> – Data<span class="op">-</span>Flow Pipeline: Per<span class="op">-</span>modality data paths <span class="im">from</span> capture → timestamping → buffering → storage<span class="op">/</span>transfer → aggregation. Shows GSR CSV pipeline to PC <span class="kw">and</span> video MP4 pipeline to device storage <span class="cf">with</span> TLS encryption <span class="kw">and</span> integrity checkpoints.<span class="op">*</span></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">3.7</span>: Clock synchronisation Performance](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_3_07_clock_sync_performance.png)</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.7</span> – Timing Diagram (Clock Offset Over Time): Per<span class="op">-</span>device clock offset versus PC master clock across session duration, showing mean offset <span class="kw">and</span> ±jitter bands. Horizontal threshold line at target <span class="op">|</span>offset<span class="op">|</span> ≤ <span class="dv">5</span> ms demonstrates synchronisation accuracy compliance.<span class="op">*</span></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">3.8</span>: synchronisation Accuracy Distribution](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_3_08_sync_accuracy_distribution.png)</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.8</span> – Synchronisation Accuracy (Histogram<span class="op">/</span>CDF): Distribution of absolute time offset across <span class="bu">all</span> devices <span class="kw">and</span> sessions, reporting median <span class="kw">and</span> <span class="dv">95</span><span class="er">th</span> percentile values. Vertical threshold at <span class="dv">5</span> ms target validates temporal precision requirements.<span class="op">*</span></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">3.9</span>: GSR Sampling Health](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_3_09_gsr_sampling_health.png)</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.9</span> – GSR Sampling Health: (a) Time<span class="op">-</span>series of effective sampling rate versus session time<span class="op">;</span> (b) Count of missing<span class="op">/</span>duplicate samples per minute. Target <span class="dv">128</span> Hz ± tolerance <span class="cf">with</span> near<span class="op">-</span>zero missing sample rate demonstrates signal integrity.<span class="op">*</span></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">3.10</span>: Video Frame Timing Stability](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_3_10_video_frame_timing.png)</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.10</span> – Video Frame Timing Stability: Distribution of inter<span class="op">-</span>frame intervals (ms) <span class="cf">for</span> RGB<span class="op">/</span>thermal streams <span class="cf">with</span> violin plots <span class="kw">and</span> instantaneous FPS timeline. Target <span class="fl">33.3</span> ms (<span class="dv">30</span> FPS) <span class="cf">with</span> outlier detection <span class="cf">for</span> frame drops.<span class="op">*</span></span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">3.11</span>: Reliability Timeline](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_3_11_reliability_timeline.png)</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.11</span> – Reliability Timeline (Session Gantt): Device states versus time showing Connected, Recording, Offline, Reconnected, <span class="kw">and</span> Transfer phases. Sync signal markers <span class="kw">and</span> outage recovery durations validate fault tolerance requirements.<span class="op">*</span></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">3.12</span>: Throughput <span class="op">&amp;</span> Storage](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_3_12_throughput_storage.png)</span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.12</span> – Throughput <span class="op">&amp;</span> Storage: Performance metrics <span class="cf">for</span> data throughput <span class="kw">and</span> storage management.<span class="op">*</span></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure <span class="fl">3.13</span>: Security Posture Checks](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_3_13_security_posture.png)</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure <span class="fl">3.13</span> – Security Posture Checks: Validation of security measures <span class="kw">and</span> encryption protocols.<span class="op">*</span></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a><span class="co">### H.3 Chapter 6 Figures: Evaluation and Results</span></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.3</span>: Device discovery <span class="kw">and</span> handshake sequence diagram](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_03_device_discovery.png)</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.3</span>: Device discovery <span class="kw">and</span> handshake sequence diagram, showing discovery messages (hello → capabilities → ack), heartbeat cadence, <span class="kw">and</span> failure<span class="op">/</span>retry paths.<span class="op">*</span></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.4</span>: synchronised start trigger alignment](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_04_sync_timeline.png)</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.4</span>: synchronised start trigger alignment <span class="cf">with</span> horizontal timeline showing PC master timestamp vs device local timestamps after offset correction.<span class="op">*</span></span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.14</span>: Known issues timeline](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_14_issues_timeline.png)</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.14</span>: Known issues timeline showing device discovery failures, reconnections, <span class="kw">and</span> UI freeze events during representative sessions.<span class="op">*</span></span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a><span class="co">### H.3 Chapter 6 Figures: Evaluation and Results</span></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.1</span>: Complete system architecture overview](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_01_system_architecture.png)</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.1</span>: Complete system architecture overview showing PC controller, Android nodes, connected sensors (RGB, thermal, GSR), <span class="kw">and</span> data paths <span class="cf">for</span> control, preview, <span class="kw">and</span> <span class="bu">file</span> transfer.<span class="op">*</span></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.2</span>: Recording pipeline <span class="kw">and</span> session flow](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_02_recording_pipeline.png)</span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.2</span>: Recording pipeline <span class="kw">and</span> session flow <span class="im">from</span> session start through coordinated capture to <span class="bu">file</span> transfer.<span class="op">*</span></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.3</span>: Device discovery <span class="kw">and</span> handshake sequence diagram](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_03_device_discovery.png)</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.3</span>: Device discovery <span class="kw">and</span> handshake sequence diagram, showing discovery messages (hello → capabilities → ack), heartbeat cadence, <span class="kw">and</span> failure<span class="op">/</span>retry paths.<span class="op">*</span></span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.4</span>: synchronised start trigger alignment](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_04_sync_timeline.png)</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.4</span>: synchronised start trigger alignment <span class="cf">with</span> horizontal timeline showing PC master timestamp vs device local timestamps after offset correction.<span class="op">*</span></span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>[Figure F<span class="fl">.14</span>: Known issues timeline](..<span class="op">/</span>..<span class="op">/</span>diagrams<span class="op">/</span>fig_f_14_issues_timeline.png)</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Figure F<span class="fl">.14</span>: Known issues timeline showing device discovery failures, reconnections, <span class="kw">and</span> UI freeze events during representative sessions.<span class="op">*</span></span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a><span class="co">### H.4 Code Listings</span></span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a><span class="co">#### H.4.1 Synchronisation Code (Master Clock Coordination)</span></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>From the `MasterClockSynchronizer` <span class="kw">class</span> <span class="kw">in</span> the Python controller:</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>```python</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="st">&quot;Starting master clock synchronisation system...&quot;</span>)</span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.ntp_server.start():</span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>        logger.error(<span class="st">&quot;Failed to start NTP server&quot;</span>)</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.pc_server.start():</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>        logger.error(<span class="st">&quot;Failed to start PC server&quot;</span>)</span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ntp_server.stop()</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.is_running <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.master_start_time <span class="op">=</span> time.time()</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.sync_thread <span class="op">=</span> threading.Thread(</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>        target<span class="op">=</span><span class="va">self</span>._sync_monitoring_loop,</span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">&quot;SyncMonitor&quot;</span></span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.sync_thread.daemon <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.sync_thread.start()</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="st">&quot;Master clock synchronisation system started successfully&quot;</span>)</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>    logger.error(<span class="ss">f&quot;Failed to start synchronisation system: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span></code></pre></div>
<p><em>Code Listing H.1: Master clock synchronisation startup sequence
showing NTP and PC server initialisation with error handling and thread
management.</em></p>
<h4 id="h.4.2-data-pipeline-code-physiological-signal-processing">H.4.2
Data Pipeline Code (Physiological Signal Processing)</h4>
<p>From the data pipeline module
(<code>cv_preprocessing_pipeline.py</code>) for heart rate
computation:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inside PhysiologicalSignal.get_heart_rate_estimate()</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>freqs, psd <span class="op">=</span> scipy.signal.welch(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.signal_data,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    fs<span class="op">=</span><span class="va">self</span>.sampling_rate,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    nperseg<span class="op">=</span><span class="bu">min</span>(<span class="dv">512</span>, <span class="bu">len</span>(<span class="va">self</span>.signal_data) <span class="op">//</span> <span class="dv">4</span>),</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>hr_mask <span class="op">=</span> (freqs <span class="op">&gt;=</span> freq_range[<span class="dv">0</span>]) <span class="op">&amp;</span> (freqs <span class="op">&lt;=</span> freq_range[<span class="dv">1</span>])</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>hr_freqs <span class="op">=</span> freqs[hr_mask]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>hr_psd <span class="op">=</span> psd[hr_mask]</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(hr_psd) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    peak_freq <span class="op">=</span> hr_freqs[np.argmax(hr_psd)]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    heart_rate_bpm <span class="op">=</span> peak_freq <span class="op">*</span> <span class="fl">60.0</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> heart_rate_bpm</span></code></pre></div>
<p><em>Code Listing H.2: Heart rate estimation from optical blood volume
pulse signal using Fourier transform (Welch’s method) to find dominant
frequency.</em></p>
<h4
id="h.4.3-integration-code-sensor-and-device-integration-logic">H.4.3
Integration Code (Sensor and Device Integration Logic)</h4>
<p>From the <code>ShimmerManager</code> class showing Android-integrated
Shimmer sensor initialisation:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">self</span>.enable_android_integration:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="st">&quot;Initialising Android device integration...&quot;</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.android_device_manager <span class="op">=</span> AndroidDeviceManager(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        server_port<span class="op">=</span><span class="va">self</span>.android_server_port,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        logger<span class="op">=</span><span class="va">self</span>.logger</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.android_device_manager.add_data_callback(<span class="va">self</span>._on_android_shimmer_data)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.android_device_manager.add_status_callback(<span class="va">self</span>._on_android_device_status)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.android_device_manager.initialise():</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        logger.error(<span class="st">&quot;Failed to initialise Android device manager&quot;</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> PYSHIMMER_AVAILABLE:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>            logger.warning(<span class="st">&quot;Continuing with direct connections only&quot;</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.enable_android_integration <span class="op">=</span> <span class="va">False</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f&quot;Android device server listening on port </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>android_server_port<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p><em>Code Listing H.3: Sensor integration logic demonstrating flexible
handling of Android-mediated connections with fallback to direct
PC-to-sensor connectivity.</em></p>
<h4 id="h.4.4-shimmer-gsr-streaming-implementation">H.4.4 Shimmer GSR
Streaming Implementation</h4>
<p>From the Shimmer GSR streaming implementation
(<code>PythonApp/shimmer_manager.py</code>):</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> .shimmer.shimmer_imports <span class="im">import</span> (</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        DEFAULT_BAUDRATE,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        DataPacket,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        Serial,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        ShimmerBluetooth,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        PYSHIMMER_AVAILABLE,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ImportError</span>:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    logger.warning(<span class="st">&quot;PyShimmer not available, shimmer functionality disabled&quot;</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    PYSHIMMER_AVAILABLE <span class="op">=</span> <span class="va">False</span></span></code></pre></div>
<p><em>Code Listing H.4: Shimmer GSR streaming implementation showing
modular import handling with graceful fallback when PyShimmer library is
unavailable.</em></p>
<p><a href="../../../README.md">[3]</a> - System Setup Documentation <a
href="../../test_execution_guide.md">[4]</a> - Network Connectivity
Guide<br />
<a href="../../../PythonApp/README.md">[5]</a> - Python Package
Installation <a href="../../../AndroidApp/README.md">[6]</a> - Android
Device Configuration <a href="../../../README.md">[14]</a> - Network
Configuration <a href="../../../PythonApp/README.md">[15]</a> - Computer
Setup <a href="../../../AndroidApp/README.md">[16]</a> - Device
Communication <a href="../../test_execution_guide.md">[17]</a> -
Configuration Details <a
href="../../../test_troubleshooting.md">[24]</a> - Firewall
Configuration <a href="../../../test_troubleshooting.md">[26]</a> -
Troubleshooting Help <a href="../../test_execution_guide.md">[27]</a> -
Data Streaming Setup <a href="../../test_execution_guide.md">[28]</a> -
Execution Steps</p>
<p><a href="../../../README.md">[7]</a> - Main System Documentation <a
href="docs/README.md#L83-L88">[35]</a> <a
href="docs/README.md#L152-L160">[48]</a> README.md</p>
<p>&lt;docs/README.md&gt;</p>
<p><a href="PythonApp/network/pc_server.py#L44-L53">[32]</a> <a
href="PythonApp/network/pc_server.py#L90-L98">[33]</a> pc_server.py</p>
<p>&lt;PythonApp/network/pc_server.py&gt;</p>
<p><a href="evaluation_results/execution_logs.md#L16-L24">[36]</a> <a
href="evaluation_results/execution_logs.md#L38-L46">[37]</a> <a
href="evaluation_results/execution_logs.md#L104-L113">[38]</a> <a
href="evaluation_results/execution_logs.md#L40-L48">[40]</a> <a
href="evaluation_results/execution_logs.md#L50-L58">[41]</a> <a
href="evaluation_results/execution_logs.md#L62-L70">[44]</a> <a
href="evaluation_results/execution_logs.md#L72-L75">[45]</a> <a
href="evaluation_results/execution_logs.md#L140-L146">[46]</a>
execution_logs.md</p>
<p>&lt;evaluation_results/execution_logs.md&gt;</p>
<p><a href="PythonApp/master_clock_synchronizer.py#L86-L94">[50]</a> <a
href="PythonApp/master_clock_synchronizer.py#L95-L102">[51]</a> <a
href="PythonApp/master_clock_synchronizer.py#L86-L102">[52]</a> <a
href="PythonApp/master_clock_synchronizer.py#L164-L172">[53]</a>
master_clock_synchronizer.py</p>
<p>&lt;PythonApp/master_clock_synchronizer.py&gt;</p>
<p><a
href="PythonApp/webcam/cv_preprocessing_pipeline.py#L72-L80">[54]</a>
cv_preprocessing_pipeline.py</p>
<p>&lt;PythonApp/webcam/cv_preprocessing_pipeline.py&gt;</p>
<p><a href="PythonApp/shimmer_manager.py#L241-L249">[55]</a> <a
href="PythonApp/shimmer_manager.py#L250-L258">[56]</a> <a
href="PythonApp/shimmer_manager.py#L241-L258">[57]</a> <a
href="PythonApp/shimmer_manager.py#L134-L143">[58]</a> <a
href="PythonApp/shimmer_manager.py#L269-L278">[59]</a> <a
href="PythonApp/shimmer_manager.py#L280-L289">[60]</a> <a
href="PythonApp/shimmer_manager.py#L145-L151">[61]</a>
shimmer_manager.py</p>
<p>&lt;PythonApp/shimmer_manager.py&gt;</p>
<h1 id="references-6">References</h1>
<p>[1] W. Boucsein, <em>Electrodermal Activity</em>. Springer Science
&amp; Business Media, 2012. ISBN: 978-1-4614-1126-0. DOI:
10.1007/978-1-4614-1126-0</p>
<p>[2] Neural Sense, “Galvanic Skin Response Technology,” 2024.
[Online]. Available: https://www.neuralsense.com/tech</p>
<p>[3] A. Jangra, S. Dwivedi, S. Sriram, and S. Krishnamurthy, “Galvanic
skin response in psychology, physiology, and free-will in humans: A
systematic review,” <em>Int. J. Psychology</em>, PMC, 2021. [Online].
Available: https://pmc.ncbi.nlm.nih.gov/articles/PMC8187483/</p>
<p>[4] Various Authors, “Neural mechanisms of stress susceptibility and
resilience,” <em>Nature Scientific Reports</em>, vol. 9, 41172, 2019.
DOI: 10.1038/s41598-019-41172-7</p>
<p>[5] Various Authors, “Human Stress Recognition from Facial Thermal
Images,” <em>Computer Modeling in Engineering and Sciences</em>,
vol. 130, no. 2, Tech Science Press, 2021. [Online]. Available:
https://www.techscience.com/CMES/v130n2/45961/html</p>
<p>[6] RTI International, “Using Thermal Imaging to Measure Mental
Effort: The Nose Knows,” 2024. [Online]. Available:
https://www.rti.org/rti-press-publication/using-thermal-imaging-measure-mental-effort-nose-know</p>
<p>[7] Various Authors, “Electrodermal Activity and Physiological
Responses,” <em>Science Direct</em>, 2025. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S136984782500244X</p>
<p>[8] Shimmer Sensing, “Shimmer3 GSR+ Unit Specifications,” 2024.
[Online]. Available:
https://www.shimmersensing.com/product/shimmer3-gsr-unit/</p>
<p>[9] LSL Community, “Lab Streaming Layer (LSL) Documentation,” 2024.
[Online]. Available: https://labstreaminglayer.readthedocs.io/</p>
<p>[10] MDPI, “Applied Sciences - Special Issue on Affective Computing,”
<em>Applied Sciences</em>, vol. 10, no. 8, 2924, 2020. DOI:
10.3390/app10082924</p>
<p>[11] H. Selye, <em>Stress Without Distress</em>. J.B. Lippincott
Company, Philadelphia, PA, 1974.</p>
<p>[12] H. Selye, <em>The Stress of Life</em>. McGraw-Hill, New York,
NY, 1956.</p>
<p>[13] Google Android Developers, “CameraX Documentation,” 2024.
[Online]. Available: https://developer.android.com/training/camerax</p>
<p>[14] Nordic Semiconductor, “Android BLE Library,” GitHub Repository,
2024. [Online]. Available:
https://github.com/NordicSemiconductor/Android-BLE-Library</p>
<p>[15] Shimmer Engineering, “Shimmer Java Android API,” GitHub
Repository, 2024. [Online]. Available:
https://github.com/ShimmerEngineering/Shimmer-Java-Android-API</p>
<p>[16] Serenegiant, “UVCCamera: USB Video Class (UVC) Camera Library
for Android,” GitHub Repository, 2024. [Online]. Available:
https://github.com/saki4510t/UVCCamera</p>
<p>[17] Riverbank Computing, “PyQt6 Documentation,” 2024. [Online].
Available: https://doc.qt.io/qtforpython/</p>
<p>[18] W. Jakob, “PyBind11: Python Bindings for C++,” 2024. [Online].
Available: https://pybind11.readthedocs.io/</p>
<p>[19] Python Software Foundation, “Zeroconf/mDNS Service Discovery,”
2024. [Online]. Available: https://python-zeroconf.readthedocs.io/</p>
<p>[20] Topdon Community, “Developing a Flutter App with Topdon Thermal
Camera,” 2024. [Online]. Available:
https://community.topdon.com/detail?id=352&amp;type=1</p>
<p>[21] J. Postel, “RFC 793: Transmission Control Protocol,” IETF, 1980.
[Online]. Available: https://tools.ietf.org/html/rfc793</p>
<p>[22] OpenCV Foundation, “OpenCV: Open Source Computer Vision
Library,” 2024. [Online]. Available: https://opencv.org/</p>
<p>[23] pandas Development Team, “pandas: Python Data Analysis Library,”
2024. [Online]. Available: https://pandas.pydata.org/</p>
<p>[24] HDF5 Group, “h5py: HDF5 for Python,” 2024. [Online]. Available:
https://www.h5py.org/</p>
