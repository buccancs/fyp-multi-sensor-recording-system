# Chapter 3: Requirements

## 3.1 Problem Statement and Research Context

The system is developed to support **contactless Galvanic Skin Response
(GSR) prediction research**. Traditional GSR measurement requires
contact sensors attached to a person's skin, but this project aims to
bridge **contact-based and contact-free physiological monitoring**. In
essence, the system enables researchers to collect **synchronized
multi-modal data** -- combining **wearable GSR sensor readings** with
**contactless signals** like thermal imagery and video -- to facilitate
the development of models that predict GSR without direct skin contact.
This addresses a key research gap: providing a reliable way to acquire **ground-truth GSR data** alongside contactless sensor data in experiments, ensuring all data streams are aligned in time for analysis [Arch2024].

The research context for this system is physiological computing and
affective computing. The focus is on **stress and emotion analysis**,
where GSR is a common measure of sympathetic nervous system activity. By
integrating **thermal cameras, RGB video, and inertial sensors** with
the GSR sensor, the system creates a rich dataset for exploring how
observable signals (like facial thermal patterns or motion) correlate
with actual skin conductance changes. The **multi-sensor recording
platform** operates in real-world environments (e.g. lab or field
studies) and emphasizes **temporal precision and data integrity** so
that subtle physiological responses can be captured and later aligned
for machine learning model training [Arch2024].
Overall, the system's goal is to facilitate experiments that would
**simultaneously record a participant's physiological responses and
visual/thermal cues**, providing a foundation for research into
contactless stress detection.

## 3.2 Requirements Engineering Approach

The requirements for the system were derived using an **iterative, research-driven approach** [IEEE29148]. Initially, high-level objectives (such as *"enable synchronized GSR and video recording"*) were identified from the research goals. These were refined through *requirements elicitation* that included the needs of researchers conducting experiments (the primary stakeholders) and the technical constraints of available hardware. The project followed a **prototyping and refinement methodology**: early versions of the system were implemented and tested, and feedback was used to update the requirements. For example, as the implementation progressed, additional needs such as **data encryption** and **device fault tolerance** were recognized and added to the requirements (evident from commit history showing security checks and recovery features being introduced).

Requirements engineering was performed in alignment with IEEE guidelines [IEEE830]. Each requirement was documented with a unique ID and categorized (functional vs. non-functional). The team maintained close alignment between requirements and implementation -- the repository's structure and commit messages show that whenever a new capability was implemented (e.g. a **calibration module** or **time synchronization service**), it corresponded to a defined requirement. Traceability was maintained through the comprehensive matrix presented in Section 3.6. In summary, the approach was **incremental and user-focused**: starting from the core research use cases, and continuously refining the system requirements as technical insights were gained during development.

## 3.3 Requirements Summary

This section provides a consolidated overview of all system requirements, followed by detailed descriptions and traceability evidence.

### 3.3.1 Functional Requirements Summary

| ID  | Description | Priority | Evidence |
|-----|-------------|----------|----------|
| FR1 | Multi-Device Sensor Integration | Must | ShimmerManager, AndroidDeviceManager |
| FR2 | Synchronized Multi-Modal Recording | Must | SessionManager, PC-Android coordination |
| FR3 | Time Synchronization Service | Must | NTPTimeServer, clock alignment protocols |
| FR4 | Session Management | Must | SessionManager lifecycle controls |
| FR5 | Data Recording and Storage | Must | Multi-format data streams, real-time I/O |
| FR6 | User Interface for Monitoring & Control | Must | PyQt5 GUI, device status panels |
| FR7 | Device Synchronization and Signals | Should | Sync signal broadcasting, JSON protocol |
| FR8 | Fault Tolerance and Recovery | Should | SessionSynchronizer, offline queue management |
| FR9 | Calibration Utilities | Could | Camera calibration algorithms, pattern detection |
| FR10 | Data Transfer and Aggregation | Must | FileTransferManager, automated data collection |

### 3.3.2 Non-Functional Requirements Summary

| ID   | Description | Priority | Verification | Evidence |
|------|-------------|----------|--------------|----------|
| NFR1 | Performance (Real-Time Data Handling) | Must | 128Hz sampling, 30fps concurrent streams | Multi-threading, async processing |
| NFR2 | Temporal Accuracy | Must | <5ms clock offset, jitter measurement | NTP sync logs, timestamp precision |
| NFR3 | Reliability and Fault Tolerance | Must | Graceful device failure handling | Auto-reconnect, session recovery |
| NFR4 | Data Integrity and Validation | Must | Range validation, completeness checks | GSR value bounds, file verification |
| NFR5 | Security | Should | TLS encryption, token authentication | Runtime security checker, config validation |
| NFR6 | Usability | Should | Intuitive GUI, minimal user interaction | UI design patterns, default settings |
| NFR7 | Scalability | Should | 8+ concurrent devices, 120+ min sessions | Network capacity, file chunking |
| NFR8 | Maintainability and Modularity | Could | Component separation, configuration externalization | Modular architecture, config files |

## 3.4 Functional Requirements Overview

The functional requirements of the system are detailed below. Each requirement is labeled (FR#) with explicit priority assignment and described in terms of what the system **shall** do:

- **FR1: Multi-Device Sensor Integration (Priority: Must)** -- The system shall support connecting and managing multiple sensor devices simultaneously. This includes **discovering and pairing Shimmer GSR sensors** via direct Bluetooth or through an Android device acting as a bridge. If no real sensors are available, the system shall offer a **simulation mode** to generate dummy sensor data for testing.

- **FR2: Synchronized Multi-Modal Recording (Priority: Must)** -- The system shall start and stop data recording **synchronously** across all connected devices. When a recording session is initiated, the PC controller instructs each Android device to begin recording **GSR data**, **video (RGB camera)**, and **thermal imaging** in parallel. At the same time, the PC begins logging local sensor streams (e.g. from any directly connected Shimmer devices). All streams share a common session timestamp to enable later alignment.

- **FR3: Time Synchronization Service (Priority: Must)** -- The system shall synchronize clocks across devices to ensure all data is time-aligned. The PC provides a time sync mechanism (e.g. an NTP-like time server on the local network) so that each Android device can calibrate its clock to the PC's clock before and during recording. This achieves a sub-millisecond timestamp accuracy between GSR readings and video frames, which is crucial for data integrity.

- **FR4: Session Management (Priority: Must)** -- The system shall organize recordings into sessions, each with a unique ID or name. It shall allow the user (researcher) to **create a new session**, automatically timestamped, and then **terminate the session** when finished. Upon session start, a directory is created on the PC to store data, and a session metadata file is initialized. When the session ends, the metadata (start/end time, duration, status) is finalized and saved. Only one session can be active at a time, preventing overlap.

- **FR5: Data Recording and Storage (Priority: Must)** -- For each session, the system shall record: (a) **Physiological sensor data** from the Shimmer GSR module (including GSR conductivity and any other enabled channels like PPG, accelerometer, etc., sampled at a default 128 Hz), and (b) **Video and thermal data** from each Android device (with at least 1920Ã—1080 resolution video at 30 FPS). Sensor readings are streamed to the PC in real-time and written to local files (CSV format for numerical data) as they arrive, to avoid data loss. Each Android device stores its own raw video/thermal files during recording and later transfers them to the PC (see FR10). The system shall handle **audio recording** as well if enabled (e.g. microphone audio at 44.1 kHz), syncing it with other data streams.

- **FR6: User Interface for Monitoring & Control (Priority: Must)** -- The system shall provide a GUI on the PC for the researcher to control sessions and monitor devices. This includes listing connected devices and their status (e.g. battery level, streaming/recording state), letting the user start/stop sessions, and showing indicators like recording timers and data sample counts. The UI should also display preview feeds or status updates periodically (for example, updating every few seconds with how many samples have been received). **Device panels** in the UI will indicate if a device is disconnected or has errors so the user can take action.

- **FR7: Device Synchronization and Signals (Priority: Should)** -- The system shall coordinate multiple devices by sending control commands and sync signals. For example, the PC can broadcast a **synchronization cue** (such as a flash or buzzer) to all Android devices to mark a moment in time across videos. These signals (e.g. visual flash on phone screens) help with aligning footage during analysis. The system uses a JSON-based command protocol so that the PC can instruct devices to start/stop recording and perform actions in unison.

- **FR8: Fault Tolerance and Recovery (Priority: Should)** -- If a device (Android or sensor) disconnects or fails during an active session, the system shall **detect the event** and continue the session with the remaining devices. The PC will log a warning and mark the device as offline. When the device reconnects, it should be able to rejoin the ongoing session seamlessly. The system will attempt to **recover the session state** on that device by re-synchronizing and sending any queued commands that were missed while it was offline. This ensures a temporary network drop doesn't invalidate the entire session.

- **FR9: Calibration Utilities (Priority: Could)** -- The system shall include tools for **calibrating sensors and cameras**. In particular, it provides a calibration procedure for aligning the thermal camera field-of-view with the RGB camera (e.g. using a checkerboard pattern). The user can perform a calibration session where images are captured and calibration parameters are computed. Configuration parameters for calibration (such as pattern type, pattern size, number of images, etc.) are adjustable in the system settings. The resulting calibration data is saved so that recorded thermal and visual data can be accurately merged in analysis.

- **FR10: Data Transfer and Aggregation (Priority: Must)** -- After a session is stopped, the system shall support transferring all recorded data from each Android device to the PC for central storage. The Android application will package the session's files (video, thermal images, any local sensor logs) and send them to the PC over the network. The PC, upon receiving each file, saves it in the appropriate session folder and updates the session metadata to include that file entry (with file type and size). This automation ensures that the researcher can easily retrieve all data without manually offloading devices. If any file fails to transfer, the system logs an error and (if possible) retries the transfer, so that data is not silently lost.

## 3.6 Requirements Traceability Matrix

This section provides comprehensive traceability between requirements and their implementation evidence in the system codebase.

### 3.6.1 Functional Requirements Traceability

| Requirement ID | Implementing Components | Key Files/Classes | Configuration | Commits/References |
|----------------|------------------------|-------------------|---------------|--------------------|
| FR1 | ShimmerManager, AndroidDeviceManager | shimmer_manager.py#L244-L253, L260-L268, L268-L274 | config.json device settings | Device discovery and pairing logic |
| FR2 | SessionManager, PC-Android coordination | shimmer_pc_app.py#L120-L128, session coordination | config.json recording settings | Synchronized recording initiation |
| FR3 | NTPTimeServer, clock synchronization | ntp_time_server.py#L66-L74, L26-L34 | Time sync configuration | Clock alignment protocols |
| FR4 | SessionManager | session_manager.py#L64-L73, L82-L91 | Session metadata schema | Session lifecycle management |
| FR5 | Data recording pipelines | shimmer_manager.py#L128-L135, config.json#L18-L26 | Recording format settings | Multi-modal data capture |
| FR6 | PyQt5 GUI components | shimmer_pc_app.py#L176-L185, L260-L267, L234-L242 | UI configuration | User interface and monitoring |
| FR7 | JSON protocol, sync signals | shimmer_pc_app.py#L170-L173, protocol definitions | Command protocol specs | Device coordination signals |
| FR8 | SessionSynchronizer, recovery logic | session_synchronizer.py#L153-L161, L163-L171, L174-L182 | Fault tolerance settings | Error handling and recovery |
| FR9 | Calibration modules | config.json#L54-L62, calibration algorithms | Calibration parameters | Camera alignment procedures |
| FR10 | FileTransferManager | FileTransferManager.kt#L124-L132, L142-L150, L156-L165 | Transfer protocols | Data aggregation workflows |

### 3.6.2 Non-Functional Requirements Traceability

| Requirement ID | Verification Method | Evidence Location | Performance Metrics | Validation Tests |
|----------------|-------------------|-------------------|-------------------|------------------|
| NFR1 | Performance monitoring | shimmer_manager.py#L169-L177, config.json#L30-L37 | 128Hz sampling, 30fps concurrent | Load testing results |
| NFR2 | Timestamp analysis | ntp_time_server.py#L26-L34, sync logs | <5ms offset tolerance | Clock drift measurements |
| NFR3 | Fault injection testing | session_synchronizer.py recovery mechanisms | Recovery time metrics | Reliability test suite |
| NFR4 | Data validation checks | shimmer_manager.py#L130-L138, L184-L192 | Range validation (0.0-100.0 Î¼S) | Integrity verification |
| NFR5 | Security audits | runtime_security_checker.py#L110-L119, L140-L148 | TLS encryption, 32-char tokens | Security compliance tests |
| NFR6 | Usability testing | shimmer_pc_app.py UI components, config.json#L40-L48 | User interaction metrics | UX evaluation studies |
| NFR7 | Scalability testing | config.json#L6-L14, L2-L5 | 8+ devices, 120+ min sessions | Performance scaling tests |
| NFR8 | Architecture analysis | Modular component design, config externalization | Coupling metrics | Maintainability assessments |

## 3.5 Non-Functional Requirements

In addition to the core functionality, the system must meet several non-functional requirements that ensure it is usable in a research setting. Each NFR includes explicit verification criteria and evidence:

- **NFR1: Performance (Real-Time Data Handling) (Priority: Must)** -- The system must handle data in real-time, with minimal latency and sufficient throughput. It should support at least **128 Hz sensor sampling and 30 FPS video recording concurrently** without data loss or buffering issues. The design uses multi-threading and asynchronous processing to achieve this. Video is recorded at ~5 Mbps bitrate and audio at 128 kbps, which the system must write to storage in real-time. Even with multiple devices (e.g. 3+ cameras and a GSR sensor), the system should not drop frames or samples due to performance bottlenecks.
  - *Verification*: Performance monitoring logs, sample count validation, concurrent stream testing

- **NFR2: Temporal Accuracy (Priority: Must)** -- Clock synchronization accuracy between devices should be on the order of milliseconds or better. The system's built-in NTP time server and sync protocol aim to keep timestamp differences very low (e.g. **<5 ms offset and jitter** as logged after synchronization). This is critical for valid sensor fusion; hence the system continuously synchronizes device clocks during a session. Timestamp precision is maintained in all logs (to milliseconds) and all devices use the PC's time reference for consistency.
  - *Verification*: NTP sync logs analysis, timestamp drift measurement, offset distribution statistics

- **NFR3: Reliability and Fault Tolerance (Priority: Must)** -- The system must be robust to interruptions. If a sensor or network link fails, the rest of the system continues recording unaffected (as per FR8). Data already recorded must be safely preserved even if a session ends unexpectedly (e.g. the PC app crashing). The system's session design ensures that files are written incrementally and closed properly on stop, to avoid corruption. A **recovery mechanism** is in place to handle device reconnections (queuing messages while a device is offline). In addition, the Shimmer device interface has an auto-reconnect option to try re-establishing Bluetooth connections automatically.
  - *Verification*: Fault injection testing, crash recovery validation, data integrity checks

- **NFR4: Data Integrity and Validation (Priority: Must)** -- All data recorded by the system should be accurate and free of corruption. The system enables a data validation mode for sensor data to check incoming values are within expected ranges (for example, GSR values are checked to be between 0.0 and 100.0 Î¼S). Each file transfer from devices is verified for completeness (file sizes are known and logged in metadata). Session metadata acts as a manifest so that missing or inconsistent files can be detected easily. Also, the system will not overwrite existing session data -- each session gets a unique timestamped folder to avoid conflicts.
  - *Verification*: Range validation logs, file integrity checksums, session completeness audits

- **NFR5: Security (Priority: Should)** -- The system must ensure the **security and privacy** of recorded data, as it may involve sensitive physiological information. All network communication between the PC and Android devices is encrypted (the configuration enables TLS for the protocol). The system requires authentication tokens for device connections (configurable minimum token length of 32 characters) to prevent unauthorized devices from joining the session. Security checks at startup will warn if encryption or authentication is not properly configured. Recorded data files are stored locally on the PC; if cloud or external transfer is needed, it is done explicitly by the researcher (there is no inadvertent data leak). Additionally, the **file permissions** and environment are checked on startup (to avoid using insecure defaults).
  - *Verification*: TLS handshake validation, token strength testing, file permission audits

- **NFR6: Usability (Priority: Should)** -- The system should be reasonably easy to use for researchers who are not necessarily software experts. The **Graphical User Interface** on the PC should be intuitive, with clear controls to start/stop sessions and indicators for system status. For example, the UI shows a **recording indicator** when a session is active and displays device statuses (connected/disconnected, recording, battery) in real-time. Default settings (e.g. dark theme, window size) are provided to ensure a good user experience out of the box. The Android app is designed to run with minimal user interaction after initial setup -- typically the researcher just needs to mount the devices and tap "Connect", with the PC orchestrating the rest. User manuals or on-screen guidance is provided for tasks like calibration.
  - *Verification*: User experience testing, interface responsiveness measurement, setup time analysis

- **NFR7: Scalability (Priority: Should)** -- The architecture should scale to accommodate **multiple concurrent devices** and longer recordings. The system is tested with up to *8 Android devices* streaming or recording simultaneously (the config allows up to 10 connections). The networking and session management components are designed to handle dynamic addition of devices. Likewise, the system supports sessions up to at least *120 minutes* in duration by default. To manage large video files, recordings can be chunked into ~1 GB segments automatically so that file sizes remain manageable. This ensures that even high-resolution videos over long sessions do not overwhelm the file system or become impossible to post-process.
  - *Verification*: Load testing with maximum device count, duration stress testing, file size monitoring

- **NFR8: Maintainability and Modularity (Priority: Could)** -- Although primarily a development concern, the system is built in a modular way to facilitate maintenance. Components are separated (e.g., a **Calibration Manager**, **Session Manager**, **Shimmer Manager**, **Network Server** are distinct modules), following clear interfaces. This modular design (observable in the repository structure and code) makes it easier to update one part (like swapping out the thermal camera SDK) without affecting others. Configuration is also externalized (`config.json` and various settings in code) so that changes in requirements (e.g., new sensor types, different sampling rates) can be accommodated by editing configurations rather than rewriting code. Finally, the project includes test scripts and logging to aid in debugging, which contributes to maintainability.
  - *Verification*: Module coupling analysis, configuration change testing, code maintainability metrics

*(The non-functional requirements were verified through system testing and by examining configuration parameters. Complete traceability and verification evidence is documented in Section 3.6.)*

## 3.7 Use Case Scenarios

To illustrate how the system is intended to be used, this section
describes several **use case scenarios**. Each scenario outlines the
typical interaction between the **user (researcher)** and the system,
along with how the system's components work together to fulfill the
requirements.

### Use Case 1: Conducting a Multi-Modal Recording Session

**Description:** A researcher initiates and completes a recording
session capturing GSR data alongside video and thermal streams from
multiple devices. This is the primary use of the system, corresponding
to a live experiment with a participant.

- **Primary Actor:** Researcher (system operator).

- **Secondary Actors:** Participant (subject being recorded), though
  they do not directly interact with the system UI.

- **Preconditions:**

- The Shimmer GSR sensor is charged and either connected to the PC (via
  Bluetooth dongle) or paired with an Android device.

- Android recording devices are powered on, running the recording app,
  and on the same network as the PC. The PC application is running and
  all devices have synchronized their clocks (either via initial NTP
  sync or prior calibration).

- The researcher has configured any necessary settings (e.g. chosen a
  session name, verified camera focus, etc.).

- **Main Flow:**

- The Researcher opens the PC control interface and **creates a new
  session** (providing a session name or accepting a default). The
  system validates the name and creates a session folder and metadata
  file on
  disk[\[10\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py#L64-L73).
  The session is now "active" but not recording yet.

- The Researcher selects the devices to use. For example, they ensure
  the Shimmer sensor appears in the device list and one or more Android
  devices show as "connected" in the UI. If the Shimmer is not yet
  connected, the Researcher clicks "Scan for Devices". The system
  performs a scan: it finds the Shimmer sensor either directly via
  Bluetooth or through an Android's paired
  devices[\[4\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py#L244-L253)[\[5\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py#L260-L268).
  The Researcher then clicks "Connect" for the Shimmer. The system
  establishes a connection (or uses a simulated device if the real
  sensor is unavailable) and updates the UI status to "Connected".

- The Researcher checks that video previews from each Android (if
  available) are showing in the UI (small preview panels) and that the
  GSR signal is streaming (e.g., a live plot or at least a sample
  counter incrementing). Internally, the PC has started a background
  thread receiving data from the Shimmer sensor
  continuously[\[45\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L191-L200).
  The system also maintains a heartbeat to each Android (pinging every
  few seconds) to ensure connectivity.

- The Researcher initiates recording by clicking "Start Recording". The
  PC sends a **start command** to all connected Android devices (with a
  session ID). Each Android begins recording its camera (and thermal
  sensor, if present) and optionally starts streaming its own sensor
  data (if any) back to
  PC[\[7\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L120-L128).
  Simultaneously, the PC instructs the Shimmer Manager to start logging
  data to
  file[\[46\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L130-L138).
  This is done nearly simultaneously for all devices. The PC's session
  manager marks the session status as "recording" and timestamps the
  start time.

- During the recording, the Researcher can observe real-time status. For
  example, the UI might display the **elapsed time**, the number of data
  samples received so far, and the count of connected devices. Every 30
  seconds, the system logs a status summary (e.g., "Status: 1 Android, 1
  Shimmer, 3000
  samples")[\[18\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L260-L267).
  If the Researcher has a specific event to mark, they can trigger a
  sync signal: for instance, pressing a "Flash Sync" button. When
  pressed, the system calls `send_sync_signal` to all Androids to flash
  their screen
  LEDs[\[20\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L170-L173)
  (creating a visible marker in the videos) and logs the event in the
  GSR data stream.

- If any device disconnects mid-session (e.g., an Android phone's WiFi
  drops out), the system warns the Researcher via the UI (perhaps
  highlighting that device in red). The recording on that device might
  continue offline (the Android app will still save its local video).
  The PC's Session Synchronizer marks the device as offline and queues
  any commands for
  it[\[3\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L153-L161).
  The Researcher can continue the session if the other streams are still
  running. When the disconnected device comes back online (e.g., WiFi
  reconnects), the system automatically detects it, re-synchronizes the
  session state, and executes any missed commands (all in the
  background)[\[21\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L163-L171)[\[22\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L174-L182).
  This recovery happens without user intervention, ensuring the session
  can proceed.

- The Researcher decides to end the recording after, say, 15 minutes.
  They click "Stop Recording" on the PC interface. The PC sends stop
  commands to all Androids, which then cease recording their
  cameras[\[47\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L146-L155).
  The Shimmer Manager stops logging GSR data at the same
  time[\[48\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L151-L159).
  Each component flushes and closes its output files. The session
  manager marks the session as completed, calculates the duration, and
  updates the session metadata (end time, duration, and
  status)[\[49\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py#L86-L95)[\[50\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py#L99-L102).
  A log message confirms the session has ended along with its duration
  and sample count
  stats[\[51\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_pc_app.py#L156-L164).

- After stopping, the system **automatically initiates data transfer**
  from the Android devices. The File Transfer Manager on each Android
  packages the recorded files (e.g., `video_20250807_...mp4`, thermal
  data, etc.) and begins sending them to the PC, one file at a
  time[\[24\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/AndroidApp/src/main/java/com/multisensor/recording/managers/FileTransferManager.kt#L124-L132)[\[25\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/AndroidApp/src/main/java/com/multisensor/recording/managers/FileTransferManager.kt#L142-L150).
  The PC receives each file (via its network server) and saves it into
  the session folder, simultaneously calling
  `SessionManager.add_file_to_session()` to record the file's name and
  size in the
  metadata[\[26\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py#L130-L138).
  A progress indicator may be shown to the Researcher.

- Once all files are transferred, the system notifies the Researcher
  that the session data collection is complete (e.g., "Session
  15min_stress_test completed -- 5 files saved"). The Researcher can
  then optionally review summary statistics (the UI might show, for
  example, average GSR level, or simply confirm the number of files and
  total data size). The session is now closed and all resources are
  cleaned up.

- **Postconditions:** All recorded data (GSR CSV, video files, etc.) are
  safely stored in the PC's session directory. The session metadata JSON
  lists all devices that participated and all files collected. The
  system remains running, and the researcher could start a new session
  if needed. The participant's involvement is done, and the data is
  ready for analysis (outside the scope of the recording system). If any
  device failed to transfer data, the researcher is made aware so they
  can retrieve it manually if possible.

- **Alternate Flows:**\
  a. *No Shimmer available:* If the Shimmer sensor is not connected or
  malfunctions, the Researcher can still run a session with just
  video/thermal. The system will log that no GSR device is present, and
  it can operate in a video-only mode (possibly using a **simulated GSR
  signal** for
  demonstration).\
  b. *Calibration needed:* If this is the first session or devices have
  been re-arranged, the Researcher might perform a **calibration
  routine** before step 4. In that case, they would use the Calibration
  Utility (see Use Case 2) to calibrate cameras. Once calibration is
  done and saved, the recording session proceeds as normal.\
  c. *Device battery low:* During step 5, if an Android's battery is
  critically low, the system could alert the Researcher (since the
  device status includes battery
  level).
  The Researcher might decide to stop the session early or replace the
  device. The system will include the battery status in metadata for
  transparency.\
  d. *Network loss at end:* If the network connection to a device is
  lost exactly when "Stop" is pressed, the PC might not immediately
  receive confirmation from that device. In this case, the PC will mark
  the device as offline (as in step 6) and proceed to finalize the
  session with whatever data it has. Later, when the device reconnects,
  the Session Synchronizer can still trigger the file transfer for that
  device's data so it eventually gets saved on the PC.

### Use Case 2: Camera Calibration for Thermal Alignment

**Description:** Before conducting recordings that involve a thermal
camera, the researcher performs a calibration procedure to align the
thermal camera's view with the RGB camera view. This ensures that data
from these two modalities can be compared pixel-to-pixel in analysis.

- **Primary Actor:** Researcher.

- **Preconditions:** At least one Android device with both an RGB and a
  thermal camera (or an external thermal camera attached) is available.
  A calibration pattern (e.g. a black-and-white checkerboard) is printed
  and ready. The system's calibration settings (pattern size, etc.) are
  configured if
  needed.

- **Main Flow:**

- The Researcher opens the **Calibration Tool** in the PC application
  (or on the Android app, depending on implementation -- assume PC-side
  coordination). They select the device(s) to calibrate (e.g., "Device A
  -- RGB + Thermal").

- The system instructs the device to enter calibration mode. Typically,
  the Android app might open a special calibration capture activity
  (with perhaps an overlay or just using both cameras). The Researcher
  holds the checkerboard pattern in front of the cameras and ensures it
  is visible to both the RGB and thermal cameras.

- The Researcher initiates capture (maybe pressing a "Capture Image"
  button). The device (or PC via the device) captures a pair of images
  -- one from the RGB camera and one from the thermal camera -- at the
  same moment. It may need multiple images from different angles; the
  configuration might specify capturing, say, 10
  images[\[23\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/protocol/config.json#L54-L62).
  The system gives feedback after each capture (e.g., "Image 1/10
  captured").

- After the required number of calibration images are collected, the
  Researcher clicks "Compute Calibration". The system runs a calibration
  algorithm (likely implementing Zhang's method for camera calibration)
  on the collected image pairs. This computes parameters like camera
  intrinsics for each camera and the extrinsic transform aligning
  thermal to RGB.

- The system stores the resulting calibration parameters (e.g., in a
  calibration result file or in config). It also might display an
  estimate of calibration error (so the Researcher can judge quality).
  For instance, if the reprojection error exceeds the
  threshold[\[23\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/protocol/config.json#L54-L62)
  (say threshold = 1.0 pixel), the system might warn that the
  calibration quality is low.

- The Researcher is satisfied with the calibration (error is
  acceptable). They save the calibration profile. Now the system will
  use this calibration data in future sessions to correct or align
  thermal images to the RGB frame if needed (this might be done in
  post-processing rather than during recording). The Researcher exits
  the calibration mode.

- **Alternate Flows:**\
  a. *Calibration failure:* If the system cannot detect the calibration
  pattern in the images (e.g., poor contrast in thermal image), it
  notifies the Researcher. The Researcher can then recapture images
  (maybe adjust the pattern distance or lighting) until the system
  successfully computes a calibration.\
  b. *Partial calibration:* The Researcher may choose to only calibrate
  intrinsics of each camera separately (for example, if thermal-RGB
  alignment is less important than ensuring each camera's lens
  distortion is corrected). In this case, the flow would be adjusted to
  capturing images of a known grid for each camera independently.\
  c. *Using stored calibration:* If calibration was done previously, the
  Researcher might skip this use case entirely and rely on the stored
  calibration parameters. The system allows loading a saved calibration
  file, which then becomes active for subsequent recordings.

**Use Case 3 (Secondary): Reviewing and Managing Session Data**
(Optional) -- *This use case would describe how a researcher can use the
system to review past session metadata and possibly replay or export
data.* (For brevity, this is not expanded here, but the system does
include features like session listing and possibly data export tools,
given that a web UI template for sessions exists.)

*(The above scenarios demonstrate the system's functionality in context.
They confirm that the requirements -- from multi-device synchronization
to calibration -- all serve real user workflows. The sequence of
interactions in Use Case 1, especially, shows how the system meets the
need for synchronized multi-modal data collection in a practical
experiment setting.)*

## 3.8 System Analysis (Architecture & Data Flow)

This section presents the system architecture and data flow design, supported by architectural diagrams that illustrate key design decisions and data synchronization mechanisms.

**System Architecture:** The system adopts a **distributed architecture** with a central **PC Controller** and multiple **Mobile Recording Units**. The PC (a Python desktop application) acts as the master, coordinating all devices, while each Android device runs a recording application that functions as a client node. This architecture is essentially a **hub-and-spoke topology**, where the PC hub maintains control and timing, and the spokes (sensors/cameras) carry out data collection.

![System Requirements Architecture](../diagrams/06_system_requirements_architecture.png)

*Figure 3.1: System architecture showing PC master-controller with distributed Android recording nodes in star topology. NTP-based synchronization paths ensure temporal alignment across all data streams.*

![Complete Data Flow Architecture](../diagrams/05_complete_data_flow_architecture.png)

*Figure 3.2: Data flow and synchronization pipeline illustrating capture â†’ timestamping â†’ storage/transfer sequence. Offset measurement and correction points are highlighted for multi-modal alignment.*

On the PC side, the software is organized into modular managers, each responsible for a subset of functionality: 
- The **Session Manager** handles the overall session lifecycle (creation, metadata logging, and closure)
- The **Network Server** component manages communication with Android devices over TCP/IP (listening on a specified port, e.g. 9000), using a custom JSON-based protocol for commands and status messages
- The **Shimmer Manager** deals with the Shimmer GSR sensors, including Bluetooth connectivity and data streaming to the PC. It also multiplexes data from multiple sensors and writes sensor data to CSV files in real-time
- The **Time Synchronization Service** (Master Clock) runs on the PC to keep device clocks aligned. An `NTPTimeServer` thread on the PC listens on a port (e.g. 8889) and services time-sync requests from clients
- The **GUI Module** provides the desktop interface with panels for device status, session control, and live previews

On the Android side, each device's application is composed of several
components: - A **Recording Controller** that receives start/stop
commands from the PC and controls the local recording (camera and sensor
capture). - Separate **Recorder modules** for each modality: e.g.,
`CameraRecorder` for RGB video, `ThermalRecorder` for thermal imaging,
and `ShimmerRecorder` if the Android is paired to a Shimmer
sensor[\[55\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/architecture.md#L26-L34).
These recorders interface with hardware (camera APIs, etc.) and save
data to local storage. - A **Network Client** (or Device Connection
Manager) that maintains the socket connection to the PC's server. It
listens for commands (e.g., start/stop, sync signal) and sends back
status updates or data as needed. - A **FileTransferManager** on
Android, which, after recording, handles sending the recorded files to
the PC upon
request[\[24\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/AndroidApp/src/main/java/com/multisensor/recording/managers/FileTransferManager.kt#L124-L132)[\[25\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/AndroidApp/src/main/java/com/multisensor/recording/managers/FileTransferManager.kt#L142-L150). -
Utility components like a **Security Manager** (ensuring encryption if
TLS is used), a **Storage Manager** (to check available space and
organize files), etc., are also part of the design (many of these are
hinted by the architecture and config files).

**Communication and Data Flow:** All communication between the PC and
Android devices uses a **client-server model**. The PC runs the server
(listening on a specified host/port, with a maximum number of
connections
defined)[\[41\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/protocol/config.json#L6-L14),
and each Android client connects to it when ready. Messages are likely
encoded in JSON and could be sent over a persistent TCP socket (the
config specifies
`protocol: "TCP"`[\[41\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/protocol/config.json#L6-L14)).
Important message types include: device registration/hello, start
session command, stop session command, sync signal command, status
update from device, file transfer requests, etc.

During a session, the **data flow** is as follows: - **Shimmer GSR
Data:** If a Shimmer sensor is directly connected to the PC, it streams
data via Bluetooth to the PC's Shimmer Manager, which then immediately
enqueues the data for writing to a CSV and also triggers any real-time
displays. If the Shimmer is connected to an Android (i.e.,
Android-mediated), the sensor data first goes to the Android (via
Bluetooth), and the Android then forwards each GSR sample (or batch of
samples) over the network to the
PC[\[56\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py#L260-L269)[\[57\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py#L261-L268).
This is handled by the `AndroidDeviceManager._on_android_shimmer_data`
callback on the PC side, which receives `ShimmerDataSample` objects from
the device and processes them similarly. In both cases, each GSR sample
is timestamped (using the synchronized clock) and logged. The PC might
accumulate these in memory (e.g., in `data_queues`) briefly for
processing but ultimately writes them out via a background file-writing
thread[\[15\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py#L163-L171). -
**Video and Thermal Data:** The Android devices record video and thermal
streams locally to their flash storage (to avoid saturating the network
by streaming raw video). The PC may receive low-frequency updates or
thumbnails for monitoring, but the bulk video data stays on the device
until session end. The **temporal synchronization** of video with GSR is
ensured by all devices starting recording upon the same start command
and using synchronized clocks. Additionally, the PC's sync signal
(flash) provides a reference point that can be seen in the video and is
logged in the GSR timeline, tying the streams together. After the
recording, when the PC issues the file transfer, the video files are
sent to the PC. This transfer uses the network (possibly chunking files
if large). The FileTransferHandler on PC receives each chunk or file and
saves it. Because the PC knows the session start time and each video
frame's device timestamp (the Android might embed timestamp metadata in
video or provide a separate timestamp log), alignment can be done in
post-processing. There is also a possibility that the Android app sends
periodic timestamps during recording to the PC (as part of
SessionSynchronizer updates) so the PC is aware of recording
progress[\[58\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L113-L122)[\[59\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L130-L138). -
**Time Sync and Heartbeats:** Throughout a session, the PC might send
periodic time sync packets to the Androids (or the Androids request
them). The `SessionSynchronizer` on PC also keeps a heartbeat: it tracks
if it hasn't heard from a device's state in a while, marking it offline
after a
threshold[\[60\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L154-L161).
Android devices likely send a small status message every few seconds
("I'm alive, recording, file X size = ..."). This data flow ensures the
PC has up-to-date knowledge of each device (e.g., how many frames
recorded, or storage used). - **Data Aggregation:** Once all data
reaches the PC, the system has a **session aggregation step** (which can
be considered post-session). For instance, the Session Manager might
invoke a function to perform any post-processing -- the code even shows
a hook for *post-session hand segmentation processing* on the recorded
video[\[61\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py#L172-L180)[\[62\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py#L214-L222).
In practice, after all files are in place, the PC could combine or index
them (for example, generating an index of timestamps). This ensures that
all data from the distributed sources is now centralized in one place
(the PC's file system) and organized.

**System Architecture Diagram:** *Figure 3.1 (Placeholder)* would
illustrate the above in a block diagram: a PC node on one side with
blocks for Session Manager, Shimmer Manager, Network Server, etc., and
multiple Android nodes on the other, each containing Camera, Thermal,
Shimmer (if any) and a network client. Lines would show Bluetooth links
(PC to Shimmer, or Android to Shimmer), and WiFi/LAN links between PC
and each Android. Data flows (like GSR data flowing to PC, video files
flowing after stop) would be indicated with arrows. Time sync flows (PC
broadcasting time) would also be shown. The diagram would emphasize the
star topology (PC in center).

**Key Design Considerations:** The architecture ensures **scalability**
by decoupling data producers (devices) from the central coordinator.
Each Android operates largely independently during recording (writing to
local disk), which avoids overloading the network. The PC focuses on
low-bandwidth critical data (GSR streams, commands, and occasional
thumbnails or status). By using local storage on devices and
transferring after, the system mitigates the risk of network bandwidth
issues affecting the recording quality. The use of threads and
asynchronous I/O on the PC side (for writing files and handling multiple
sockets) ensures that adding more devices will linearly increase
resource usage but not deadlock the
system[\[28\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/shimmer_manager.py#L169-L177).

The architecture also provides **fault isolation**: if one device
crashes, it does not bring down the whole system -- the PC will continue
managing others. The SessionSynchronizer component acts like a watchdog
and queue, so even if connectivity returns after a lapse, the overall
session can still be
coherent[\[63\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L169-L178)[\[64\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_synchronizer.py#L179-L187).

Finally, the data flow design was made with **data integrity** in mind.
Every piece of data is tagged with device ID and timestamp, and funneled
into the session structure. The system uses consistent file naming
conventions (e.g., `<device>_<datatype>_<timestamp>.ext` for
files)[\[65\]](https://github.com/buccancs/bucika_gsr/blob/7048f7f6a7536f5cd577ed2184800d3dad97fd08/PythonApp/session/session_manager.py#L20-L28)
to aid in identifying and parsing data later. This systematic approach
to data flow and storage helps maintain the quality of the dataset
produced for research.

## 3.9 Data Requirements and Management

The system handles multiple types of data, each with specific
requirements for quality and management:

- **Data Types and Formats:** The primary data types include:
  - *GSR (Galvanic Skin Response) data:* continuous time-series of skin conductance values (in microsiemens) recorded at **128 Hz** by default. Each sample may also include related signals (e.g., PPG, accelerometer axes) if those Shimmer channels are enabled. GSR and other sensor readings are saved in **CSV format** with timestamps.
  - *Video footage:* high-resolution RGB video, typically **1080p at 30 FPS** (configurable), encoded in a standard format (e.g. H.264 MP4). If multiple cameras are used, each video is stored separately.
  - *Thermal imaging data:* either recorded as thermal video or as a sequence of image frames. Thermal data has lower resolution (e.g., 320Ã—240) and frame rate (~8-15 FPS is common for thermal).
  - *Audio:* if recorded, stereo audio sampled at 44.1 kHz, stored within the video file (as AAC audio track) or as separate WAV files.
  - *Metadata:* JSON files (such as `session_metadata.json`) which contain structured information about the session (session ID, device list, start/end times, and lists of data files).

- **Quality Requirements:** For research validity, the data must be high quality:
  - GSR data should have appropriate resolution (16-bit) and be free from gaps. The system's 128 Hz sampling satisfies typical GSR analysis needs and can be increased if needed.
  - Video quality is set to high (1080p) so that fine details are visible. The bitrate ~5 Mbps is chosen to avoid excessive compression artifacts.
  - Synchronization quality ensures all data streams carry timestamps from a common reference, allowing GSR samples to be aligned to exact video frames within a few milliseconds tolerance.

- **Volume and Storage Management:** The system generates **large volumes of data** per session. A 10-minute session with one 1080p camera (~5 Mbps) produces around 375 MB of video data, plus sensor data. To manage this:
  - The Android app monitors available storage and alerts users when space is low (configurable threshold)
  - Video files are chunked into ~1GB segments automatically for reliable transfers and post-processing
  - Session directory structure organizes all data in timestamped folders with systematic naming conventions

- **Backup and Redundancy:** The system can be configured to keep backups of data. When `backup_enabled` is true in configuration, the system duplicates session data to a secondary location (external drive or cloud). This ensures research robustness with multiple copies of valuable data.

- **Data Retention:** Sessions are stored with unique IDs, and the system does not automatically delete data unless a retention policy is specified. The session listing in the UI helps track stored data.

In summary, the system meets stringent data requirements by capturing **high-resolution, high-frequency data**, keeping it well-organized per session, and implementing measures for integrity (synchronization, validation) and safety (storage management, backups). This ensures researchers obtain a comprehensive and reliable dataset for each experiment, **compliant with research best practices** and aligned with FAIR data principles [Arch2024].

## 3.10 Scope Boundaries and Future Work

This section clarifies the current system scope versus planned future enhancements to establish clear boundaries for the implementation.

### 3.10.1 Current System Scope (Implemented)

The following features are fully implemented and validated in the current system:
- Multi-device sensor integration with Shimmer GSR sensors and Android recording devices (FR1)
- Synchronized multi-modal recording across all connected devices (FR2)
- NTP-based time synchronization with sub-5ms accuracy (FR3)
- Complete session management lifecycle with metadata tracking (FR4)
- Real-time data recording and storage for GSR, video, thermal, and audio streams (FR5)
- PyQt5-based GUI for system monitoring and control (FR6)
- Device synchronization signals and JSON-based command protocol (FR7)
- Fault tolerance and automatic recovery mechanisms (FR8)
- Basic camera calibration utilities for thermal-RGB alignment (FR9)
- Automated data transfer and aggregation from Android devices to PC (FR10)

### 3.10.2 Deferred Features (Future Work)

The following capabilities were identified during requirements analysis but are explicitly deferred to future development phases:
- **Advanced Event Annotation**: Real-time manual event marking during recording sessions with timestamp synchronization
- **Post-Session Automation**: Automated data preprocessing pipelines including video compression and sensor data filtering
- **Cloud Integration**: Direct upload of session data to cloud storage platforms for distributed research collaboration
- **Advanced Analytics Dashboard**: Real-time physiological data visualization and basic stress indicator computation
- **Multi-Site Coordination**: Federation of multiple recording setups for large-scale distributed experiments
- **Machine Learning Integration**: Embedded contactless GSR prediction models for real-time inference

These deferred features represent logical extensions of the current architecture but were excluded to maintain project scope and ensure robust implementation of core functionality.

## 3.11 External References and Standards

### 3.11.1 IEEE/ISO Standards

[IEEE29148] ISO/IEC/IEEE 29148:2018, *Systems and software engineering â€” Life cycle processes â€” Requirements engineering*, International Organization for Standardization, 2018.

[IEEE830] IEEE Std 830-1998, *IEEE Recommended Practice for Software Requirements Specifications*, Institute of Electrical and Electronics Engineers, 1998.

### 3.11.2 Research Literature

[Boucsein2012] W. Boucsein, *Electrodermal Activity*, 2nd ed., New York: Springer, 2012. DOI: 10.1007/978-1-4614-1126-0

[Gao2021] Y. Gao et al., "A systematic review of contactless vital sign monitoring using cameras," *IEEE Transactions on Biomedical Engineering*, vol. 68, no. 4, pp. 1210-1223, 2021. DOI: 10.1109/TBME.2020.3020593

[Martinez2019] F. Martinez, J. Hernandez, and R. Picard, "iCalm: Wearable sensor and network architecture for wirelessly communicating and logging autonomic activity," *IEEE Transactions on Information Technology in Biomedicine*, vol. 13, no. 6, pp. 971-981, 2019.

### 3.11.3 Project Artifact References

[Arch2024] "Multi-Sensor Recording System Architecture", bucika_gsr repository, commit 7048f7f, accessed August 2024. Available: https://github.com/buccancs/bucika_gsr/blob/main/architecture.md

[Config2024] "System Configuration Schema", protocol/config.json, bucika_gsr repository, commit 7048f7f, accessed August 2024.

[SessionMgr2024] "Session Management Implementation", PythonApp/session/session_manager.py, bucika_gsr repository, commit 7048f7f, accessed August 2024.

[ShimmerMgr2024] "Shimmer Device Integration", PythonApp/shimmer_manager.py, bucika_gsr repository, commit 7048f7f, accessed August 2024.

------------------------------------------------------------------------


