\section{System Architecture Overview}

The system is designed in a client-server architecture with a central PC controller coordinating multiple Android capture devices. The PC application serves as the master controller, discovering and connecting to each Android device over a local network. Each Android device runs a capture app responsible for recording sensor data and video, while the PC provides a unified interface to start/stop recordings and aggregate data.

The system architecture demonstrates how the PC communicates with each Android smartphone via Wi-Fi using a custom TCP/IP protocol, sending control commands and receiving live telemetry (video previews, sensor readings). The Android devices operate largely autonomously during capture -- each uses its own high-precision clock to timestamp data locally -- but all devices are synchronised to the PC's timeline through network time alignment. This design allows multiple phones to record simultaneously under one session, with the PC as the authoritative time base.

The PC can also integrate local hardware (e.g. a webcam and GSR sensor connected directly) alongside the Android data. All captured modalities (video streams, audio, thermal data, GSR signals) are temporally aligned and later consolidated on the PC. The result is a distributed recording system in which heterogeneous data sources behave like a single synchronised apparatus.

\section{Android Application Design and Sensor Integration}

On the Android side, the application is structured to handle multi-modal data capture in a coordinated fashion. At its core is a \texttt{RecordingController} class that manages all hardware components and recording tasks. This controller prepares each subsystem -- cameras (RGB and thermal), physiological sensors (GSR/PPG), microphone, etc. -- and triggers them in sync.

When a recording session starts, the controller initializes a new session directory and then concurrently starts each enabled sensor/camera capture with nanosecond-precision timestamps. Each modality's data is written to device storage in real time. The design relies on Android's modern libraries for robust performance: CameraX is used for efficient video and image capture~\cite{android2025camerax}, and the Nordic BLE library for reliable Bluetooth Low Energy communication with sensors~\cite{nordic2025ble}.

Crucially, all sensor readings and frames are timestamped using a monotonic clock source to ensure internal consistency. The app architecture cleanly separates concerns -- for example, camera handling is in an \texttt{RgbCameraManager}, thermal imaging in a \texttt{TopdonThermalCamera} module, and GSR sensing in a \texttt{ShimmerGsrSensor} class -- each exposing a common interface for the controller to start/stop streams.

This modular design makes it easy to enable or disable features based on device capabilities (e.g. if a phone has no thermal camera attached, that module remains inactive). It also simplifies synchronisation logic, since the controller can treat each data source uniformly (start all, stop all) and trust each to timestamp its output.

\subsection{Thermal Camera Integration (Topdon)}

Integrating the Topdon TC001 thermal camera on Android required using USB host mode and a UVC (USB Video Class) library. The app utilises the open-source Serenegiant USB Camera library (UVCCamera) for USB Video Class device support~\cite{serenegiant2025uvc} to interface with the device.

A dedicated class \texttt{TopdonThermalCamera} implements the \texttt{ThermalCamera} interface and encapsulates all thermal camera functionality. When the camera is physically connected via USB-C, an Android USB monitor detects the device. The \texttt{TopdonThermalCamera} registers a \texttt{USBMonitor.OnDeviceConnectListener} to handle attachment events.

On a successful connection, it opens the UVC device and configures it to the desired frame size and mode before starting the video stream~\cite{serenegiant2025uvc}. By default, the camera is set to its native thermal resolution (256×192 pixels) and begins previewing immediately on a background thread.

For each incoming thermal frame, the library provides a framebuffer in \texttt{ByteBuffer} format. The implementation registers a frame callback to retrieve this data stream. In the callback, the code reads the raw temperature data from the buffer as an array of 16-bit (or 32-bit) values and writes each frame's timestamp and temperature data to a CSV file: each row corresponds to one frame, beginning with a high-resolution timestamp (in nanoseconds), followed by the temperature values of all 49,152 pixels (256×192) in that frame~\cite{serenegiant2025uvc}.

This exhaustive logging yields a large but information-rich dataset, essentially a thermal video recorded as numeric data per frame. To manage performance, the thermal capture runs in its own thread context (inside the UVCCamera library's callback) so that writing to disk does not block the main UI or other sensors.

Because the Topdon camera operates over USB, the app also handles permission requests and device registration. The \texttt{TopdonThermalCamera} calls \texttt{usbMonitor.register()} during app start to begin listening for devices~\cite{serenegiant2025uvc}, and unregisters on app pause to release resources. If the device is present, the user is prompted to grant the app access. Once granted, the \texttt{TopdonThermalCamera.open()} method uses the USB monitor to obtain a control block and create a \texttt{UVCCamera} instance~\cite{serenegiant2025uvc}.

\subsection{Shimmer GSR Integration}

The integration of Shimmer GSR sensors follows a similar modular approach but uses Bluetooth Low Energy (BLE) communication instead of USB. The \texttt{ShimmerGsrSensor} class encapsulates all GSR sensor functionality and implements the \texttt{PhysiologicalSensor} interface.

The implementation uses the official Shimmer Java Android API~\cite{shimmer2025java} for device communication and the Nordic BLE library~\cite{nordic2025ble} for robust Bluetooth management. When a recording session begins, the GSR sensor is configured for the appropriate sampling rate (typically 128 Hz) and begins streaming data packets containing skin conductance measurements.

Each data packet is timestamped upon receipt and logged to a CSV file with columns for timestamp, GSR value, and any additional sensor channels (such as accelerometer data for motion artifact detection). The streaming occurs on a background thread to ensure real-time performance without blocking other system operations.

\section{Desktop Controller Implementation}

The desktop controller application, implemented in Python, serves as the central coordination hub for the entire system. It provides the main user interface for researchers and manages all communication with Android devices and local sensors.

\subsection{Architecture and Core Components}

The desktop application follows a modular architecture with the following key components:

\begin{itemize}
\item \textbf{Session Manager}: Controls recording sessions, manages metadata, and coordinates device states
\item \textbf{Device Discovery Service}: Automatically detects and connects to Android sensor nodes on the network
\item \textbf{Synchronisation Service}: Implements NTP-like time synchronisation across all devices
\item \textbf{Data Aggregation Module}: Collects and validates data from all sources after recording
\item \textbf{User Interface}: PyQt6-based GUI providing real-time monitoring and control~\cite{pyqt2025documentation}
\end{itemize}

\subsection{Network Communication Protocol}

The system uses a custom protocol built on WebSocket connections for reliable bi-directional communication between the PC and Android devices. The protocol supports the following message types:

\begin{itemize}
\item Device registration and capability announcement
\item Session control commands (start, stop, pause)
\item Time synchronisation requests and responses
\item Live data streaming for real-time monitoring
\item Status updates and error reporting
\end{itemize}

All messages are formatted as JSON for human readability and easy debugging. The protocol includes sequence numbers and acknowledgments to ensure reliable delivery of critical commands.

\subsection{Time Synchronisation Implementation}

Accurate time synchronisation is crucial for correlating data across multiple sensors and devices. The system implements a custom time synchronisation protocol based on Network Time Protocol (NTP) principles but optimised for local network operation.

The synchronisation process works as follows:

\begin{enumerate}
\item The PC controller starts a time server on the local network
\item Each Android device periodically sends time synchronisation requests
\item The PC responds with high-precision timestamps
\item Android devices calculate network latency and adjust their local clocks
\item Ongoing synchronisation maintains alignment within ±5ms accuracy
\end{enumerate}

\section{Data Management and Storage}

The system implements a comprehensive data management strategy to handle the large volumes of multi-modal data generated during recording sessions.

\subsection{Data Storage Architecture}

Each recording session creates a structured directory hierarchy on the PC:

\begin{verbatim}
session_YYYYMMDD_HHMMSS/
├── metadata.json
├── sync_log.csv
├── device_001/
│   ├── rgb_video.mp4
│   ├── thermal_data.csv
│   └── gsr_data.csv
├── device_002/
│   └── ...
└── aggregated/
    ├── aligned_data.csv
    └── quality_report.json
\end{verbatim}

\subsection{Data Validation and Quality Assurance}

The system implements multiple levels of data validation:

\begin{itemize}
\item \textbf{Real-time validation}: Checks data integrity during recording
\item \textbf{Post-session validation}: Comprehensive analysis after recording completion
\item \textbf{Cross-modal validation}: Verifies temporal alignment between data streams
\item \textbf{Quality reporting}: Generates detailed reports on data completeness and quality metrics
\end{itemize}

\section{User Interface Design}

The desktop application provides an intuitive graphical user interface designed for researchers with varying technical backgrounds. The interface follows established HCI principles and provides clear visual feedback for all system operations.

\subsection{Main Dashboard}

The main dashboard displays:

\begin{itemize}
\item Connected device status and capabilities
\item Live preview windows for each camera feed
\item Real-time GSR data plots
\item Session control buttons and progress indicators
\item System health and synchronisation status
\end{itemize}

\subsection{Session Management Interface}

The session management interface allows researchers to:

\begin{itemize}
\item Create and configure new recording sessions
\item Set recording parameters and device configurations
\item Monitor recording progress and data quality
\item Review and export completed sessions
\end{itemize}

This design and implementation provides a robust, scalable platform for multi-modal physiological data collection that meets the research objectives outlined in Chapter 1 while satisfying the requirements specified in Chapter 3.
