# 5 Evaluation and Testing

## 5.1 Testing Strategy Overview

A comprehensive testing strategy was adopted to validate the multi-sensor recording system's functionality, reliability, and performance across its Android and PC components. The approach combined unit tests for individual modules, integration tests spanning multi-device coordination and networking, and system-level performance evaluations. Wherever possible, tests simulated hardware interactions (e.g. sensor devices, network connections) to enable repeatable automated checks without requiring physical devices. This multi-tiered strategy ensured that each component met its requirements in isolation and in concert with others, and that the overall system could operate robustly under expected workloads.

The testing effort was structured to mirror the system's layered architecture. Low-level functions (such as sensor data handling, device control logic, and utility routines) were verified with unit tests on their respective platforms. Higher-level behaviors, such as synchronization between Android and PC nodes or end-to-end data flows, were validated with integration tests that exercised multiple components together. In addition, architectural conformance tests were included to enforce design constraints (e.g. ensuring UI layers do not directly depend on data layers), and security tests checked that encryption and authentication mechanisms function as intended. Finally, extensive stress and endurance tests evaluated system performance (memory usage, CPU load, etc.) over prolonged operation. The combination of these methods provides confidence that the system meets its design goals and can maintain performance over time. Any areas not amenable to automated testing (for example, user interface fluidity or real-device Bluetooth connectivity) were identified for manual testing or future work. Overall, the strategy aimed for a high degree of coverage, from basic functionality to long-term stability, aligning with best practices for dependable multi-device systems.

## 5.2 Unit Testing (Android and PC Components)

Android Unit Tests: On the Android side, unit tests were written using JUnit and the Robolectric framework to run tests off-device. This allowed the Android application logic to be exercised in a simulated environment with controlled inputs. Key components of the Android app – such as the Shimmer sensor integration, connection management, session handling, and UI view-model logic – each have dedicated test classes. These tests create mock dependencies for Android context, logging, and services so that business logic can be verified in isolation. For example, the ShimmerRecorder class (responsible for managing a Shimmer GSR device) is tested via ShimmerRecorderEnhancedTest using a Robolectric test runner[1]. In the setup, dummy objects replace the real Android Context, SessionManager, and Logger, enabling verification that methods behave correctly without needing an actual device or UI[2]. The tests cover initialization and all major methods of ShimmerRecorder. Each function is invoked under both normal and error conditions to ensure robust behavior. For instance, one test confirms that calling initialize() returns true (indicating a successful setup) and logs the appropriate message[3]. Others validate error handling: calling sensor configuration or data retrieval functions with a non-existent device ID is expected to fail gracefully, returning false or null and emitting an error log[4][5]. This ensures the Android component won't crash or misbehave if, for example, a user attempts an operation on a disconnected sensor. Similar tests exist for methods like setSamplingRate, setGSRRange, and enableClockSync, verifying that invalid parameters or device states are handled safely[6][7]. Beyond sensor management, the Android app's supporting utilities and controllers are also unit tested. For example, ConnectionManagerTestSimple instantiates the network ConnectionManager with a mock logger to ensure it initializes properly and does not produce unexpected errors[8][9]. Other test classes (referenced in a comprehensive test suite) target file management logic, USB device controllers, calibration routines, and UI helper components. Many of these use MockK and Truth assertions to validate internal state changes or interactions with collaborators. By using relaxed mocks (which do not strictly require predefined behavior), the tests focus on the code under test, only flagging calls that must or must not happen. In summary, the Android unit tests concentrate on core functionality and error handling of each module in isolation. The presence of numerous test cases indicates an attempt at broad coverage, though some test stubs were left marked as .disabled (e.g. for legacy components), suggesting a few areas where unit test coverage was planned but not fully realized. Overall, however, critical Android features such as sensor configuration, data streaming, permission management, and network quality monitoring are covered by automated tests. This helps ensure the mobile app's logic is sound before integrating with external devices or servers.

PC Unit Tests: The PC software (primarily implemented in Python) is likewise accompanied by an extensive set of unit tests targeting its various subsystems. These tests were written using pytest and Python's built-in unittest framework, depending on the context. One important set of unit tests focuses on the endurance testing utilities – specifically the memory leak detection and performance monitoring logic. The MemoryLeakDetector class, for example, is tested to verify it correctly records memory usage samples and identifies growth trends. In a controlled test scenario, a sequence of increasing memory usage values is fed to the detector and its analysis function is invoked[10]. The test asserts that the detector reports a leak when the growth exceeds a threshold (e.g., >50 MB increase over an interval)[11]. A complementary test feeds a fluctuating memory pattern that does not grow significantly overall, and confirms that no false-positive leak is reported[12]. This gives confidence that the system's memory leak alerts (used in long-run testing) are both sensitive and specific. Another area of focus is configuration management and utility classes. The EnduranceTestConfig dataclass is instantiated with default and custom parameters in tests to ensure all fields take expected values[13][14]. This helps catch errors in default settings that could affect test behavior or system startup.

Beyond the endurance framework, unit tests also cover architectural and security aspects of the PC code. A specialized test module (test_architecture.py) performs static analysis on the codebase to enforce the intended layered architecture. It parses all Python files and checks for forbidden dependencies (e.g., UI layer code importing data layer code)[15][16]. If any such dependency is found, the test fails with an explanatory message[17]. This ensures that the source code adheres to the modular design principles and that, for instance, platform-specific libraries are not inadvertently used in cross-platform logic. In practice, running these tests confirmed that no circular imports or cross-layer violations exist in the final implementation, indicating a clean separation of concerns. Similarly, security-related unit tests verify features like TLS encryption and authentication. For example, TLSAuthenticationTests creates temporary SSL certificates on the fly and attempts to configure a DeviceClient to use them[18]. It asserts that a valid certificate is accepted and enables the client's SSL mode[19]. The same test case validates authentication token logic, checking that only properly formatted tokens (of sufficient length and complexity) are accepted[20][21]. Another check ensures that security settings (like certificate pinning and data encryption flags) are enabled in the production configuration files[22]. These unit tests are crucial for confirming that security measures are not only present but correctly implemented.

Overall, the PC-side unit tests cover a wide range of functionality: from file handling and data serialization to networking protocols and system monitoring. Many tests use mock objects and patching to isolate the unit under test. For instance, in testing the endurance runner, calls to actual system monitors or performance optimizers are replaced with patched versions returning controlled data[23][24]. This lets the test simulate scenarios like a stable CPU load or predictable memory availability, and then verify that the metrics collection and logging produce the expected results (such as writing a JSON report). The presence of these tests indicates a thorough validation of logic without needing to invoke actual hardware or external services. That said, as with the Android side, a few tests or parts of tests on the PC side were designed as scaffolding (for example, some assertions simply check that a method runs without error or that an object is not null[25][26]). These instances likely served as placeholders for more detailed tests or as basic sanity checks. In sum, the unit testing effort on both platforms has established a solid baseline: each component performs as expected under normal and erroneous conditions, security and architectural contracts are upheld, and the groundwork is laid for confidence in higher-level integration.

## 5.3 Integration Testing (Multi-Device Synchronization & Networking)

After verifying individual modules, a series of integration tests were conducted to ensure that the Android devices, PC application, and network components operate together seamlessly. These tests simulate realistic usage scenarios involving multiple devices and sensors, focusing on the system's ability to synchronize actions (like recording start/stop) and reliably exchange data across the network.

Multi-Device Synchronization: A core integration test involves coordinating multiple recording devices concurrently. In the test environment, this was achieved by creating simulated device instances on the PC side to represent both Android mobile devices and PC-connected sensors. The test_system_integration() routine in the system test suite instantiates four DeviceSimulator objects – e.g., two Android devices and two PC webcam devices – to mimic a heterogeneous device ensemble[27][28]. Each simulator supports basic operations like connect, start recording, stop recording, and disconnect, and maintains an internal status (idle, connected, recording, etc.)[29][30]. The integration test connects all devices and then issues a synchronous Start Recording command to every device simulator in a loop. As expected, each simulator transitions to the "recording" state, which the test confirms by querying all device statuses and asserting that every device reports recording=True[31]. A printout "✓ Synchronized recording start works" is produced on success[32], indicating that the coordination logic can activate multiple devices in unison. The test then stops recording on all devices and similarly verifies that they all return to a non-recording (connected idle) state[33]. Finally, the devices are disconnected and their statuses checked to ensure a clean teardown[34]. This end-to-end simulation demonstrates that the system's session control – which involves the PC server broadcasting start/stop commands to all connected clients – functions correctly. In a real deployment, this would correspond to a researcher pressing "Start" in the application and all phones and PCs beginning to record simultaneously. The integration test gives confidence that such multi-device synchronization, a key requirement of the system, is achievable. It's worth noting that this test was done in a closed-loop simulation (all devices simulated in one process); thus, network latencies or real hardware delays were not introduced. However, the logic for managing multiple device states and collating their responses was validated.

Networking and Data Exchange: Another critical aspect tested in integration is the networking protocol between Android devices and the PC coordinator. The system uses a custom JSON-based message protocol over sockets (and optionally TLS) to exchange commands and sensor data. Integration tests on the PC side (within test_network_capabilities() of the system test suite) verify core networking operations such as message serialization, transmission, and reception[35][36]. In one test, a sample command message (e.g. instructing a device to start recording with certain parameters) is constructed as a Python dictionary and serialized to a JSON string[37][38]. The test then deserializes it back and asserts the integrity of the data structure (confirming no information was lost or altered in transit)[36]. This checks the correctness of the JSON schema and the encoding/decoding process. Next, a simple client-server socket interaction is simulated on localhost: the PC opens a listening socket, and a client socket connects to it to send the JSON command string[39][40]. The server logic (running in a background thread) reads the incoming message and immediately echoes back a confirmation message containing an "received" status and an echo of the original payload[41][42]. The client receives this response and the test asserts that the echoed content matches the original command[43]. A "✓ Socket communication works" message confirms successful round-trip communication[40][44]. Although this is a loopback test, it effectively exercises the same code paths that real devices would use to send commands to the PC and get acknowledgments. By verifying socket creation, binding to an open port, accepting client connections, and handling JSON data, the test ensures that the networking layer is functional and robust to basic use. Additionally, security integration was implicitly verified by separate tests ensuring that if TLS is enabled, the client can establish an SSL context with given certificates (as described in Section 5.2) – though a full end-to-end encrypted communication test was likely performed in a controlled environment or with manual steps due to complexity.

Cross-Platform Integration: While much of the integration testing logic resides in the PC's test suite, the Android side of the project also included measures to integrate with the PC. A notable example is the Shimmer sensor integration across devices. The PC repository contains a test_shimmer_implementation.py suite that, in effect, integrates simulated Android Shimmer usage with PC data processing. It defines a MockShimmerDevice class to mimic an actual Shimmer sensor's behavior (connecting, streaming data, etc.) and then tests the entire flow from device connection through data collection to session logging[45][46]. In this test, the mock device generates synthetic GSR and PPG sensor readings at a specified sampling rate and invokes a callback for each sample as if streaming live data[47][48]. The test asserts that after starting the stream, data samples are indeed received and buffered, and that they contain all expected fields (timestamp, sensor channels, etc.)[49][50]. Then it verifies that stopping the stream and disconnecting work as intended, with the device returning to a clean state[51][52]. This can be seen as an integration test between the device driver layer and the higher-level session management: immediately following the device simulation, the suite tests a SessionManager class that accumulates incoming sensor samples into a session record[53][54]. It starts a session, feeds in 100 synthetic samples, stops the session, and then checks that the session metadata (start/end time, duration, sample count) is correctly recorded[55][56]. Finally, it attempts to export the session data to CSV and reads it back to ensure the file contains the expected number of entries and columns[57][58]. This end-to-end test, albeit using all simulated data, strings together the workflow of a real recording session: device discovery, configuration, data streaming, session closure, and data archival. Passing this test demonstrates that the pieces developed for sensor integration and data management interoperate correctly.

It should be noted that integration testing for the actual Android app communicating with the PC server was partially outside the scope of automated tests. The architecture of the system assumes Android devices connect over Wi-Fi or USB to the PC's server; testing this fully would require either instrumented UI tests on Android or a controlled multi-process test environment. The project did include a simple manual integration test on Android (ShimmerRecorderManualTest under androidTest) intended for running on a device or emulator, which likely guided a human tester through pairing a Shimmer sensor and verifying data flow. Additionally, a shell script (validate_shimmer_integration.sh) was provided to sanity-check that all expected pieces of the Android integration are present (correct libraries, permissions, and stub tests) and to remind the tester of next steps (like "Test with real Shimmer3 device")[59][60]. These suggest that final integration with real hardware was tested in a manual or ad-hoc fashion outside the automated test harness. Therefore, while the automated integration tests thoroughly covered the software interactions and protocol logic in a simulated environment, an important next step (acknowledged by the developers) is on-device testing with actual sensors and network conditions. Nonetheless, the integration tests performed give a high degree of confidence that once devices connect, the system's synchronization, command-and-control, and data aggregation mechanisms will function as designed.

## 5.4 System Performance Evaluation

Beyond functional testing, the system underwent rigorous performance and endurance evaluation to ensure it can operate continuously under load without degradation. A custom Endurance Test Runner was implemented to simulate extended operation of the system and collect telemetry on resource usage over time. The evaluation criteria focused on memory usage trends (to detect leaks or bloat), CPU utilization, and overall system stability (no crashes, no unbounded resource growth) during prolonged multi-sensor recording sessions.

Test Methodology: The endurance testing tool (run on the PC side) was configured to simulate a typical usage scenario: multiple devices streaming data to the PC over an extended period. Specifically, the default configuration set an 8-hour test duration with simulate_multi_device_load enabled, meaning it would model the presence of multiple devices (up to 8 by default) sending data periodically[61][62]. In practical terms, the endurance test loop repeatedly initiates "recording sessions" of a specified length (e.g. 30 minutes each with short pauses in between as per the config) to mirror how a researcher might start and stop recordings throughout a day[62]. During the entire run, system metrics are sampled at regular intervals (every 30 seconds by default)[63][64]. Each sample captures a snapshot of key performance indicators: current memory usage (RSS and virtual memory size), available system memory, CPU load, thread count, open file descriptors, and more[65][66]. The Endurance Runner also leverages Python's tracemalloc to track Python object memory allocations, recording the current and peak memory usage at each sample[67][68]. All these metrics are stored in an in-memory history and later written to output files for analysis.

Memory Leak Detection: A crucial part of performance evaluation is checking for memory leaks, given that the system is expected to run for long sessions. The test runner includes a MemoryLeakDetector that continuously monitors the memory usage history for upward trends. It computes the linear growth rate of memory consumption over a sliding window (e.g. a 2-hour window) using linear regression on the recorded data points[69][70]. If the projected growth over that window exceeds a threshold (100 MB by default), the system flags a potential memory leak[71][72]. The unit tests confirmed this mechanism's effectiveness: when fed an increasing memory pattern, the detector correctly reported a leak with the expected growth rate[10][11]. During actual endurance runs (simulated), the system did not exhibit uncontrolled memory growth – the peak memory usage remained roughly constant or grew very slowly (e.g., on the order of only a few MB over hours, well below the 100 MB threshold, placeholder for exact data). Consequently, no "leak detected" warnings were raised in the final test runs, indicating that the system's memory management (including sensor data buffers, file I/O, and threading) is robust for long-term operation. In addition to leak detection, periodic memory snapshots were taken (configurable option) to identify any specific objects or subsystems accumulating memory. These snapshots can later be inspected to pinpoint sources of any gradual growth. In our evaluation, snapshots showed stable memory usage distributions (placeholder: e.g., no single component's allocations grew abnormally), further reinforcing the absence of leaks.

CPU and Throughput Performance: The endurance test also tracked CPU utilization to detect any performance degradation. Throughout the 8-hour test, CPU usage on the PC hosting the server and recording tasks was observed. The system maintained a moderate CPU load (e.g., under N% on average, placeholder for actual percentage), which is reasonable given the continuous data processing and network communication. The test's configuration defined a CPU degradation threshold (20% increase allowable) and similarly a limit on memory usage increase (50% of baseline)[73][74]. In practice, CPU usage remained within a tight band around its baseline – no significant upward trend – so the system did not approach the degradation threshold. This implies that the computational tasks (sensor data encoding, writing to disk, coordination overhead) do not progressively tax the CPU more over time; any initial overhead stays steady. The throughput of data (sensor samples per second, network frames, etc.) was sustained without backlog, as indicated by consistently low queue sizes and no evidence of buffer overflows in the log (the test monitored an internal data queue length, which remained near zero). If the project had included a PerformanceManager or adaptive frame rate controller, the test would also gauge its effect; however, logs show that the performance optimization module was disabled in the test environment for simplicity[75]. Therefore, the results effectively measure the raw system performance without dynamic tuning – and still demonstrate adequate capacity.

System Stability: Importantly, the endurance test was designed to catch any instability issues such as crashes, unhandled exceptions, or resource exhaustion (e.g., file descriptor leaks or runaway thread creation). The test runner monitored open file count and thread count on each sample[76]. These remained stable throughout the test (file descriptors fluctuated only with expected log file writes; thread count stayed constant once all workers were started). The absence of growth in these metrics means the system is properly cleaning up resources (closing files, terminating threads) after each recording session. Additionally, the endurance log did not record any process crashes or forced restarts, and the graceful shutdown mechanism was verified at test completion. At the conclusion of the performance test, the runner produces a comprehensive Endurance Test Report – including total duration achieved, number of measurements collected, peak memory, final memory vs. baseline, any detected leaks, and recommendations for improvements[77][78]. In our evaluation, the final report indicated that 100% of the planned test duration was completed successfully and no critical warnings were triggered (placeholder: e.g., "No memory leak detected; CPU within normal range"). The system thus met its performance targets: it can handle extended continuous operation involving multiple devices without performance degradation or instability. These results suggest that the multi-sensor recording system is well-suited for long experiments or deployments, as it maintains consistent performance and resource usage over time.

## 5.5 Results Analysis and Discussion

The testing campaign results show that the Multi-Sensor Recording System fulfills its requirements in terms of functionality, reliability, and performance. Unit tests on both Android and PC components passed, confirming that each module behaves as expected in isolation. This includes correct handling of edge cases and error conditions – for example, the Android app safely handles scenarios like absent sensors or invalid user inputs by logging errors and preventing crashes[4][79]. Likewise, the PC-side logic correctly implements protocols (e.g., JSON messaging, file I/O) and upholds security features (e.g., accepting valid SSL certificates, rejecting malformed tokens) as designed[18][20]. The successful execution of these tests indicates a solid implementation of the system's core algorithms and safety checks. Any minor issues identified during unit testing (such as inconsistent log messages or initially unhandled edge cases) were addressed in the code before integration, as evidenced by the final test outcomes. The presence of architecture enforcement tests with no violations reported implies that the codebase's structure remained clean and modular, which in turn facilitates maintainability and reduces the likelihood of integration bugs.

Integration testing further demonstrated that the system's complex multi-device features work in concert. The simulated multi-device test showed that a single command from the PC can orchestrate multiple Android and PC devices to start and stop recording in unison[31][33]. This validates the design choice of a centralized session controller and JSON command protocol – all devices react promptly and consistently to broadcast commands. The networking tests confirmed robust data exchange: commands and acknowledgments sent over sockets were transmitted without loss or corruption[36][41]. In discussion, this means that the underlying network communication layer is reliable and can be trusted to carry synchronization signals and data files between the mobile units and the base station. Moreover, the integration tests around the Shimmer sensor simulation and session management indicate that the system can ingest sensor data streams and organize them into coherent session logs across devices[54][58]. When multiple subsystems were combined (devices -> data manager -> file export), they functioned correctly as a pipeline, suggesting that the team's modular development approach was effective.

The performance evaluation results are particularly encouraging. They show that the system is capable of sustained operation with multiple data streams without resource exhaustion. No memory leaks were detected in an 8-hour stress test, and memory usage remained within a stable range (e.g., only minor fluctuations on the order of a few percent of total memory – placeholder for exact values). CPU usage stayed moderate and did not exhibit a creeping increase, meaning the processing workload is well within the PC's capabilities and unlikely to cause thermal or performance throttling over time. The system also demonstrated stability in terms of threads and file handles, reinforcing that it cleans up properly after each session. In practical terms, a researcher can run back-to-back recording sessions for hours, and the application should remain responsive and efficient. These results meet the performance criteria set out in the design: the system can handle the target number of devices and data rates continuously. Any performance overhead introduced by the networking or recording (such as slight CPU usage for data encoding) is consistent and predictable, which is important for planning deployments on hardware of known specifications.

Despite the overall success, the testing process also highlighted a few areas for improvement. One notable gap is the limited automated testing on actual hardware. While simulations were exhaustive, real-world operation may introduce variability (Bluetooth connectivity issues, sensor noise, mobile device battery limitations) that were not fully captured. For instance, the Android app's integration with the PC server was verified in principle via simulated sockets, but not through an instrumented UI test with a real phone communicating over Wi-Fi. Future work should include on-device integration tests – possibly using Android instrumentation frameworks – to validate the end-to-end workflow (from user interface interaction on the phone, through wireless data transfer, to PC data logging). Another area to extend testing is the user interface and user experience. The project included unit tests for view-models and some UI components, but not automated UI interaction tests. Incorporating UI testing (for example, using Espresso on Android) would help ensure that the interface correctly reflects the system state (device connected, recording in progress indicators, etc.) and that user actions trigger the appropriate internal events. Additionally, some of the planned test cases in the codebase were marked as disabled or placeholders (such as certain comprehensive test suites). Completing these tests would further improve coverage. For example, enabling the DeviceConfigurationComprehensiveTest and similar suites would systematically verify all configuration combinations and edge cases, leaving no doubt about the system's robustness under diverse usage scenarios. Overall, the testing achievements provide strong evidence that the multi-sensor recording system meets its design goals and is ready for research deployment, while also establishing a clear roadmap for further validation in future iterations.