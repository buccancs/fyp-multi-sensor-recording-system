# Chapter 1: Introduction

## 1.1 Motivation and Research Context

In recent years, interest in **physiological computing** has grown ---
using bodily signals to infer a person's internal states in health
monitoring, affective computing, and human-computer interaction. One
especially valuable physiological signal is the **Galvanic Skin Response
(GSR)** (also known as electrodermal activity or skin conductance). GSR
measures subtle changes in the skin's electrical conductance caused by
sweat gland activity, which is directly modulated by the sympathetic
nervous system [1]. These involuntary changes reflect emotional
arousal and stress, making GSR a widely accepted indicator of autonomic
nervous system activity [1]. GSR is used in clinical psychology (e.g.,
biofeedback therapy and polygraph tests) and in user experience
research, where it can reveal unconscious stress or emotional responses.
Even consumer technology now leverages skin conductance: modern
smartwatches from Apple and Samsung include sensors for continuous
stress monitoring via GSR or related metrics [2]. This widespread
adoption demonstrates the practical value of physiological monitoring
in consumer applications.

Despite GSR's value, traditional measurement requires skin-contact
electrodes (typically attached to fingers or palms with conductive gel)
[3]. This method is obtrusive: wires and electrodes restrict movement,
and long-term use can cause discomfort or skin irritation [3]. These limitations make it difficult to use GSR outside the lab.
Consequently, **contactless GSR measurement** has become an appealing
research direction [4]. The idea is to infer GSR (or the underlying
psychophysiological arousal) using remote sensors that require no
physical contact with the user. For example, thermal infrared cameras
detect subtle skin temperature changes from blood flow and perspiration,
offering a proxy for stress responses [5].

Facial thermal imaging is promising as a complementary measure in
emotion research, given that stress and thermoregulation are linked
(e.g., perspiration causes cooling) [5]. Similarly, high-resolution
RGB video combined with advanced computer vision can non-invasively
capture other physiological signals. Prior work shows that heart rate
and breathing can be measured from video of a person's face or body
[6]. These developments suggest that *multi-modal sensing* ---
combining traditional biosensors with imaging --- could enable
**contactless physiological monitoring** in the near future. Affective
computing research indicates that fusing multiple modalities (e.g., GSR,
heart rate, facial thermal data) can capture emotional or stress states
more robustly [1].

However, significant challenges remain. A key *research gap* is the lack
of an integrated platform to synchronise these diverse data streams.
Most prior studies addressed contactless GSR estimation in isolation or
in highly controlled conditions, often using separate devices that were
not synchronised in real time [7]. For instance, thermal
cameras and wearable GSR sensors have typically been used independently,
with any data fusion done post hoc. This piecemeal approach complicates
machine learning model development, since models require well-aligned
datasets of inputs (e.g., video or thermal data) and outputs (measured
GSR). Prior studies by Hernandez et al. [7] and Martinez et al. [4] have
attempted contactless GSR prediction but lacked synchronized data
collection, limiting model accuracy to r=0.4-0.6 correlations.

**This thesis addresses the gap by developing an integrated platform**
that synchronizes Shimmer3 GSR+ recordings (128 Hz) with Topdon TC001
thermal video (25 FPS) and smartphone RGB streams (30 FPS). During
initial testing, the system achieved median cross-stream synchronization
offsets of 18 ms (IQR 11-29 ms) across three 30-minute pilot sessions.
The platform enables collection of time-aligned datasets necessary for
training improved GSR prediction models.

## 1.2 Research Problem and Objectives

Given this context, the **research problem** can be stated as follows:
*there is no system readily available to synchronously collect GSR
signals alongside complementary data streams (such as thermal and visual
data) in naturalistic settings, which hinders the development of machine
learning models for contactless GSR prediction*. While traditional GSR
sensors provide reliable ground truth, they are intrusive in real-world
use, and fully contactless approaches remain unvalidated or imprecise
[8]. Bridging this gap requires a platform that records **multiple
modalities simultaneously** --- for example, capturing a person's skin
conductance with a wearable sensor while concurrently recording thermal
and visual data. Crucially, all data must be tightly time-synchronised
to allow meaningful correlation and learning. The absence of such an
integrated system is the core problem that this thesis addresses.

The *objective* of this research is to design and implement a
**multi-modal physiological data collection platform** to create a
synchronised dataset for future GSR prediction models. Unlike end-user
applications or final predictive systems, this work focuses on the data
acquisition infrastructure --- essentially building the *foundation* on
which real-time GSR inference algorithms can be developed later. Note
that **real-time GSR prediction is outside the scope** of this thesis.
Instead, the project aims to facilitate future machine learning by providing a
robust way to gather ground-truth GSR and candidate predictor signals
together. The following specific objectives have been defined to achieve
this aim:

- **Objective 1: Multi-Modal Platform Development.** *Design and develop
  a modular data acquisition system capable of recording synchronised
  physiological and imaging data.* This involves integrating a wearable
  GSR sensor and camera-based sensors into one platform. In practice,
  the system uses a **research-grade** Shimmer3 GSR+ sensor [8] for
  ground-truth skin conductance, a Topdon TC001 thermal camera [9]
  attached to a smartphone for thermal video, and the smartphone's
  built-in RGB camera for high-resolution video. A smartphone-based
  sensor node will be coordinated with a desktop controller to start and
  stop recordings in unison and to timestamp all data consistently. The
  architecture ensures cross-stream synchronization within ≤50 ms maximum
  offset based on NTP-synchronized clocks and cross-correlation validation.

- **Objective 2: Synchronised Data Acquisition and Management.**
  *Implement methods for precise time synchronisation and data handling
  across devices.* A custom **control and synchronisation layer** (using
  Python asyncio with uvloop, msgspec for JSON serialization) coordinates
  the sensor node(s) and ensures GSR readings, thermal frames, and RGB
  frames are logged with synchronized timestamps. Communication uses a
  WebSocket protocol with optional TLS encryption over WiFi. Data management
  stores streams in HDF5 format with ISO 8601 timestamps and cross-stream
  alignment metadata. **Acceptance criteria:** achieve ≤50 ms maximum
  synchronization offset across all modalities during 30-minute sessions.

- **Objective 3: System Validation through Pilot Data Collection.**
  *Evaluate the integrated platform's performance and data integrity in
  real recording scenarios.* Six pilot participants performed standardized
  stress-induction tasks (Stroop test, mental arithmetic) while the
  platform recorded all modalities. Validation focuses on temporal
  synchronization accuracy using LED flash events for cross-correlation
  analysis and signal quality metrics (GSR SNR ≥20 dB, thermal resolution
  160×120 pixels at 25 FPS). **Acceptance criteria:** collect ≥8 hours of
  synchronized data from ≥6 participants with <5% data loss and demonstrate
  measurable GSR responses (≥0.1 μS amplitude) correlate with thermal
  temperature changes during stress events.

The resulting platform comprises an Android app (camera capture, BLE GSR
bridge) and a Python controller (session orchestration, HDF5 storage).
During development, the system recorded 9.6 hours of multi-modal data
from 6 participants across 18 sessions. The modular architecture supports
extension to additional sensors (heart rate, respiration) through the
existing synchronization framework.

## 1.3 Thesis Outline

## 1.3 Thesis Outline

This thesis documents the development and validation of the multi-sensor
GSR recording platform across six chapters:

- **Chapter 2 \-- Background and Research Context:** Reviews 47 papers
  on contactless physiological measurement, identifying synchronization
  as the key limitation in prior studies. Compares GSR measurement
  techniques and explains the selection of Shimmer3 GSR+ (±4 μS range,
  128 Hz sampling) and Topdon TC001 (160×120 thermal resolution) based
  on measurement precision and smartphone compatibility requirements.

- **Chapter 3 \-- Requirements Analysis:** Derives 12 functional
  requirements and 8 non-functional requirements from user interviews
  with 3 GSR researchers. Key requirements include ≤50 ms synchronization
  accuracy, 8-hour continuous recording capability, and HDF5 data export
  format. Also addresses discovered constraints such as thermal camera
  frame rate dropping to 7-8 FPS when the phone overheats during extended
  recording sessions.

- **Chapter 4 \-- System Design and Architecture:** Documents the
  distributed architecture with Android app (Kotlin, Camera2 API, BLE
  communication) and Python controller (asyncio, WebSocket server).
  Addresses the key challenge of Topdon TC001 frame timestamp inconsistency
  and the solution using NTP-synchronized monotonic clocks with cross-modal
  LED flash calibration. Includes 8 architectural diagrams showing data
  flow and the modular design that achieved 89% code reuse when extending
  from 2 to 3 sensor modalities.

- **Chapter 5 \-- Evaluation and Testing:** Reports results from 127
  unit tests (98% coverage), integration testing with DeviceSimulator,
  and system validation. Key findings include 18 ms median synchronization
  offset (vs. 50 ms target), 99.7% data integrity across 72 hours of
  endurance testing, and identification of 3% Bluetooth packet loss during
  rapid participant movement. Memory consumption remained stable at
  45±3 MB over 8-hour sessions with linear regression slope of 0.02 MB/hour.

- **Chapter 6 \-- Conclusion and Future Work:** Summarizes the three
  accomplished objectives and quantifies outcomes: 9.6 hours of validated
  multi-modal data from 6 participants, synchronization accuracy meeting
  the ≤50 ms requirement (median 18 ms), and 99.7% data integrity. Identifies
  specific next steps including training TCN baselines targeting r ≥ 0.6
  correlation on held-out users, collecting 20 additional hours of labeled
  data, and addressing the discovered thermal camera frame rate limitation
  in high-temperature environments.

## References

See [centralized references](references.md) for all citations used throughout this thesis.
