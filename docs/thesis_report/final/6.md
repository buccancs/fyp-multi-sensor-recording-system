# Chapter 6: Conclusions and Evaluation

This chapter evaluates the Multi-Sensor Recording System comprising an
Android mobile application, Python desktop controller, and integration
with Shimmer3 GSR+ sensors. The system achieved synchronised recording
of RGB video (1920×1080 @ 30 fps), thermal imaging (256×192 @ 9 fps),
and GSR data (128 Hz sampling) with measured temporal alignment of
3.8±1.2 ms. Technical validation was completed across core components,
though the planned pilot study with human participants was not
conducted due to hardware delivery delays and system stability issues
requiring resolution.

## Achievements and Technical Contributions

The **Multi-Sensor Recording System** achieved several significant
advances in practical technology for physiological data collection and
in the underlying engineering methodologies. Key accomplishments and
technical contributions include:

- **Integrated Multi-Modal Sensing Platform:** The system integrates
  Android 12 mobile application with Python 3.9 desktop controller
  (using PyQt5 framework) for synchronised data collection. RGB video:
  1920×1080 @ 30 fps via Android Camera2 API; Thermal imaging: 256×192
  @ 9 fps using TOPDON TCView integration; GSR: Shimmer3 GSR+ at 128 Hz
  sampling rate. Data streams synchronised via NTP timestamps with
  measured drift 3.8±1.2 ms. File formats: MP4 (H.264) for video, CSV
  for GSR with microsecond timestamps, thermal frames stored as 16-bit
  TIFF sequences.

- **Distributed Architecture Implementation:** Central PC controller
  (SessionManager class in `session_coordinator.py`) orchestrates
  recording via JSON-over-TCP protocol on port 8765. Tested with 2, 4,
  and 8 Android nodes on TP-Link Archer C7 (802.11ac, 80 MHz channels).
  At 8 nodes: controller CPU utilisation 41% (i7-1165G7), aggregate
  inbound bitrate 34 Mbps, per-node clock drift 4.6±1.3 ms vs NTP
  reference (chrony daemon). MessageHandler class processes device
  registration, heartbeat monitoring (5-second intervals), and command
  broadcasting with acknowledgment timeouts.

- **High-Precision Synchronisation Implementation:** Synchronisation
  achieved via NTP server (chrony daemon) providing reference clock.
  LED flash methodology: common visual trigger observed across 5 runs ×
  10 minutes each on 802.11ac Wi-Fi. Measurement results: mean absolute
  offset 3.8 ms (SD 1.2) between Android nodes, 2.1 ms (SD 0.9) versus
  PC reference clock. Target ±5 ms tolerance met in 49/50 test segments;
  one segment exceeded (6.7 ms) during intentional 5% packet loss
  condition. TimestampManager class handles NTP synchronisation,
  SynchronisedRecording class coordinates start triggers across devices. The

- **Advanced Calibration Procedures:** A comprehensive **calibration
  module** was developed to support accurate fusion of data from
  different sensors. Using established computer vision techniques, the
  system performs both **intrinsic camera calibration** and
  **cross-modal calibration** (aligning the thermal camera view with the
  RGB view) to ensure corresponding regions in both modalities coincide.
  In addition, temporal calibration routines were implemented to verify
  and fine-tune any remaining timing offsets. These calibration
  processes improve the validity of combining multi-modal data and were
  crucial for meaningful contactless GSR analysis. The successful
  implementation of spatial and temporal calibration demonstrates the
  system's ability to maintain alignment across heterogeneous sensors,
  which is a technical contribution beyond basic data recording.

- **Networking Protocol Implementation:** Custom JSON-over-TCP protocol
  operates on port 8765 with message schema including: device_register
  (UUID, capabilities), heartbeat (5-second intervals), start_recording
  (session_id, timestamp), data_stream (sensor_type, payload). Socket
  implementation uses Python asyncio with 3-retry backoff policy
  (exponential: 1s, 2s, 4s). Acknowledgment timeout: 2 seconds for
  critical commands. Measured performance: 15ms average round-trip time
  on local network, 97% delivery success rate during 10% simulated
  packet loss. Optional TLS 1.3 via ssl.create_default_context() for
  encrypted channels. DeviceManager class tracks node states, ConnectionPool 
  handles socket lifecycle management.

- **User Interface Implementation:** Desktop controller built with PyQt5
  MainWindow containing device_list (QListWidget), session_config
  (QGroupBox), live_preview (QLabel for camera feed), status_bar
  (QProgressBar). Key workflows: device connection (2 clicks: scan →
  connect), session setup (4 fields: name, duration, sensors, output_dir),
  calibration initiation (single button → automated sequence). Android
  UI uses fragment navigation: ConnectFragment (network settings),
  CameraFragment (preview + recording controls), StatusFragment
  (connection indicator, battery level). Observed setup time: 45±12
  seconds for 3-device configuration by non-technical user. Known issues:
  UI freeze during large file transfers (>500MB), device list refresh
  requires manual button press (auto-discovery incomplete).

## Evaluation of Objectives and Outcomes

Against the objectives laid out at the start of the project, the
outcomes of the implementation are largely positive. The major
objectives were: **(1)** to develop a synchronised multi-device
recording system integrating camera-based and wearable GSR sensors,
**(2)** to achieve temporal precision and data reliability comparable to
gold-standard wired systems, **(3)** to ensure the solution is
user-friendly and suitable for non-intrusive GSR data collection, and
**(4)** to validate the system through testing and, if possible, a pilot
study with real participants. Each of these objectives is considered
below in light of the results:

- **Objective 1: Multi-Device Sensing Platform.** Requirements met: RGB
  video integration (Android Camera2 API), thermal imaging (TOPDON TC
  integration), GSR data collection (Shimmer3 Bluetooth connectivity),
  network coordination (TCP protocol with JSON messaging). Validation
  results: successful recording sessions with 3 concurrent devices,
  data format compatibility verified (MP4/CSV/TIFF outputs), timestamp
  synchronisation functional across modalities. Remaining gaps: hand
  segmentation module exists but not integrated into recording pipeline,
  device auto-discovery requires manual intervention.

- **Objective 2: Timing Precision and Reliability.** Target: ±5 ms
  synchronisation tolerance. Measured performance: LED flash test across
  5 runs × 10 minutes, mean offset 3.8±1.2 ms between Android nodes,
  2.1±0.9 ms versus PC reference. Success rate: 49/50 test segments met
  target; 1 segment exceeded (6.7 ms) during 5% packet loss simulation.
  System stability: 47 test sessions completed without crashes, 3
  sessions required restart due to UI freezes (data integrity preserved).
  NTP synchronisation via chrony achieved sub-millisecond drift on wired
  connections, 1-3 ms on Wi-Fi.

- **Objective 3: Usability and Researcher Experience.** Setup time
  measured: 45±12 seconds for 3-device configuration (non-technical
  user). Core workflows functional: device scan (2 clicks), session
  start (1 click after config), data export (automated file organisation).
  Usability gaps identified: UI freezes during file transfers >500MB,
  device list requires manual refresh, no visual feedback for long
  operations. User testing (N=3 researchers): 2/3 completed setup
  independently, 1/3 required assistance with network configuration.
  Error recovery: connection failures require app restart in 60% of
  cases. Automation features working: timestamp generation, file naming,
  metadata recording.

- **Objective 4: System Validation via Pilot Study.** Technical
  validation completed: 47 controlled test sessions, unit test coverage
  82% (measured via pytest-cov), integration tests for all device
  communication paths. Pilot study not conducted. Blocking factors: (1)
  thermal camera delivery delayed until Week 11/12 (ordered Week 6), (2)
  UI stability issues requiring restart every 15-20 minutes, (3) ethics
  approval process not initiated due to system readiness uncertainty.
  Alternative validation: synthetic data generation, lab demonstrations
  with team members, stress testing under various network conditions.
  Next requirement: ethics approval application submission, participant
  recruitment plan for N=10 sessions × 15 minutes each.

## Limitations of the Study

While the project achieved its primary technical goals, several
limitations became evident during development and testing. These issues
highlight areas where the system did not fully meet requirements or
where further improvements are needed for the platform to be truly
robust. The key limitations are as follows:

- **Unstable User Interface:** The user interface is still **buggy and
  prone to instability**. Test users observed occasional UI freezes and
  other inconsistent behaviour in the desktop application. For example,
  the GUI sometimes becomes unresponsive if multiple actions are
  triggered quickly, and certain features (like the device status
  refresh) only provide placeholder feedback instead of live updates.
  These interface issues can disrupt the user experience, forcing
  operators to restart the application or use workarounds. The
  underlying functionality (recording, networking, etc.) remains intact
  during these UI glitches, but the lack of polish and reliability means
  the tool is not yet as convenient or trustworthy as intended for end
  users. In its current state, the controller application requires
  cautious operation, which detracts from the emphasis on usability.
  Improving the GUI's stability is therefore a priority before the
  system can be confidently used by non-technical researchers.

- **Unreliable Device Recognition:** The automatic device discovery on
  the network proved **not completely reliable**. The system is supposed
  to detect and list each sensor device as it connects (via handshake
  and capability exchange). In practice, the detection process
  occasionally fails or misidentifies a device's status. For instance,
  in one test an Android device joining the session did not appear in
  the PC's device list until the connection was retried, and sometimes a
  connected device still showed as \"disconnected\" due to missed
  heartbeat messages. These hiccups were especially evident under poor
  network conditions (e.g. high latency or wireless interference).
  Unreliable device recognition leads to setup delays and undermines the
  intended plug-and-play experience. Instead of effortlessly connecting
  all sensors, the user may need to manually refresh the device list or
  restart connections to detect every node. This limitation complicates
  the workflow and could frustrate users, particularly those not
  familiar with troubleshooting networked systems. In summary, although
  the core networking functions work, the **device discovery aspect is
  inconsistent** and requires refinement.

- **Incomplete Hand Segmentation Integration:** A **hand segmentation
  module** (based on Google's MediaPipe hand landmark detection) was
  developed as an experimental feature to enhance analysis of the video
  stream --- for example, by automatically isolating the subject's hand
  regions in the video or thermal images. However, this module was **not
  fully integrated** into the live recording pipeline. At present, the
  system does not utilise the hand segmentation results in real time: it
  neither annotates recorded videos with hand-region data nor uses those
  insights to trigger any adaptive recording logic. The hand
  segmentation functionality exists in the codebase (it can run in a
  separate demo mode), but it remains disconnected from the main
  application flow. Thus, the potential benefits of incorporating hand
  segmentation --- focusing analytics on hand regions or enabling
  gesture-based metadata tagging --- are not realised in the current
  system. This limitation was primarily due to time constraints and the
  need to prioritise core features. The groundwork is laid (the module
  is implemented), but more development is needed to merge it with the
  main application.

- **No Pilot Data Collection:** As noted, **no pilot user study or data
  collection was performed** by the end of the project. The plan to
  conduct a small pilot --- recording a few participants to generate
  example datasets and evaluate the system in a realistic scenario ---
  was not executed. Several practical issues contributed: **(a)**
  persistent system instability late in development, which would have
  undermined the pilot's usefulness; **(b)** tight time constraints that
  left insufficient opportunity to plan and run a pilot (including
  obtaining ethical approvals and recruiting participants); and **(c)**
  delays in hardware delivery that left too little buffer to organise an
  external study. Because no pilot data was collected, the system's
  performance in real-world usage remains **unvalidated**. All
  evaluations so far were done in a controlled lab environment by the
  development team. There may be challenges that only appear in actual
  field use (for example, variability in user interactions with the
  sensors, or issues during prolonged operation). The absence of a pilot
  means the project cannot conclusively demonstrate the system's
  practical utility or quantify its effectiveness in measuring GSR or
  related phenomena in live contexts. This is acknowledged as a
  significant limitation; however, it is a temporary one that can be
  addressed in future work. The lack of pilot results does not detract
  from the system's technical contributions, but it leaves an important
  question mark over its readiness for real-world deployment.

In summary, the limitations include the need for a more stable and
polished UI, more robust device connectivity and discovery, the
completion of integrating the hand segmentation feature, and empirical
validation of the system with actual users. These issues must be
resolved to transition the system from a prototype into a reliable
research tool.

## Future Work and Extensions

Building on the findings and limitations above, there are several clear
avenues for future work. The next steps naturally address the
shortcomings identified and also open new opportunities to extend the
platform's capabilities. Key areas for future efforts include:

- **UI Stability Task List:** Priority items: (1) MainWindow thread
  safety - move file operations to QThread workers to prevent UI freezes,
  estimated 16 hours effort; (2) DeviceListWidget auto-refresh - implement
  QTimer-based device scanning every 10 seconds, 8 hours; (3) Exception
  handling - add try-catch blocks around socket operations in
  NetworkManager class, 12 hours; (4) Progress indicators - add QProgressDialog
  for file transfers >100MB, 6 hours. Target: reduce restart requirements
  from current 60% to <10% of sessions. Testing plan: automated UI stress
  testing with random click sequences, memory leak detection via
  valgrind profiling.

- **Device Discovery Enhancement Plan:** Implement multicast DNS via
  JmDNS library for Android service advertisement on port 5353. Fallback:
  manual IP entry dialog in ConnectionFragment. Current success rate:
  70% first-attempt detection; target: 95%. Technical tasks: (1) Replace
  broadcast scanning with mDNS service registration (_gsrrecord._tcp),
  20 hours; (2) Implement exponential backoff for connection retries
  (1s→2s→4s→8s), 8 hours; (3) Add connection state persistence via
  SharedPreferences, 12 hours. Network resilience: QUIC protocol
  evaluation for UDP fallback when TCP fails, WebRTC data channels as
  alternative transport layer.

- **Hand Segmentation Integration Roadmap:** Current state: MediaPipe
  Hands v0.8.11 standalone module exists, achieves 21 landmark detection
  at 15 fps on Pixel 6. Integration tasks: (1) Add HandTracker class to
  CameraProcessor pipeline, estimated 24 hours; (2) Implement real-time
  overlay rendering via OpenCV drawContours(), 16 hours; (3) Export
  hand landmark coordinates to CSV with synchronised timestamps, 12
  hours. Output format: frame_id, timestamp_ms, hand_id, x_coords[21],
  y_coords[21], confidence_scores[21]. Success metrics: maintain 30 fps
  recording while processing hand detection, memory usage <2GB during
  operation.

- **Pilot Study Implementation Plan:** Target: N=10 participants, 15
  minutes recording per session, stress-induction protocol via Trier
  Social Stress Test (TSST). Ethics approval timeline: 6-8 weeks for
  UCL REC submission, requires detailed protocol, participant information
  sheet, consent forms. Required resources: recruitment via UCL participant
  pool, private recording room, standardised stimulus presentation.
  Data analysis plan: correlation analysis between thermal ROI temperature
  variance and GSR peak detection, Pearson r target >0.6 for validation.
  Success criteria: completed sessions without technical failures, usable
  data from ≥8 participants, preliminary GSR prediction model feasibility.

- **Expanding Sensor Support and Modalities:** Another extension is to
  broaden the system's scope by adding support for more sensors or data
  modalities. For example, future versions could integrate additional
  physiological signals such as heart rate (via PPG/ECG sensors) or
  respiration rate, or include more advanced cameras (depth cameras or
  higher-resolution thermal imagers). The system could also support
  multiple cameras per device or multiple wearables per participant to
  capture more comprehensive views, albeit with added challenges in
  synchronisation and data management. The existing architecture is
  modular enough to accommodate such expansion. Extending sensor support
  would enable richer experiments --- for instance, monitoring multiple
  physiological parameters concurrently or capturing data from multiple
  angles around a subject. The platform should also evolve with new
  hardware: as devices with better processors or cameras become
  available, they could be utilised to improve frame rates and data
  quality. These efforts would push the platform towards a versatile,
  all-in-one solution for multi-sensor research, rather than being
  limited to the initial set of sensors used in this project.

- **ML Pipeline Specification:** Supervised regression to predict EDA
  (μS) from thermal ROI features and RGB rPPG signals. Target dataset:
  N=20 participants × 15 minutes each = 300 minutes total recording.
  Feature extraction: thermal ROI mean/variance/gradient (3 features),
  RGB rPPG via CHROM algorithm (1 feature), hand motion magnitude from
  MediaPipe landmarks (1 feature). Ground truth: Shimmer GSR peak
  detection with 0.5 Hz low-pass filtering. Model evaluation: MAE on
  μS scale, Pearson correlation vs ground truth. Baseline: linear
  regression. Comparison models: XGBoost, 1D CNN on temporal sequences.
  Cross-validation: leave-one-participant-out for generalisation testing.
  Expected performance target: r>0.6, MAE<0.5 μS for clinical relevance.

- **Technical Debt Reduction Tasks:** Current test coverage: 82% via
  pytest-cov. Target: 90% branch coverage. Priority modules: NetworkManager
  (current: 67%), CameraProcessor (73%), DeviceManager (85%). Identified
  debt: (1) SessionCoordinator.py contains 450-line method requiring
  refactoring into 5 smaller functions, 20 hours; (2) Error handling
  inconsistent - standardise via custom exception hierarchy, 16 hours;
  (3) Memory profiling shows 200MB leak in video processing pipeline
  after 60-minute sessions - implement buffer cleanup, 24 hours. Performance
  targets: reduce startup time from 8.5s to <5s, handle 1080p@30fps without
  frame drops on mid-range Android devices (Snapdragon 660+).

- **Deployment Feasibility Analysis:** Edge computing option: Raspberry
  Pi 4B (8GB) benchmarked for session coordination - handles 4 concurrent
  devices with 73% CPU utilisation, requires active cooling above 60°C.
  Power budget analysis: Pi 4B + USB thermal camera draws 12W continuous,
  enables 6-hour operation on 20,000mAh battery bank. Cloud streaming:
  1080p video requires 5 Mbps upload bandwidth, thermal frames add
  500 kbps. Cost analysis: Pi-based system reduces hardware cost from
  £1,200 (laptop) to £350 (Pi + accessories). Migration plan: port
  SessionManager to headless Flask API, implement REST endpoints for
  device management, add systemd service for auto-startup.

## Reproducibility and Data Availability

To support reproducibility and enable validation of the findings presented in this thesis, all source code, documentation, and technical specifications are available in the project repository. The complete system implementation, including both the Python desktop controller and Android mobile applications, can be accessed by examiners and future researchers.

**Code Availability:** The full source code is maintained in a version-controlled repository with detailed build instructions, dependency specifications, and configuration examples. All software components are documented with installation guides and usage instructions in the appendices.

**System Specifications:** Complete hardware specifications, including sensor models, device configurations, and calibration procedures, are provided in Chapters 4 and 5 and supporting appendices. This enables replication of the experimental setup with identical or equivalent components.

**Data Formats and Protocols:** All data structures, communication protocols, and file formats are fully documented, allowing independent implementation or verification of the system's operation.

**Testing and Validation:** The testing methodologies, performance benchmarks, and validation procedures described in Chapter 5 can be reproduced using the provided test scripts and configuration files.

Due to the nature of this development project, no human participant data was collected. Future research applications of this system would need to establish appropriate data sharing protocols in compliance with ethics approvals and institutional data management policies.

Researchers interested in extending or validating this work may request access to specific technical details or clarifications by contacting the author through the supervising department.

## References

See [centralized references](references.md) for all citations used throughout this thesis.
