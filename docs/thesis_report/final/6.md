# Chapter 6: Conclusions and Evaluation

This chapter critically assesses the developed Multi-Sensor Recording
System. It highlights the system\'s achievements and technical
contributions, evaluates how well the outcomes meet the initial
objectives, discusses the limitations encountered, and outlines
potential future work. The project set out to create a **contactless
Galvanic Skin Response (GSR) recording platform** using multiple
synchronised sensors. The final implementation demonstrates substantial
progress towards this goal. All core system components were implemented
and tested, establishing a strong foundation for future research in
non-intrusive physiological monitoring.

## Achievements and Technical Contributions

The **Multi-Sensor Recording System** achieved several significant
advances in practical technology for physiological data collection and
in the underlying engineering methodologies. Key accomplishments and
technical contributions include:

- **Integrated Multi-Modal Sensing Platform:** The project delivered a
  functional platform consisting of an **Android mobile application**
  and a **Python-based desktop controller** operating in unison. This
  cross-platform system supports synchronised recording of
  **high-resolution RGB video**, **thermal infrared imagery**, and
  **physiological GSR signals** (from a Shimmer3 GSR+ sensor)
  simultaneously. The heterogeneous sensors run concurrently, enabling a
  rich multi-modal dataset for contactless GSR research. This successful
  integration of multiple modalities addresses the core project goal of
  enabling **contactless GSR measurement** by combining traditional
  contact sensors with camera-based sensing.

- **Distributed Hybrid Architecture:** A distributed architecture was
  designed and implemented to coordinate multiple sensor devices in
  parallel. In this design, a central PC controller orchestrates
  recording sessions while each mobile device performs local data
  capture and preliminary processing. This **hybrid star--mesh
  architecture** balances centralised control with on-device
  computation, providing both coordination and scalability. The system
  is capable of managing **up to eight sensor nodes simultaneously**,
  demonstrating that the multi-device approach can maintain
  synchronisation and reliability across devices. This design contrasts
  with traditional physiological monitoring systems that rely on a
  single device or purely central data loggers. Experiments show that a
  distributed approach can expand the scope of data collection (for
  example, recording multiple camera angles or multiple participants
  concurrently) without sacrificing performance.

- **High-Precision Synchronisation Mechanisms:** Achieving tight
  temporal alignment across all data streams was a critical challenge
  addressed by a custom **multi-modal synchronisation framework**. The
  implementation uses multiple techniques, such as a Network Time
  Protocol (NTP) server for global clock reference, timestamped command
  exchange, and periodic clock calibration across devices. As a result,
  video frames, thermal images, and GSR readings are all timestamped
  against a common clock with only a few milliseconds of drift.
  Empirical testing in controlled conditions indicates that the system
  can maintain this level of temporal precision, meeting the design
  requirement of ±5 ms tolerance. This precision is comparable to
  research-grade wired acquisition systems, demonstrating that a
  distributed wireless sensor network can be used for rigorous
  physiological measurements without significant loss of timing
  fidelity.

- **Advanced Calibration Procedures:** A comprehensive **calibration
  module** was developed to support accurate fusion of data from
  different sensors. Using established computer vision techniques, the
  system performs both **intrinsic camera calibration** and
  **cross-modal calibration** (aligning the thermal camera view with the
  RGB view) to ensure corresponding regions in both modalities coincide.
  In addition, temporal calibration routines were implemented to verify
  and fine-tune any remaining timing offsets. These calibration
  processes improve the validity of combining multi-modal data and were
  crucial for meaningful contactless GSR analysis. The successful
  implementation of spatial and temporal calibration demonstrates the
  system's ability to maintain alignment across heterogeneous sensors,
  which is a technical contribution beyond basic data recording.

- **Robust Networking and Device Management:** The project implemented a
  custom networking protocol for coordinating devices, built on
  lightweight JSON message exchanges over sockets. This protocol
  supports device handshaking and identification, remote command
  dissemination (e.g. broadcasting start/stop signals to all devices),
  periodic heartbeats and status updates, and real-time streaming of
  sensor data back to the controller. A central **Session Manager** on
  the PC and client logic on each mobile device handle session
  configuration and state management. The networking layer was designed
  for reliability and low latency: it includes connection retries,
  acknowledgments for critical messages, and buffering of data to
  prevent loss during brief network interruptions. Basic security
  measures (optional TLS encryption and device identity verification
  during handshakes) were also implemented to protect data in transit.
  The result is a robust distributed system where multiple mobile nodes
  can join and operate in a synchronised session with minimal manual
  intervention. This reliable communication and device management
  framework enables **scalable multi-device recording** and is a key
  technical contribution of the project.

- **User Interface and Usability Focus:** Considerable effort was
  devoted to developing a user interface and workflow that would allow
  researchers to operate the system with relative ease. The **desktop
  controller features a graphical UI** for managing devices (e.g.
  connecting or disconnecting sensors, monitoring status), configuring
  recording sessions (selecting data streams, setting session
  identifiers), initiating calibration procedures, and observing live
  previews or status indicators during a session. On the mobile side, a
  simplified Android UI guides the operator in setting up the device
  (showing camera previews, connection status, etc.) without requiring
  low-level commands. The system also automates aspects of session
  management, such as organising recorded files and metadata for each
  session, to streamline post-processing. While the interface is not yet
  fully polished, this emphasis on user experience means the platform is
  closer to a practical research tool. It shows that even a complex
  multi-sensor system can be made accessible to non-expert users through
  thoughtful interface design and automation of background tasks.

## Evaluation of Objectives and Outcomes

Against the objectives laid out at the start of the project, the
outcomes of the implementation are largely positive. The major
objectives were: **(1)** to develop a synchronised multi-device
recording system integrating camera-based and wearable GSR sensors,
**(2)** to achieve temporal precision and data reliability comparable to
gold-standard wired systems, **(3)** to ensure the solution is
user-friendly and suitable for non-intrusive GSR data collection, and
**(4)** to validate the system through testing and, if possible, a pilot
study with real participants. Each of these objectives is considered
below in light of the results:

- **Objective 1: Multi-Device Sensing Platform.** This objective was
  **fully achieved**. The final system successfully integrates multiple
  sensor modalities (video, thermal, and GSR) and multiple devices in a
  unified platform. A desktop application and Android devices
  communicate seamlessly to collect synchronised data. All essential
  components --- the mobile app, network infrastructure, and desktop
  control software --- were completed and shown to work together. The
  fully implemented platform demonstrates that the foundational goal of
  creating a multi-sensor recording system was realised. Researchers now
  have a prototype tool that can collect rich datasets combining
  physiological signals with visual and thermal observations, a strong
  step towards enabling contactless GSR measurement.

- **Objective 2: Timing Precision and Reliability.** This objective was
  **achieved**, with outcomes meeting (and in some cases exceeding) the
  expectations in laboratory tests. The system's clock synchronisation
  and scheduling mechanisms were validated in controlled experiments,
  demonstrating that all devices can record data with only a few
  milliseconds of discrepancy. This satisfies the initial requirement
  (targeting at most tens of milliseconds of drift) and provides
  confidence that the data streams are effectively simultaneous for
  analysis. Moreover, the system proved stable and reliable during these
  tests: no data loss or crashes occurred in typical runs under good
  network conditions. These results suggest that the networking and
  synchronisation design is sound. The combination of NTP-based time
  alignment, unified start triggers, and continuous device status
  monitoring yields performance comparable to wired systems in timing
  accuracy and recording reliability. Thus, the temporal precision
  benchmark was successfully met.

- **Objective 3: Usability and Researcher Experience.** This objective
  was **mostly achieved**. The project emphasised an accessible user
  experience, and the delivered software reflects this in features such
  as the GUI for session control and automated data management. Users do
  not need to manually synchronise clocks or start recordings on each
  device; the system provides central controls to handle these tasks
  behind the scenes. In practice, a single operator can configure
  devices and launch a recording session without dealing with low-level
  settings, demonstrating a degree of user-friendliness suitable for
  research environments. However, the ease-of-use goal is only partially
  fulfilled because some aspects of the experience need further
  refinement. During testing, it became apparent that the desktop
  application's interface could become unresponsive in certain
  scenarios, and automatic device discovery was not seamless (sometimes
  a device did not appear without a manual reconnection). These issues
  did not prevent basic operation but indicate that the learning curve
  and robustness for end users can be improved. Despite these
  shortcomings (further detailed as limitations below), the core design
  proves that the system is **practical for real-world use**. In a lab
  setting, a researcher was able to run the entire system and collect
  synchronised multi-modal data. The minor usability issues that remain
  are solvable, and with some fixes the platform is poised to become
  genuinely user-friendly.

- **Objective 4: System Validation via Pilot Study.** This objective was
  **only partially achieved**. The system underwent functional testing
  and internal evaluation, but a planned pilot study with human
  participants was **not conducted** by the project's end. On one hand,
  technical validation of individual components was successful: unit
  tests and simulated recordings showed that each part worked correctly
  both in isolation and together. On the other hand, the lack of an
  actual pilot means the system's performance in a real-world
  experimental context (with real users and longer sessions) was not
  demonstrated. The pilot was intended as a proof-of-concept deployment
  of the platform in its target scenario, providing example data and
  revealing any practical issues under realistic use. Unfortunately, due
  to a combination of factors --- including time constraints late in
  development, unexpected hardware delivery delays (the thermal camera
  arrived later than expected), and residual system instability
  requiring frequent restarts --- running a pilot within the project
  period proved infeasible. Deferring the pilot was a difficult decision
  since it left an important aspect of the project incomplete. However,
  this compromise was necessary to focus on solidifying the system's
  implementation. The absence of pilot data is acknowledged as a gap,
  but it does not diminish the system's demonstrated capabilities;
  rather, it points to an important next step beyond this work. In
  summary, all technical objectives were met, but the **practical
  demonstration with end users remains an outstanding task**. Completing
  that task in the future will be essential to fully validate the system
  under real operating conditions.

## Limitations of the Study

While the project achieved its primary technical goals, several
limitations became evident during development and testing. These issues
highlight areas where the system did not fully meet requirements or
where further improvements are needed for the platform to be truly
robust. The key limitations are as follows:

- **Unstable User Interface:** The user interface is still **buggy and
  prone to instability**. Test users observed occasional UI freezes and
  other inconsistent behaviour in the desktop application. For example,
  the GUI sometimes becomes unresponsive if multiple actions are
  triggered quickly, and certain features (like the device status
  refresh) only provide placeholder feedback instead of live updates.
  These interface issues can disrupt the user experience, forcing
  operators to restart the application or use workarounds. The
  underlying functionality (recording, networking, etc.) remains intact
  during these UI glitches, but the lack of polish and reliability means
  the tool is not yet as convenient or trustworthy as intended for end
  users. In its current state, the controller application requires
  cautious operation, which detracts from the emphasis on usability.
  Improving the GUI's stability is therefore a priority before the
  system can be confidently used by non-technical researchers.

- **Unreliable Device Recognition:** The automatic device discovery on
  the network proved **not completely reliable**. The system is supposed
  to detect and list each sensor device as it connects (via handshake
  and capability exchange). In practice, the detection process
  occasionally fails or misidentifies a device's status. For instance,
  in one test an Android device joining the session did not appear in
  the PC's device list until the connection was retried, and sometimes a
  connected device still showed as \"disconnected\" due to missed
  heartbeat messages. These hiccups were especially evident under poor
  network conditions (e.g. high latency or wireless interference).
  Unreliable device recognition leads to setup delays and undermines the
  intended plug-and-play experience. Instead of effortlessly connecting
  all sensors, the user may need to manually refresh the device list or
  restart connections to detect every node. This limitation complicates
  the workflow and could frustrate users, particularly those not
  familiar with troubleshooting networked systems. In summary, although
  the core networking functions work, the **device discovery aspect is
  inconsistent** and requires refinement.

- **Incomplete Hand Segmentation Integration:** A **hand segmentation
  module** (based on Google's MediaPipe hand landmark detection) was
  developed as an experimental feature to enhance analysis of the video
  stream --- for example, by automatically isolating the subject's hand
  regions in the video or thermal images. However, this module was **not
  fully integrated** into the live recording pipeline. At present, the
  system does not utilise the hand segmentation results in real time: it
  neither annotates recorded videos with hand-region data nor uses those
  insights to trigger any adaptive recording logic. The hand
  segmentation functionality exists in the codebase (it can run in a
  separate demo mode), but it remains disconnected from the main
  application flow. Thus, the potential benefits of incorporating hand
  segmentation --- focusing analytics on hand regions or enabling
  gesture-based metadata tagging --- are not realised in the current
  system. This limitation was primarily due to time constraints and the
  need to prioritise core features. The groundwork is laid (the module
  is implemented), but more development is needed to merge it with the
  main application.

- **No Pilot Data Collection:** As noted, **no pilot user study or data
  collection was performed** by the end of the project. The plan to
  conduct a small pilot --- recording a few participants to generate
  example datasets and evaluate the system in a realistic scenario ---
  was not executed. Several practical issues contributed: **(a)**
  persistent system instability late in development, which would have
  undermined the pilot's usefulness; **(b)** tight time constraints that
  left insufficient opportunity to plan and run a pilot (including
  obtaining ethical approvals and recruiting participants); and **(c)**
  delays in hardware delivery that left too little buffer to organise an
  external study. Because no pilot data was collected, the system's
  performance in real-world usage remains **unvalidated**. All
  evaluations so far were done in a controlled lab environment by the
  development team. There may be challenges that only appear in actual
  field use (for example, variability in user interactions with the
  sensors, or issues during prolonged operation). The absence of a pilot
  means the project cannot conclusively demonstrate the system's
  practical utility or quantify its effectiveness in measuring GSR or
  related phenomena in live contexts. This is acknowledged as a
  significant limitation; however, it is a temporary one that can be
  addressed in future work. The lack of pilot results does not detract
  from the system's technical contributions, but it leaves an important
  question mark over its readiness for real-world deployment.

In summary, the limitations include the need for a more stable and
polished UI, more robust device connectivity and discovery, the
completion of integrating the hand segmentation feature, and empirical
validation of the system with actual users. These issues must be
resolved to transition the system from a prototype into a reliable
research tool.

## Future Work and Extensions

Building on the findings and limitations above, there are several clear
avenues for future work. The next steps naturally address the
shortcomings identified and also open new opportunities to extend the
platform's capabilities. Key areas for future efforts include:

- **Stability Improvements and UI Refinement:** The top priority is to
  **harden the system's stability** and refine the user interface.
  Future work should involve thorough debugging of the desktop
  application's GUI event handling to eliminate crashes, freezes, and
  unresponsiveness. This may include more extensive UI testing (covering
  edge cases and rapid interactions) to catch issues early. Improving
  the Android app's interface stability (e.g. its fragment navigation
  and preview updates) is also important. Fixing the known UI bugs and
  streamlining the workflow will increase overall reliability. A
  polished, stable interface will enable researchers to trust the system
  during long recording sessions. These refinements would turn the
  prototype into a more production-ready tool for daily experimental
  use.

- **Reliable Device Discovery and Networking:** Another crucial
  improvement is to make device recognition and network communication
  **more reliable and seamless**. Efforts should focus on strengthening
  the automatic device discovery protocol so devices are consistently
  detected on the first attempt. This might involve using more robust
  discovery mechanisms (e.g. broadcast/multicast announcements with
  verification) or adding a manual pairing procedure as a fallback, as
  well as improving how the system handles network latency or packet
  loss. Additionally, optimising the networking code and error handling
  will help ensure that once devices are connected, they remain in sync
  without hiccups. For example, better heartbeat monitoring and
  reconnection logic would prevent devices from appearing \"lost\" after
  momentary network drops. The goal is truly plug-and-play operation,
  where researchers can power on the sensor nodes and have them appear
  ready in the controller UI with minimal intervention. By enhancing the
  networking layer's resilience, the system will be easier to use and
  more robust in diverse network environments.

- **Full Integration of Hand Segmentation (and Other Analytics):**
  Integrating the hand segmentation module into the live data pipeline
  is a clear next step. Future development should tie the
  MediaPipe-based hand landmark detection into recording sessions so the
  system can record raw video along with derived information about the
  subject's hand position or gestures in real time. For example, the
  system could overlay bounding boxes or masks on the video where hands
  are detected, or log hand motion data alongside other sensor streams.
  Once this module is integrated, the hand segmentation data could feed
  into real-time analytics --- for instance, detecting if a participant
  wipes their hands or moves out of frame, and then flagging those
  events in the data. Beyond hand segmentation, the platform could be
  extended with additional computer vision analytics (such as facial
  expression recognition or remote photoplethysmography if a camera is
  pointed at a face) to enrich the set of contactless measures. These
  extensions would transform the platform from a passive recording tool
  into an intelligent monitoring system that interprets certain aspects
  of human behaviour on the fly. Such enhancements build on the current
  work and would provide greater value for research, especially in
  studies of human affect or stress where gestures and facial cues are
  informative.

- **Conducting Pilot Studies and Empirical Validation:** A top priority
  for future work is to **use the system in an actual pilot study** or
  experiments with human participants. Conducting a well-designed pilot
  will serve multiple purposes: it will validate the system's end-to-end
  functionality in a realistic setting, provide an initial dataset to
  analyse the correlation between the contactless signals (thermal
  imaging, video) and traditional GSR readings, and reveal any practical
  issues not discovered in lab tests (such as usability hurdles during
  setup or sensor interference in real environments). One plan is to
  recruit a small number of participants and collect synchronised
  multi-modal data in scenarios relevant to the intended application
  (for example, exposing subjects to stimuli that elicit GSR changes
  while recording them). A pilot study would also enable quantitative
  evaluation of performance: contactless measurements from the cameras
  could be compared against the Shimmer GSR data to assess accuracy.
  Additionally, feedback from users during these sessions would guide
  further improvements. Ultimately, executing such pilot studies is
  essential to move the project from a prototype to a validated research
  instrument. This work will demonstrate the practical utility of the
  system and build confidence that the platform can be reliably used in
  real-world research on physiological monitoring.

- **Expanding Sensor Support and Modalities:** Another extension is to
  broaden the system's scope by adding support for more sensors or data
  modalities. For example, future versions could integrate additional
  physiological signals such as heart rate (via PPG/ECG sensors) or
  respiration rate, or include more advanced cameras (depth cameras or
  higher-resolution thermal imagers). The system could also support
  multiple cameras per device or multiple wearables per participant to
  capture more comprehensive views, albeit with added challenges in
  synchronisation and data management. The existing architecture is
  modular enough to accommodate such expansion. Extending sensor support
  would enable richer experiments --- for instance, monitoring multiple
  physiological parameters concurrently or capturing data from multiple
  angles around a subject. The platform should also evolve with new
  hardware: as devices with better processors or cameras become
  available, they could be utilised to improve frame rates and data
  quality. These efforts would push the platform towards a versatile,
  all-in-one solution for multi-sensor research, rather than being
  limited to the initial set of sensors used in this project.

- **Machine Learning for Contactless GSR Estimation:** With a large
  multi-modal dataset, a logical next step is to apply machine learning
  or statistical modelling to estimate physiological metrics like GSR
  from purely contactless signals. The ultimate vision is to measure GSR
  without electrodes. To work towards that, future research can use the
  synchronised thermal and RGB video (and other data streams) as inputs
  to train predictive models of stress or arousal, using the Shimmer GSR
  readings as ground truth. Initially, researchers can explore
  correlations between features (such as facial temperature changes,
  perspiration on the hands or face, or heart rate derived from video)
  and GSR peaks. Then algorithms ranging from regression models to
  modern deep learning techniques could attempt to predict GSR levels
  from the camera-based inputs. If successful, this would validate the
  core hypothesis that **contactless physiological sensing can
  approximate traditional sensor readings**. Such a result would be not
  just an engineering milestone but also a scientific contribution,
  demonstrating new methods for non-intrusive biosignal monitoring.
  Thus, integrating data analysis and machine learning is a meaningful
  direction that directly leverages the data collected by the system.

- **System Optimisation and Technical Debt Reduction:** As the project
  moves from prototype towards a more mature platform, future work
  should also address various engineering optimisations. Parts of the
  code that were implemented quickly as proofs-of-concept need to be
  refactored to meet production-quality standards, reducing technical
  debt. Writing comprehensive unit tests and performing integration
  testing on both the Python controller and the Android app will be
  important to catch regressions and ensure all components behave as
  expected under a range of conditions. Performance bottlenecks
  identified via profiling should be addressed (for instance, ensuring
  the system can handle high data throughput when recording full HD
  video alongside high sampling-rate sensors). Additionally, deployment
  considerations --- such as packaging the software for easy
  installation, improving user documentation, and perhaps containerising
  components for easier setup --- would make the system more accessible
  to other researchers. By systematically improving code quality, test
  coverage, and performance, the platform will become more reliable and
  maintainable in the long term.

- **Towards Real-World Deployment:** Further ahead, steps can be taken
  to make the system more portable and convenient for use outside the
  lab. For example, a future iteration might reduce reliance on a full
  PC by allowing one Android device to act as the session coordinator,
  or by using a lightweight edge computing device (like a single-board
  computer) to replace the bulky laptop/desktop. Similarly, reducing
  system latency and power consumption could allow data to be streamed
  to a cloud service for remote monitoring, or processed on-device to
  provide immediate feedback. These enhancements would bring the project
  closer to field applications such as ambulatory stress monitoring or
  on-site health assessments, where a smaller and more autonomous setup
  would be beneficial. While these ideas are beyond the immediate scope
  of the current project, they illustrate how the platform could evolve
  into an even more portable and versatile research tool with continued
  development.
