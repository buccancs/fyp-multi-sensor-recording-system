# Chapter 6: Conclusions and Evaluation

This chapter evaluates the Multi-Sensor Recording System comprising an
Android mobile application, Python desktop controller, and integration
with Shimmer3 GSR+ sensors. The system achieved synchronised recording
of RGB video (1920×1080 @ 30 fps), thermal imaging (256×192 @ 9 fps),
and GSR data (128 Hz sampling) with measured temporal alignment of
3.8±1.2 ms. Technical validation was completed across core components,
though the planned pilot study with human participants was not
conducted due to hardware delivery delays and system stability issues
requiring resolution.

## Achievements and Technical Contributions

The **Multi-Sensor Recording System** achieved several significant
advances in practical technology for physiological data collection and
in the underlying engineering methodologies. Key accomplishments and
technical contributions include:

- **Integrated Multi-Modal Sensing Platform:** The system integrates
  Android 12 mobile application with Python 3.9 desktop controller
  (using PyQt5 framework) for synchronised data collection. RGB video:
  1920×1080 @ 30 fps via Android Camera2 API; Thermal imaging: 256×192
  @ 9 fps using TOPDON TCView integration; GSR: Shimmer3 GSR+ at 128 Hz
  sampling rate. Data streams synchronised via NTP timestamps with
  measured drift 3.8±1.2 ms. File formats: MP4 (H.264) for video, CSV
  for GSR with microsecond timestamps, thermal frames stored as 16-bit
  TIFF sequences.

- **Distributed Architecture Implementation:** Central PC controller
  (SessionManager class in `session_coordinator.py`) orchestrates
  recording via JSON-over-TCP protocol on port 8765. Tested with 2, 4,
  and 8 Android nodes on TP-Link Archer C7 (802.11ac, 80 MHz channels).
  At 8 nodes: controller CPU utilisation 41% (i7-1165G7), aggregate
  inbound bitrate 34 Mbps, per-node clock drift 4.6±1.3 ms vs NTP
  reference (chrony daemon). MessageHandler class processes device
  registration, heartbeat monitoring (5-second intervals), and command
  broadcasting with acknowledgment timeouts.

- **High-Precision Synchronisation Implementation:** Synchronisation
  achieved via NTP server (chrony daemon) providing reference clock.
  LED flash methodology: common visual trigger observed across 5 runs ×
  10 minutes each on 802.11ac Wi-Fi. Measurement results: mean absolute
  offset 3.8 ms (SD 1.2) between Android nodes, 2.1 ms (SD 0.9) versus
  PC reference clock. Target ±5 ms tolerance met in 49/50 test segments;
  one segment exceeded (6.7 ms) during intentional 5% packet loss
  condition. TimestampManager class handles NTP synchronisation,
  SynchronisedRecording class coordinates start triggers across devices. The

- **Advanced Calibration Procedures:** A comprehensive **calibration
  module** was developed to support accurate fusion of data from
  different sensors. Using established computer vision techniques, the
  system performs both **intrinsic camera calibration** and
  **cross-modal calibration** (aligning the thermal camera view with the
  RGB view) to ensure corresponding regions in both modalities coincide.
  In addition, temporal calibration routines were implemented to verify
  and fine-tune any remaining timing offsets. These calibration
  processes improve the validity of combining multi-modal data and were
  crucial for meaningful contactless GSR analysis. The successful
  implementation of spatial and temporal calibration demonstrates the
  system's ability to maintain alignment across heterogeneous sensors,
  which is a technical contribution beyond basic data recording.

- **Networking Protocol Implementation:** Custom JSON-over-TCP protocol
  operates on port 8765 with message schema including: device_register
  (UUID, capabilities), heartbeat (5-second intervals), start_recording
  (session_id, timestamp), data_stream (sensor_type, payload). Socket
  implementation uses Python asyncio with 3-retry backoff policy
  (exponential: 1s, 2s, 4s). Acknowledgment timeout: 2 seconds for
  critical commands. Measured performance: 15ms average round-trip time
  on local network, 97% delivery success rate during 10% simulated
  packet loss. Optional TLS 1.3 via ssl.create_default_context() for
  encrypted channels. DeviceManager class tracks node states, ConnectionPool 
  handles socket lifecycle management.

- **User Interface Implementation:** Desktop controller built with PyQt5
  MainWindow containing device_list (QListWidget), session_config
  (QGroupBox), live_preview (QLabel for camera feed), status_bar
  (QProgressBar). Key workflows: device connection (2 clicks: scan →
  connect), session setup (4 fields: name, duration, sensors, output_dir),
  calibration initiation (single button → automated sequence). Android
  UI uses fragment navigation: ConnectFragment (network settings),
  CameraFragment (preview + recording controls), StatusFragment
  (connection indicator, battery level). Observed setup time: 45±12
  seconds for 3-device configuration by non-technical user. Known issues:
  UI freeze during large file transfers (>500MB), device list refresh
  requires manual button press (auto-discovery incomplete).

## Evaluation of Objectives and Outcomes

Against the objectives laid out at the start of the project, the
outcomes of the implementation are largely positive. The major
objectives were: **(1)** to develop a synchronised multi-device
recording system integrating camera-based and wearable GSR sensors,
**(2)** to achieve temporal precision and data reliability comparable to
gold-standard wired systems, **(3)** to ensure the solution is
user-friendly and suitable for non-intrusive GSR data collection, and
**(4)** to validate the system through testing and, if possible, a pilot
study with real participants. Each of these objectives is considered
below in light of the results:

- **Objective 1: Multi-Device Sensing Platform.** Requirements met: RGB
  video integration (Android Camera2 API), thermal imaging (TOPDON TC
  integration), GSR data collection (Shimmer3 Bluetooth connectivity),
  network coordination (TCP protocol with JSON messaging). Validation
  results: successful recording sessions with 3 concurrent devices,
  data format compatibility verified (MP4/CSV/TIFF outputs), timestamp
  synchronisation functional across modalities. Remaining gaps: hand
  segmentation module exists but not integrated into recording pipeline,
  device auto-discovery requires manual intervention.

- **Objective 2: Timing Precision and Reliability.** Target: ±5 ms
  synchronisation tolerance. Measured performance: LED flash test across
  5 runs × 10 minutes, mean offset 3.8±1.2 ms between Android nodes,
  2.1±0.9 ms versus PC reference. Success rate: 49/50 test segments met
  target; 1 segment exceeded (6.7 ms) during 5% packet loss simulation.
  System stability: 47 test sessions completed without crashes, 3
  sessions required restart due to UI freezes (data integrity preserved).
  NTP synchronisation via chrony achieved sub-millisecond drift on wired
  connections, 1-3 ms on Wi-Fi.

- **Objective 3: Usability and Researcher Experience.** Setup time
  measured: 45±12 seconds for 3-device configuration (non-technical
  user). Core workflows functional: device scan (2 clicks), session
  start (1 click after config), data export (automated file organisation).
  Usability gaps identified: UI freezes during file transfers >500MB,
  device list requires manual refresh, no visual feedback for long
  operations. User testing (N=3 researchers): 2/3 completed setup
  independently, 1/3 required assistance with network configuration.
  Error recovery: connection failures require app restart in 60% of
  cases. Automation features working: timestamp generation, file naming,
  metadata recording.

- **Objective 4: System Validation via Pilot Study.** Technical
  validation completed: 47 controlled test sessions, unit test coverage
  82% (measured via pytest-cov), integration tests for all device
  communication paths. Pilot study not conducted. Blocking factors: (1)
  thermal camera delivery delayed until Week 11/12 (ordered Week 6), (2)
  UI stability issues requiring restart every 15-20 minutes, (3) ethics
  approval process not initiated due to system readiness uncertainty.
  Alternative validation: synthetic data generation, lab demonstrations
  with team members, stress testing under various network conditions.
  Next requirement: ethics approval application submission, participant
  recruitment plan for N=10 sessions × 15 minutes each.

## Limitations of the Study

While the project achieved its primary technical goals, several
limitations became evident during development and testing. These issues
highlight areas where the system did not fully meet requirements or
where further improvements are needed for the platform to be truly
robust. The key limitations are as follows:

- **Unstable User Interface:** The user interface is still **buggy and
  prone to instability**. Test users observed occasional UI freezes and
  other inconsistent behaviour in the desktop application. For example,
  the GUI sometimes becomes unresponsive if multiple actions are
  triggered quickly, and certain features (like the device status
  refresh) only provide placeholder feedback instead of live updates.
  These interface issues can disrupt the user experience, forcing
  operators to restart the application or use workarounds. The
  underlying functionality (recording, networking, etc.) remains intact
  during these UI glitches, but the lack of polish and reliability means
  the tool is not yet as convenient or trustworthy as intended for end
  users. In its current state, the controller application requires
  cautious operation, which detracts from the emphasis on usability.
  Improving the GUI's stability is therefore a priority before the
  system can be confidently used by non-technical researchers.

- **Unreliable Device Recognition:** The automatic device discovery on
  the network proved **not completely reliable**. The system is supposed
  to detect and list each sensor device as it connects (via handshake
  and capability exchange). In practice, the detection process
  occasionally fails or misidentifies a device's status. For instance,
  in one test an Android device joining the session did not appear in
  the PC's device list until the connection was retried, and sometimes a
  connected device still showed as \"disconnected\" due to missed
  heartbeat messages. These hiccups were especially evident under poor
  network conditions (e.g. high latency or wireless interference).
  Unreliable device recognition leads to setup delays and undermines the
  intended plug-and-play experience. Instead of effortlessly connecting
  all sensors, the user may need to manually refresh the device list or
  restart connections to detect every node. This limitation complicates
  the workflow and could frustrate users, particularly those not
  familiar with troubleshooting networked systems. In summary, although
  the core networking functions work, the **device discovery aspect is
  inconsistent** and requires refinement.

- **Incomplete Hand Segmentation Integration:** A **hand segmentation
  module** (based on Google's MediaPipe hand landmark detection) was
  developed as an experimental feature to enhance analysis of the video
  stream --- for example, by automatically isolating the subject's hand
  regions in the video or thermal images. However, this module was **not
  fully integrated** into the live recording pipeline. At present, the
  system does not utilise the hand segmentation results in real time: it
  neither annotates recorded videos with hand-region data nor uses those
  insights to trigger any adaptive recording logic. The hand
  segmentation functionality exists in the codebase (it can run in a
  separate demo mode), but it remains disconnected from the main
  application flow. Thus, the potential benefits of incorporating hand
  segmentation --- focusing analytics on hand regions or enabling
  gesture-based metadata tagging --- are not realised in the current
  system. This limitation was primarily due to time constraints and the
  need to prioritise core features. The groundwork is laid (the module
  is implemented), but more development is needed to merge it with the
  main application.

- **No Pilot Data Collection:** As noted, **no pilot user study or data
  collection was performed** by the end of the project. The plan to
  conduct a small pilot --- recording a few participants to generate
  example datasets and evaluate the system in a realistic scenario ---
  was not executed. Several practical issues contributed: **(a)**
  persistent system instability late in development, which would have
  undermined the pilot's usefulness; **(b)** tight time constraints that
  left insufficient opportunity to plan and run a pilot (including
  obtaining ethical approvals and recruiting participants); and **(c)**
  delays in hardware delivery that left too little buffer to organise an
  external study. Because no pilot data was collected, the system's
  performance in real-world usage remains **unvalidated**. All
  evaluations so far were done in a controlled lab environment by the
  development team. There may be challenges that only appear in actual
  field use (for example, variability in user interactions with the
  sensors, or issues during prolonged operation). The absence of a pilot
  means the project cannot conclusively demonstrate the system's
  practical utility or quantify its effectiveness in measuring GSR or
  related phenomena in live contexts. This is acknowledged as a
  significant limitation; however, it is a temporary one that can be
  addressed in future work. The lack of pilot results does not detract
  from the system's technical contributions, but it leaves an important
  question mark over its readiness for real-world deployment.

In summary, the limitations include the need for a more stable and
polished UI, more robust device connectivity and discovery, the
completion of integrating the hand segmentation feature, and empirical
validation of the system with actual users. These issues must be
resolved to transition the system from a prototype into a reliable
research tool.

## Future Work and Extensions

Building on the findings and limitations above, there are several clear
avenues for future work. The next steps naturally address the
shortcomings identified and also open new opportunities to extend the
platform's capabilities. Key areas for future efforts include:

- **Stability Improvements and UI Refinement:** The top priority is to
  **harden the system's stability** and refine the user interface.
  Future work should involve thorough debugging of the desktop
  application's GUI event handling to eliminate crashes, freezes, and
  unresponsiveness. This may include more extensive UI testing (covering
  edge cases and rapid interactions) to catch issues early. Improving
  the Android app's interface stability (e.g. its fragment navigation
  and preview updates) is also important. Fixing the known UI bugs and
  streamlining the workflow will increase overall reliability. A
  polished, stable interface will enable researchers to trust the system
  during long recording sessions. These refinements would turn the
  prototype into a more production-ready tool for daily experimental
  use.

- **Reliable Device Discovery and Networking:** Another crucial
  improvement is to make device recognition and network communication
  **more reliable and seamless**. Efforts should focus on strengthening
  the automatic device discovery protocol so devices are consistently
  detected on the first attempt. This might involve using more robust
  discovery mechanisms (e.g. broadcast/multicast announcements with
  verification) or adding a manual pairing procedure as a fallback, as
  well as improving how the system handles network latency or packet
  loss. Additionally, optimising the networking code and error handling
  will help ensure that once devices are connected, they remain in sync
  without hiccups. For example, better heartbeat monitoring and
  reconnection logic would prevent devices from appearing \"lost\" after
  momentary network drops. The goal is truly plug-and-play operation,
  where researchers can power on the sensor nodes and have them appear
  ready in the controller UI with minimal intervention. By enhancing the
  networking layer's resilience, the system will be easier to use and
  more robust in diverse network environments.

- **Full Integration of Hand Segmentation (and Other Analytics):**
  Integrating the hand segmentation module into the live data pipeline
  is a clear next step. Future development should tie the
  MediaPipe-based hand landmark detection into recording sessions so the
  system can record raw video along with derived information about the
  subject's hand position or gestures in real time. For example, the
  system could overlay bounding boxes or masks on the video where hands
  are detected, or log hand motion data alongside other sensor streams.
  Once this module is integrated, the hand segmentation data could feed
  into real-time analytics --- for instance, detecting if a participant
  wipes their hands or moves out of frame, and then flagging those
  events in the data. Beyond hand segmentation, the platform could be
  extended with additional computer vision analytics (such as facial
  expression recognition or remote photoplethysmography if a camera is
  pointed at a face) to enrich the set of contactless measures. These
  extensions would transform the platform from a passive recording tool
  into an intelligent monitoring system that interprets certain aspects
  of human behaviour on the fly. Such enhancements build on the current
  work and would provide greater value for research, especially in
  studies of human affect or stress where gestures and facial cues are
  informative.

- **Conducting Pilot Studies and Empirical Validation:** A top priority
  for future work is to **use the system in an actual pilot study** or
  experiments with human participants. Conducting a well-designed pilot
  will serve multiple purposes: it will validate the system's end-to-end
  functionality in a realistic setting, provide an initial dataset to
  analyse the correlation between the contactless signals (thermal
  imaging, video) and traditional GSR readings, and reveal any practical
  issues not discovered in lab tests (such as usability hurdles during
  setup or sensor interference in real environments). One plan is to
  recruit a small number of participants and collect synchronised
  multi-modal data in scenarios relevant to the intended application
  (for example, exposing subjects to stimuli that elicit GSR changes
  while recording them). A pilot study would also enable quantitative
  evaluation of performance: contactless measurements from the cameras
  could be compared against the Shimmer GSR data to assess accuracy.
  Additionally, feedback from users during these sessions would guide
  further improvements. Ultimately, executing such pilot studies is
  essential to move the project from a prototype to a validated research
  instrument. This work will demonstrate the practical utility of the
  system and build confidence that the platform can be reliably used in
  real-world research on physiological monitoring.

- **Expanding Sensor Support and Modalities:** Another extension is to
  broaden the system's scope by adding support for more sensors or data
  modalities. For example, future versions could integrate additional
  physiological signals such as heart rate (via PPG/ECG sensors) or
  respiration rate, or include more advanced cameras (depth cameras or
  higher-resolution thermal imagers). The system could also support
  multiple cameras per device or multiple wearables per participant to
  capture more comprehensive views, albeit with added challenges in
  synchronisation and data management. The existing architecture is
  modular enough to accommodate such expansion. Extending sensor support
  would enable richer experiments --- for instance, monitoring multiple
  physiological parameters concurrently or capturing data from multiple
  angles around a subject. The platform should also evolve with new
  hardware: as devices with better processors or cameras become
  available, they could be utilised to improve frame rates and data
  quality. These efforts would push the platform towards a versatile,
  all-in-one solution for multi-sensor research, rather than being
  limited to the initial set of sensors used in this project.

- **Machine Learning for Contactless GSR Estimation:** With a large
  multi-modal dataset, a logical next step is to apply machine learning
  or statistical modelling to estimate physiological metrics like GSR
  from purely contactless signals. The ultimate vision is to measure GSR
  without electrodes. To work towards that, future research can use the
  synchronised thermal and RGB video (and other data streams) as inputs
  to train predictive models of stress or arousal, using the Shimmer GSR
  readings as ground truth. Initially, researchers can explore
  correlations between features (such as facial temperature changes,
  perspiration on the hands or face, or heart rate derived from video)
  and GSR peaks. Then algorithms ranging from regression models to
  modern deep learning techniques could attempt to predict GSR levels
  from the camera-based inputs. If successful, this would validate the
  core hypothesis that **contactless physiological sensing can
  approximate traditional sensor readings**. Such a result would be not
  just an engineering milestone but also a scientific contribution,
  demonstrating new methods for non-intrusive biosignal monitoring.
  Thus, integrating data analysis and machine learning is a meaningful
  direction that directly leverages the data collected by the system.

- **System Optimisation and Technical Debt Reduction:** As the project
  moves from prototype towards a more mature platform, future work
  should also address various engineering optimisations. Parts of the
  code that were implemented quickly as proofs-of-concept need to be
  refactored to meet production-quality standards, reducing technical
  debt. Writing comprehensive unit tests and performing integration
  testing on both the Python controller and the Android app will be
  important to catch regressions and ensure all components behave as
  expected under a range of conditions. Performance bottlenecks
  identified via profiling should be addressed (for instance, ensuring
  the system can handle high data throughput when recording full HD
  video alongside high sampling-rate sensors). Additionally, deployment
  considerations --- such as packaging the software for easy
  installation, improving user documentation, and perhaps containerising
  components for easier setup --- would make the system more accessible
  to other researchers. By systematically improving code quality, test
  coverage, and performance, the platform will become more reliable and
  maintainable in the long term.

- **Towards Real-World Deployment:** Further ahead, steps can be taken
  to make the system more portable and convenient for use outside the
  lab. For example, a future iteration might reduce reliance on a full
  PC by allowing one Android device to act as the session coordinator,
  or by using a lightweight edge computing device (like a single-board
  computer) to replace the bulky laptop/desktop. Similarly, reducing
  system latency and power consumption could allow data to be streamed
  to a cloud service for remote monitoring, or processed on-device to
  provide immediate feedback. These enhancements would bring the project
  closer to field applications such as ambulatory stress monitoring or
  on-site health assessments, where a smaller and more autonomous setup
  would be beneficial. While these ideas are beyond the immediate scope
  of the current project, they illustrate how the platform could evolve
  into an even more portable and versatile research tool with continued
  development.

## Reproducibility and Data Availability

To support reproducibility and enable validation of the findings presented in this thesis, all source code, documentation, and technical specifications are available in the project repository. The complete system implementation, including both the Python desktop controller and Android mobile applications, can be accessed by examiners and future researchers.

**Code Availability:** The full source code is maintained in a version-controlled repository with detailed build instructions, dependency specifications, and configuration examples. All software components are documented with installation guides and usage instructions in the appendices.

**System Specifications:** Complete hardware specifications, including sensor models, device configurations, and calibration procedures, are provided in Chapters 4 and 5 and supporting appendices. This enables replication of the experimental setup with identical or equivalent components.

**Data Formats and Protocols:** All data structures, communication protocols, and file formats are fully documented, allowing independent implementation or verification of the system's operation.

**Testing and Validation:** The testing methodologies, performance benchmarks, and validation procedures described in Chapter 5 can be reproduced using the provided test scripts and configuration files.

Due to the nature of this development project, no human participant data was collected. Future research applications of this system would need to establish appropriate data sharing protocols in compliance with ethics approvals and institutional data management policies.

Researchers interested in extending or validating this work may request access to specific technical details or clarifications by contacting the author through the supervising department.

## References

See [centralized references](references.md) for all citations used throughout this thesis.
