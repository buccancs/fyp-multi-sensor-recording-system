# Chapter 4: Design and Implementation

## 4.1 System Architecture Overview (PC--Android System Design)

At a high level, the Multi-Sensor Recording System employs a distributed architecture comprising an Android mobile application and a Python-based desktop controller. The Android device serves as a dedicated sensor node for data capture, while the PC functions as the central coordinating hub. This PC–Android system design adopts a master–slave paradigm: the desktop Python application orchestrates one or more Android sensor nodes to achieve tightly synchronized operation across all devices. The architecture strikes a balance between individual device autonomy and centralized oversight. Each Android device can manage its own sensors and data logging locally (even running standalone if needed), but under the desktop controller's supervision all devices participate in unified sessions with common timing and control. The design prioritizes temporal synchronization, reliability, and modularity. A custom network communication layer links the mobile and desktop components, carrying command-and-control messages, status updates, and data previews over a Wi-Fi or LAN connection. Communication is event-driven and buttressed by robust error handling, so that transient network issues or device glitches do not compromise an ongoing session. Additionally, each device buffers and stores data locally. This means that even if connectivity drops momentarily, data collection on that device continues uninterrupted; once the connection is re-established, the system can seamlessly realign the buffered data streams in time. This fault-tolerant design—bolstered by comprehensive logging on both the mobile and PC sides—ensures data integrity and consistency throughout every recording session. Figure 4.1 illustrates the system architecture, highlighting the central role of the desktop controller and its network links to the mobile sensor nodes. Figure 4.1: System architecture overview of the multi-sensor recording system. This diagram depicts the central desktop controller (PC) communicating with one or more Android devices over a network. Each Android device interfaces with onboard and external sensors (cameras, thermal sensor, GSR sensor) and handles local data acquisition and storage. The desktop controller provides a GUI for the user and runs coordination services (network server, synchronisation engine, data manager), sending control commands to the Android app and receiving live status and preview data. The design shows a hybrid star topology: the PC is the hub coordinating distributed mobile nodes, enabling synchronized start/stop triggers, real-time monitoring, and unified timekeeping across the system.

## 4.2 Android Application Design and Sensor Integration

The Android application serves as a comprehensive sensor data collection platform, integrating both the phone's native sensors (e.g. the built-in camera) and external sensor devices. It is developed in Kotlin and is organized with a clear layered architecture to separate concerns. In particular, the app follows an MVVM (Model–View–ViewModel) design pattern: a lightweight UI layer (Activities/Fragments and their ViewModels) interacts with a robust business logic layer comprised of managers and controllers, which in turn rely on lower-level sensor interfacing components. This architecture is designed for high modularity and maintainability, allowing each sensor modality (camera, thermal, GSR, etc.) to be managed in isolation while ensuring all subsystems remain synchronized under the hood. Several key components structure the app's functionality. A SessionManager module coordinates the overall recording sessions, a DeviceManager handles the attached sensor devices, and a ConnectionManager manages the network link to the PC controller (see Appendix F, Section G.203 for connection management implementation[\[G.203\]](AndroidApp/src/main/java/com/multisensor/recording/recording/ConnectionManager.kt#L14-L50)). Beneath these, the data acquisition layer defines specialized recorder classes for each sensor modality—for example, a CameraRecorder for the phone's RGB camera, a ThermalRecorder for the USB thermal camera, and a ShimmerRecorder for the GSR sensor—each encapsulating the details of interfacing with its respective hardware. Each recorder funnels its sensor's data into the session management framework and ultimately into local storage, while also forwarding live preview data through the network layer for remote monitoring. The application heavily leverages Android's asynchronous programming features (threads, Handlers, and coroutines) to handle the high data rates from multiple sensors in parallel. This ensures that tasks like writing to storage or processing sensor inputs do not block the main UI thread. We also integrate dependency injection (via Hilt) to manage cross-cutting concerns such as logging and configuration, which further decouples components and keeps the codebase modular. Importantly, the Android app is designed to achieve precise time alignment of multi-modal data at capture time. All sensor readings and frames are timestamped against a common reference clock (either the system clock or a synchronized master clock) at the moment of recording (see Appendix F, Section G.200 for master clock synchronization implementation[\[G.200\]](PythonApp/master_clock_synchronizer.py#L85-L106)). For example, when a session begins via a remote start command from the PC, the app initializes each sensor almost simultaneously and labels every piece of data with a timestamp that allows it to be correlated across devices later. In addition, the app performs certain preprocessing on-device. For instance, a hand region segmentation module (built on MediaPipe) runs in real time on the camera feed to detect hand landmarks. This feature helps focus subsequent analysis on specific regions of interest (like the subject's hand or face in the thermal images) without requiring external post-processing. In effect, the Android application functions as a flexible yet controlled data collection node, seamlessly integrating heterogeneous sensors under a unified workflow. (Figure 4.2: Layered design of the Android application. The figure outlines the app's architecture with four layers: a presentation layer (UI and ViewModels for user interaction and state), a business logic layer (managers like SessionManager, DeviceManager, and ConnectionManager coordinating recording and devices), a data acquisition layer (sensor-specific recorder modules for camera, thermal, and GSR, as well as processing components like hand segmentation), and an infrastructure layer (network communication client, local storage handlers, and performance monitors). Arrows illustrate the flow: user actions in the UI invoke ViewModel updates, which delegate to managers that control the sensor recorders. The recorders produce data that is stored locally and also sent through the network layer to the PC. This diagram highlights modular separation: each sensor integration is implemented in its own module, all orchestrated by the central session manager.)

### 4.2.1 Thermal Camera Integration (Topdon)

One distinguishing feature of the system is its thermal imaging capability, enabled by integrating a Topdon TC001 thermal camera with the Android device. The Topdon TC001 is a USB-C infrared camera accessory, and the app incorporates it through the manufacturer's SDK. The integration is designed to allow real-time thermal video capture in parallel with the phone's normal camera feed. In practice, the Android device acts as a USB host (via OTG), and the app uses the SDK's APIs for device discovery, configuration, and frame retrieval. Once the thermal camera is attached, the app's ThermalRecorder module manages its entire lifecycle: it listens for USB-connect events, requests the necessary USB permission from the Android system, and initializes the camera feed. The Topdon camera provides a thermal image at a native resolution of 256×192 pixels and a frame rate of 25 FPS, which the app uses as the default thermal video mode. These parameters (resolution, frame rate, and calibration settings) are configurable via a ThermalCameraSettings profile if needed to balance image detail against performance; by default, however, the system opts for the maximum available resolution and a frame rate typical of the device. Each captured thermal frame includes both an infrared image (often visualized as a color or grayscale thermogram) and the underlying temperature reading for each pixel. The ThermalRecorder retrieves frames via the SDK's callbacks on a background thread to avoid blocking the UI. It timestamps each frame with a high-resolution timestamp (synchronized to the system or master clock) and queues the frame for further processing and storage. The app continuously writes the raw thermal data stream to a file in real time. This file uses a proprietary binary format containing a header (with metadata such as resolution, frame rate, and calibration parameters) followed by the sequence of frames, each tagged with its timestamp. This raw log preserves all thermal data for precise post-hoc analysis of temperature values. In parallel, the app can also generate human-viewable thermal images (e.g. JPEG frames) from the raw data to provide quick feedback to the user. The ThermalRecorder can supply an optional downsampled live preview feed: it converts incoming thermal frames into a viewable image (applying a false-color colormap and appropriate scaling) and streams these preview frames over the network to the desktop controller. This live preview gives the researcher immediate visual confirmation of what the thermal camera sees, which is invaluable for making sure the sensor is properly aimed and operating as expected during a session. Integrating the Topdon device presented a few challenges that the design needed to address. First, USB power and bandwidth management on the phone required careful handling. The app monitors the camera's connect/disconnect events and handles unexpected unplugging gracefully—for example, if the camera is removed mid-session, the system logs a warning and stops the thermal recorder, but other sensors continue unaffected. Another key consideration was keeping the thermal data in sync with the other modalities: all thermal frames are timestamped in the same time base as the phone's video frames and GSR samples, ensuring the thermal data can be temporally aligned with the RGB video and physiological signals during analysis. As a result of these measures, the thermal imaging module is tightly integrated into the Android device's sensing capabilities, adding this modality with minimal latency. Importantly, the thermal camera integration is completely incorporated into the session workflow—the user can simply toggle thermal recording on or off for a given session, and if it is enabled, the system will automatically initialize the Topdon camera when the session starts (on command from the PC) and begin capturing thermal data concurrently with the other sensors. Likewise, it gracefully finalizes and closes the thermal device when the session ends. All of this happens behind the scenes, preserving a seamless user experience. (Figure 4.3: Thermal camera integration flow. This figure illustrates how the Android app interfaces with the Topdon TC001 thermal camera via USB. When the camera is plugged in, the app's USB listener requests permission from the Android OS and initializes the Topdon SDK. During a recording session, the app continuously pulls thermal frames from the camera at ~25 Hz. Each frame is time-stamped and written to local storage as part of a thermal data file, and simultaneously a scaled preview image is sent over the network to the PC for real-time monitoring. The diagram also highlights the coordination required: the PC's "start recording" command triggers the camera initialisation (opening the USB device and starting capture) almost concurrently with other sensors, ensuring the thermal stream is synchronized with the overall session timeline.)

### 4.2.2 GSR Sensor Integration (Shimmer)

Beyond cameras, the system also incorporates a physiological sensor—specifically a Shimmer3 GSR+ device—which captures galvanic skin response (electrodermal activity) and can optionally provide additional signals like photoplethysmogram (PPG) and motion (via built-in accelerometers). The Shimmer3 GSR+ is a research-grade wearable sensor that connects to the phone via Bluetooth. The Android app's ShimmerRecorder module is responsible for managing this Bluetooth connection and logging the data from the device (see Appendix F, Section G.202 for full implementation[\[G.202\]](AndroidApp/src/main/java/com/multisensor/recording/recording/ShimmerRecorder.kt#L40-L80)). When the app starts up (or whenever the user initiates a Shimmer connection), it scans for the Shimmer device and pairs with it (using the device's default PIN and name as needed). The integration uses the Shimmer Android SDK (from Shimmer Research), which provides an API—classes like ShimmerBluetoothManagerAndroid—to handle the low-level Bluetooth link and streaming of sensor data. After establishing the connection, the app configures the Shimmer device with the desired sensor channels and sampling rate. By default it enables the GSR (skin conductance) channel on the Shimmer, along with the PPG channel and basic kinematic channels (the accelerometer axes) to provide context, all sampled at 51.2 Hz. The GSR measurement range on the device is set to a suitable sensitivity (for example, a ±4 µS range) to capture typical skin conductance levels. All of these default settings can be adjusted in the app's settings if a different configuration is needed for a particular experiment. Data from the Shimmer arrives as a continuous stream of packets, each containing a set of measurements (GSR, PPG, etc.) along with a timestamp from the Shimmer's own internal clock. The ShimmerRecorder runs a dedicated thread to listen for these incoming packets through the Shimmer SDK's callback system. As each packet comes in, the app immediately captures a corresponding local timestamp (to mark when it was received) and then parses out the sensor values. The readings are buffered and appended in real time to a CSV file that serves as the session's physiological data log. Each line of this CSV file typically includes: a timestamp (in milliseconds) from the phone's perspective, the original device timestamp from the Shimmer (for reference), the GSR conductance value (in microsiemens), any PPG readings, acceleration data, and the Shimmer's battery level. For example, the CSV's header might be: Timestamp_ms, DeviceTime_ms, SystemTime_ms, GSR_Conductance_uS, PPG_A13, Accel_X_g, ... Battery_Percentage, clearly enumerating each field. Recording both the device-reported times and the local reception times allows the system to later evaluate any clock drift or transmission delays and correct for them during analysis. Like the thermal module, the Shimmer integration also offers a live data streaming feature for monitoring purposes. The ShimmerRecorder can send sampled data points (for instance, the latest GSR value) over a network socket to the desktop in real time (maintaining an optional streaming socket on a designated port). On the desktop side, those incoming samples could be plotted as a live graph or even trigger alerts (for example, if a physiological signal exceeds a certain threshold during the experiment). Nevertheless, the primary record of the GSR data remains on the Android device itself, which ensures that no data is lost even if the network stream is lagging. The system includes safeguards for connectivity as well. If the Bluetooth link to the Shimmer drops during a recording, the app automatically attempts to reconnect a few times (with brief delays between tries). All reconnection attempts (and any resulting data gaps) are recorded in the log for transparency. In practice, keeping a Bluetooth connection stable over long sessions can be challenging due to wireless interference and smartphone power-management policies. To mitigate this, our implementation uses Android's modern Bluetooth APIs and appropriately handles runtime permissions (especially on Android 12+, which requires fine- and coarse-location permission for Bluetooth scanning). By supporting both legacy and newer Bluetooth permission models, the app maintains compatibility across different Android OS versions. In summary, the Shimmer GSR integration adds the ability for the system to capture high-quality physiological signals in perfect sync with the video and thermal streams. Thanks to the modular architecture of ShimmerRecorder, this sensor's recording can start and stop in unison with the other sensors under the coordination of the session manager. When a session is initiated, the app (upon the PC's command) establishes the Bluetooth link to the Shimmer, starts the data stream, and begins logging GSR data; conversely, when the session concludes, the app cleanly closes the connection and finalizes the CSV log file. The physiological data collected (e.g. the time-varying skin conductance) serves as a ground-truth timeline of the subject's arousal or stress level, which can later be directly compared to the subject's visual and thermal data. (Figure 4.4: GSR sensor (Shimmer3) integration. The figure shows the Shimmer GSR+ device wirelessly connected to the Android smartphone via Bluetooth. In a recording session, the Android app subscribes to the Shimmer's data stream, receiving packets that contain GSR and PPG readings. These data packets are timestamped and logged on the phone. The figure highlights the flow from sensor electrodes on the subject, through the Shimmer device's analogue front-end (measuring skin conductance), transmitted over Bluetooth to the phone, and then into the app's data recording pipeline. Any loss of connection triggers the app's reconnection logic, ensuring continuity of data. A small real-time graph icon on the PC side suggests that as data is recorded, key values (like GSR level) are also sent to the desktop for live display.)

## 4.3 Desktop Controller Design and Functionality

The desktop controller is a Python-based application with a comprehensive graphical user interface, acting as the command center for the whole multi-sensor system. The GUI is built with the PyQt5 framework, and beneath it runs a collection of backend services and managers that handle tasks like device communication, data management, and synchronization. Architecturally, the desktop application is organized into layers and components that mirror many of the roles present in the Android app—albeit at a higher, system-wide level of coordination. A Presentation Layer forms the top level of the desktop software. It comprises the main window and a set of UI panels (tabs) that the researcher uses to control and monitor the system. For example, the interface includes:
Devices tab – to manage connected Android devices and other sensors,
Recording tab – to start/stop recording sessions and to view live previews of sensor data,
Calibration tab – to perform multi-camera calibration procedures (e.g. capturing chessboard images for camera alignment),
Files/Analysis tab – to review and manage the recorded data after a session.
This user interface is designed to be intuitive and responsive, giving real-time feedback on system status (such as device connection state, battery levels, and recording progress) during an experiment. Beneath the UI is the Application Layer, which implements the core logic. The centerpiece of this layer is the main application controller (or "Session Manager") on the PC, which orchestrates the workflow of each recording session: it responds to user inputs from the UI, coordinates timing across devices, and updates the UI with status information. Alongside this, the desktop has several specialized manager modules—such as a DeviceManager (to keep track of all connected Android devices and any other sensors like USB webcams), a CalibrationManager (to conduct multi-camera calibration routines via OpenCV), and optionally a StimulusController (if the system needs to present visual or audio stimuli to the subject during an experiment). Each of these components encapsulates a distinct area of functionality, following a clear separation-of-concerns principle. For instance, the DeviceManager is responsible for discovering devices and maintaining their connection status, but it delegates actual communication tasks to a lower-level network service; the CalibrationManager encapsulates procedures for capturing calibration images and computing camera alignment parameters without cluttering the main application logic. At the next layer down, the desktop application includes a set of Service Layer backend components that handle specific types of I/O and background processing. Notable services in this layer include:
Network Service – implements the socket server that listens for connections from the Android apps,
Webcam Service – controls any USB webcams attached to the PC (if the study uses external cameras in addition to the phone's camera),
Shimmer Service – provides an alternative way to connect to Shimmer sensors directly from the PC (via a Bluetooth dongle),
File Service – manages data storage and file operations on the PC side.
The inclusion of both an Android-side Shimmer integration and a PC-side Shimmer Service is intentional: the system is designed to support multiple configurations. In some sessions, an Android phone worn by a subject might handle the Shimmer data as described in Section 4.2.2. However, the desktop application can also connect to a Shimmer device directly (for instance, in a lab setting where the PC itself is within Bluetooth range of the sensor). This design provides multi-path integration for the GSR sensor to improve robustness. The desktop's ShimmerManager can accept data from either a direct Bluetooth connection or via the Android's relayed stream, with one path serving as a fallback for the other. This dual pipeline ensures that even if one data path encounters an issue, the physiological data can still be acquired through the other. All these services feed into the Infrastructure Layer on the PC, which handles cross-cutting concerns like logging, synchronization, and error management. A dedicated synchronization engine runs on the desktop to maintain the master clock and keep time aligned across devices (details in Section 4.4). A centralized logging system records events from all parts of the application (device connect/disconnect events, commands sent, errors, etc.) for debugging and audit purposes. The infrastructure also includes an error handling module and a performance monitor that together track the system's health: for example, if a device unexpectedly disconnects, the UI is immediately notified and the system either attempts an automatic reconnection or gracefully marks that device as inactive for the remainder of the session. From a functionality perspective, the desktop controller offers the researcher a one-stop interface to manage multi-device recording sessions. Using the UI, the user can configure an experiment session (select which devices and sensors will be active, set participant or session metadata, etc.), then initiate a synchronized start. When the user hits "Start," the controller dispatches start commands to all connected Android devices (and also starts any local recordings like a webcam or a PC-connected Shimmer) nearly simultaneously. During recording, the PC interface updates in real time to give the operator confidence that all modalities are working properly. For example, the controller displays live preview thumbnails from each Android's camera feed, and it shows real-time numeric readouts or simple graphs of sensor data (such as GSR values) as they stream in. It also continuously refreshes status indicators like each phone's battery level, available storage, and the current timestamp offsets between devices. The desktop software even performs background synchronization checks: it monitors for any clock drift among the devices or signs that a device's data stream is lagging. If a potential issue is detected (for instance, if one phone's clock starts to diverge or a sensor's data rate slows down), the system can warn the user so that it can be addressed promptly. When the researcher stops the session, the controller issues a coordinated stop command to all devices and waits for each device to confirm that it has safely finalized its data. It then collates the session's metadata — for example, the filenames produced by each device, any recorded timing offsets, and calibration information — and can present a summary of the session or save a manifest file for later reference. In addition to core recording controls, the desktop software offers some utility features for specialized tasks. One is a camera calibration tool (built with OpenCV) that helps the researcher calibrate and align multiple cameras: for instance, the user can capture images of a checkerboard pattern using the different cameras (phone RGB, thermal, etc.) and the tool will calculate the calibration parameters to map between their viewpoints. Another feature is a stimulus presentation module that can display images or play audio on an attached screen as part of the experimental protocol (useful if the study requires showing stimuli to the participant). Both of these features are integrated into the desktop UI and are controlled through the same session manager, ensuring that any calibration captures or presented stimuli are properly timestamped and synchronized with the sensor data. In summary, the desktop controller is effectively the brains of the system, orchestrating all components to work in unison. It masks the complexity of managing multiple distributed devices by providing a single unified interface and by automating the low-level details of communication and timing. Implementing it in Python with Qt (plus libraries like OpenCV, NumPy, and PySerial/Bluetooth) gives the software both power and flexibility in a research setting: the controller can be easily extended or scripted to support new sensors or analysis routines, yet it still meets real-time performance requirements thanks to optimized libraries and an asynchronous design. This combination of a robust backend with a user-friendly frontend makes the desktop controller a critical bridge between the researcher and the distributed sensor network. (Figure 4.5: High-level architecture of the Desktop Controller application. The figure breaks down the desktop software into its main components: a GUI layer (with windows/tabs for Recording Control, Device Management, Calibration, etc.), an application logic layer (the main orchestrator and managers for sessions, devices, and calibration), and a service layer (which includes the network socket server, webcam interface, Shimmer interface, file and data management services). The diagram also shows an infrastructure layer beneath, containing the synchronisation engine, logging system, and error handling modules that support the entire application. Arrows in the figure illustrate how user actions in the GUI propagate to the application layer (e.g., "Start Session" triggers the Session Manager), which then calls into various services (sending network commands, initialising webcams, etc.). Similarly, data flows upward: e.g., a preview frame from an Android device comes in through the Network Service and is passed to the GUI for display. The figure emphasizes modular design -- each sensor or function has a dedicated service, coordinated by the central application logic, enabling easy maintenance and future scalability.)

## 4.4 Communication Protocol and Synchronisation Mechanism

A core challenge of this project is enabling reliable, low-latency communication between the PC controller and the Android devices, along with a mechanism to synchronise their clocks for coordinated actions. The system addresses this with a custom-designed communication protocol built on standard networking protocols, and an integrated synchronisation service that keeps all devices aligned to a master clock. Communication Protocol: The Android app and desktop controller communicate over TCP/IP using a JSON-based messaging protocol. Upon startup, the desktop controller opens a server socket (by default on TCP port 9000) and waits for incoming connections. Each Android device, when its app is launched, will initiate a connection to the PC's IP address and port. A simple handshake is performed in which the Android sends an identifying message containing its device ID (a unique name or serial number) and a list of its capabilities (e.g. indicating if it has a thermal camera, GSR sensor, etc.). The PC acknowledges this and registers the device. All messages between PC and Android are formatted as JSON objects, with a length-prefixed framing (the first 4 bytes of each message indicate the message size) to ensure the stream is parsed correctly. This design avoids reliance on newline characters or other fragile delimiters that could be unreliable for binary data; instead, it robustly handles message boundaries, allowing binary payloads (like images) to be transmitted by encoding them (often images are base64-encoded within the JSON). The protocol defines several message types, including:
control commands (e.g. start_recording, stop_recording),
status updates (e.g. periodic status messages from Android with battery level, free storage, current recording status, etc.),
sensor data messages for streaming (such as preview_frame for a JPEG preview image, or gsr_sample for a live GSR data point),
acknowledgments (the devices reply to key commands with an ACK/NACK to confirm receipt).
The PC's network service is multi-threaded and non-blocking — it can handle multiple device connections simultaneously and route messages to the appropriate handlers, emitting Qt signals that update the UI or trigger internal logic. The communication is two-way: the PC can send commands to all or individual devices, and devices send asynchronous updates or data back to the PC. To support different data exchange needs, the system effectively implements multiple logical channels over this link. For example, high-frequency binary data like video preview frames or sensor streams are sent in a compact form (with minimal JSON overhead aside from a message header), whereas less frequent control commands use a more verbose but human-readable JSON structure. In conceptual terms, we can think of a control channel and a data channel operating over the same socket. In future or in extensions, the design also allots a separate file transfer mechanism (e.g. an offline file download after recording, or an HTTP/FTP transfer) if large recorded files on the phone need to be pulled to the PC; however, in the current implementation, file transfer is often done manually after sessions or through external means, and the focus of the communication protocol is on real-time coordination and monitoring. All communications happen over the local network (typically the devices are on the same Wi-Fi or Ethernet LAN). Security is not heavily emphasized in this research prototype (messages are unencrypted JSON), but the system can be isolated on a private network during experiments for safety. Synchronisation Mechanism: Achieving time synchronisation across devices is critical because we want, for instance, a thermal frame and a GSR sample that occur at the "same time" to truly represent the same moment. In a distributed system with independent clocks, our approach is to designate the desktop PC as the master clock and synchronise all other devices to it. The desktop controller runs a component called the MasterClockSynchronizer (or Synchronisation Engine) which fulfills two primary roles: it distributes the current master time to clients (devices) and coordinates simultaneous actions based on that time (see Appendix F, Section G.200 for full implementation[\[G.200\]](PythonApp/master_clock_synchronizer.py#L85-L106)). Concretely, the PC launches a lightweight NTP (Network Time Protocol) server on a UDP port (default 8889) to which devices can query for time. The Android app, upon connecting, performs an initial clock sync handshake — this can be a custom sync message or an NTP query — to measure the offset between its local clock and the PC clock. Given the typical latencies on a local network are low (on the order of a few milliseconds), this offset can be estimated with high precision using techniques akin to Cristian's algorithm or NTP's exchange (the system may send a timestamped sync message and get a response to calculate round-trip delay and clock offset). The SynchronizationEngine on the PC possibly refines this by periodic pings (e.g. every 5 seconds) to adjust for any drift during a long session. In practice, the Android device will apply any calculated offset to its own timestamps for data labelling, meaning if its clock was 5 ms ahead of the PC, it will subtract 5 ms from all timestamps to align with the master timeline. When a recording session is initiated, the PC doesn't just send a blind "start" command — it issues a coordinated start time. For example, the PC might determine "start recording at time T = 1622541600.000 (Unix epoch seconds)" a few hundred milliseconds in the future, and send a message to each device: "start_recording at T with session_id X". Each Android device receives this and waits until its local clock (synchronised to master) hits T to begin capturing data. Because all devices are sync'd to the master within a few milliseconds accuracy, this effectively aligns the start of recording across devices to a very tight margin (usually well below 50 ms difference, often within a few ms). The devices then proceed to timestamp their data relative to this common start. The PC also notes its own start time for any local recordings (like webcams) to align with the same T. During recording, the devices continue to exchange sync information. Each status update from device to PC may include the device's current clock vs. the master clock (or implicitly, the PC knows when it sent a sync and what the device's last offset was). If any device's clock starts to drift beyond an acceptable tolerance (say more than a few milliseconds), the PC can issue a re-synchronisation or simply record the drift for later correction. The synchronisation engine might incorporate simple drift compensation — for instance, if one phone tends to run its clock slightly faster, the system can predict and adjust timing gradually (rather than waiting for a large error to accumulate). In this implementation, because the recording durations might be on the order of minutes to an hour, and modern devices have reasonably stable clocks, straightforward NTP-based periodic correction is sufficient to maintain sub-millisecond alignment. Finally, the communication protocol assists synchronisation by carrying timing info in every message. The JSON messages often include timestamps. For example, when an Android sends a preview frame to the PC, it tags it with the timestamp of frame capture; the PC can compare that with its own reception time and the known offset to estimate network delay and clock skew in real-time. Similarly, the PC's commands can carry the master timestamp. This pervasive inclusion of timestamps means that even if absolute clock sync had a small error, each piece of data can be re-aligned precisely in post-processing using interpolation or offset adjustment. In summary, the PC–Android communication is realised via a reliable JSON/TCP socket protocol, enabling complete remote control and live data streaming, while the synchronisation mechanism ensures all devices operate on a unified timeline. Together, these allow the system to achieve a high degree of temporal precision: tests have shown the system tolerates network latency variations from ~1 ms up to hundreds of milliseconds without losing synchronisation. This is accomplished by designing for asynchronous, non-blocking communication and by decoupling the command from the execution time (i.e. schedule actions in the future on a shared clock). The result is a robust coordination layer that underpins the multi-modal data collection with the necessary timing guarantees. (Figure 4.6: Communication and synchronisation sequence. This figure illustrates the sequence of interactions for device connection and a synchronised session start. Initially, each Android device connects to the desktop's socket server and sends a JSON handshake (including device ID and sensor capabilities). The desktop acknowledges and lists the device as ready. The figure then shows the synchronisation phase: the desktop (master) sends a time sync request or NTP response to the phone, and the phone adjusts its clock offset. When the researcher clicks "Start" on the PC, the desktop broadcasts a StartRecording message with a specified start timestamp. All Android devices (and the PC's own data acquisition for webcams) wait until the shared clock reaches that timestamp, then begin recording simultaneously. During recording, devices send periodic Status messages (with current frame counts, battery, and time sync quality) and stream preview data (video frames, sensor samples) to the PC. The PC might send occasional Sync messages if needed to fine-tune clocks. Finally, on "Stop", the PC sends a stop command and each device halts recording and closes files, confirming back to the PC. This sequence diagram underscores how the protocol and sync mechanism work in tandem to coordinate distributed devices in time.)