\label{chap:4} \chapter{Chapter 4: Design and Implementation}

\section{4.1 System Architecture Overview (PC---Android System Design)}

At a high level, the \textbf{Multi-Sensor Recording System} employs a distributed architecture comprising an Android mobile application and a Python-based desktop controller. The Android device serves as a dedicated sensor node for data capture, while the PC functions as the central coordinating hub. This \textbf{PC–Android system design} adopts a master–slave paradigm: the desktop Python application orchestrates one or more Android sensor nodes to achieve tightly synchronized operation across all devices. The architecture strikes a balance between individual device autonomy and centralized oversight. Each Android device can manage its own sensors and data logging locally (even running standalone if needed), but under the desktop controller's supervision all devices participate in unified sessions with common timing and control. 

The design prioritizes \textbf{temporal synchronization, reliability, and modularity}. A custom network communication layer links the mobile and desktop components, carrying command-and-control messages, status updates, and data previews over a Wi-Fi or LAN connection. Communication is event-driven and buttressed by robust error handling, so that transient network issues or device glitches do not compromise an ongoing session. Additionally, each device buffers and stores data locally. This means that even if connectivity drops momentarily, data collection on that device continues uninterrupted; once the connection is re-established, the system can seamlessly realign the buffered data streams in time. This fault-tolerant design—bolstered by comprehensive logging on both the mobile and PC sides—ensures data integrity and consistency throughout every recording session.

\textit{Figure 4.1: System architecture overview of the multi-sensor recording system. This diagram depicts the central desktop controller (PC) communicating with one or more Android devices over a network. Each Android device interfaces with onboard and external sensors (cameras, thermal sensor, GSR sensor) and handles local data acquisition and storage. The desktop controller provides a GUI for the user and runs coordination services (network server, synchronisation engine, data manager), sending control commands to the Android app and receiving live status and preview data. The design shows a hybrid star topology: the PC is the hub coordinating distributed mobile nodes, enabling synchronized start/stop triggers, real-time monitoring, and unified timekeeping across the system.}

\section{4.2 Android Application Design and Sensor Integration}

The Android application serves as a comprehensive sensor data collection platform, integrating both the phone's native sensors (e.g. the built-in camera) and external sensor devices. It is developed in Kotlin and is organized with a clear layered architecture to separate concerns. In particular, the app follows an \textbf{MVVM (Model–View–ViewModel)} design pattern: a lightweight UI layer (Activities/Fragments and their ViewModels) interacts with a robust business logic layer comprised of managers and controllers, which in turn rely on lower-level sensor interfacing components. This architecture is designed for high modularity and maintainability, allowing each sensor modality (camera, thermal, GSR, etc.) to be managed in isolation while ensuring all subsystems remain synchronized under the hood.

Several key components structure the app's functionality. A \texttt{SessionManager} module coordinates the overall recording sessions, a \texttt{DeviceManager} handles the attached sensor devices, and a \texttt{ConnectionManager} manages the network link to the PC controller. Beneath these, the data acquisition layer defines specialized recorder classes for each sensor modality—for example, a \texttt{CameraRecorder} for the phone's RGB camera, a \texttt{ThermalRecorder} for the USB thermal camera, and a \texttt{ShimmerRecorder} for the GSR sensor—each encapsulating the details of interfacing with its respective hardware. Each recorder funnels its sensor's data into the session management framework and ultimately into local storage, while also forwarding live preview data through the network layer for remote monitoring.

The application heavily leverages Android's asynchronous programming features (threads, Handlers, and coroutines) to handle the high data rates from multiple sensors in parallel. This ensures that tasks like writing to storage or processing sensor inputs do not block the main UI thread. We also integrate dependency injection (via Hilt) to manage cross-cutting concerns such as logging and configuration, which further decouples components and keeps the codebase modular.

Importantly, the Android app is designed to achieve \textbf{precise time alignment} of multi-modal data at capture time. All sensor readings and frames are timestamped against a common reference clock (either the system clock or a synchronized master clock) at the moment of recording. For example, when a session begins via a remote start command from the PC, the app initializes each sensor almost simultaneously and labels every piece of data with a timestamp that allows it to be correlated across devices later.

\textit{Figure 4.2: Layered design of the Android application. The figure outlines the app's architecture with four layers: a presentation layer (UI and ViewModels for user interaction and state), a business logic layer (managers like SessionManager, DeviceManager, and ConnectionManager coordinating recording and devices), a data acquisition layer (sensor-specific recorder modules for camera, thermal, and GSR, as well as processing components like hand segmentation), and an infrastructure layer (network communication client, local storage handlers, and performance monitors).}

\subsection{4.2.1 Thermal Camera Integration (Topdon)}

One distinguishing feature of the system is its thermal imaging capability, enabled by integrating a \textbf{Topdon TC001} thermal camera with the Android device. The Topdon TC001 is a USB-C infrared camera accessory, and the app incorporates it through the manufacturer's SDK. The integration is designed to allow real-time thermal video capture in parallel with the phone's normal camera feed. In practice, the Android device acts as a USB host (via OTG), and the app uses the SDK's APIs for device discovery, configuration, and frame retrieval.

Once the thermal camera is attached, the app's \texttt{ThermalRecorder} module manages its entire lifecycle: it listens for USB-connect events, requests the necessary USB permission from the Android system, and initializes the camera feed. The Topdon camera provides a thermal image at a native resolution of \textbf{256×192 pixels} and a frame rate of \textbf{25 FPS}, which the app uses as the default thermal video mode. These parameters (resolution, frame rate, and calibration settings) are configurable via a \texttt{ThermalCameraSettings} profile if needed to balance image detail against performance.

Each captured thermal frame includes both an infrared image (often visualized as a color or grayscale thermogram) and the underlying temperature reading for each pixel. The \texttt{ThermalRecorder} retrieves frames via the SDK's callbacks on a background thread to avoid blocking the UI. It timestamps each frame with a high-resolution timestamp (synchronized to the system or master clock) and queues the frame for further processing and storage.

The app continuously writes the raw thermal data stream to a file in real time. This file uses a proprietary binary format containing a header (with metadata such as resolution, frame rate, and calibration parameters) followed by the sequence of frames, each tagged with its timestamp. This raw log preserves all thermal data for precise post-hoc analysis of temperature values.

\textit{Figure 4.3: Thermal camera integration flow. This figure illustrates how the Android app interfaces with the Topdon TC001 thermal camera via USB. When the camera is plugged in, the app's USB listener requests permission from the Android OS and initializes the Topdon SDK. During a recording session, the app continuously pulls thermal frames from the camera at approximately 25 Hz.}

\subsection{4.2.2 GSR Sensor Integration (Shimmer)}

Beyond cameras, the system also incorporates a physiological sensor—specifically a \textbf{Shimmer3 GSR+} device—which captures galvanic skin response (electrodermal activity) and can optionally provide additional signals like photoplethysmogram (PPG) and motion (via built-in accelerometers). The Shimmer3 GSR+ is a research-grade wearable sensor that connects to the phone via Bluetooth. The Android app's \texttt{ShimmerRecorder} module is responsible for managing this Bluetooth connection and logging the data from the device.

When the app starts up (or whenever the user initiates a Shimmer connection), it scans for the Shimmer device and pairs with it (using the device's default PIN and name as needed). The integration uses the Shimmer Android SDK (from Shimmer Research), which provides an API—classes like \texttt{ShimmerBluetoothManagerAndroid}—to handle the low-level Bluetooth link and streaming of sensor data.

After establishing the connection, the app configures the Shimmer device with the desired sensor channels and sampling rate. By default it enables the GSR (skin conductance) channel on the Shimmer, along with the PPG channel and basic kinematic channels (the accelerometer axes) to provide context, all sampled at \textbf{51.2 Hz}. The GSR measurement range on the device is set to a suitable sensitivity (for example, a ±4 µS range) to capture typical skin conductance levels.

Data from the Shimmer arrives as a continuous stream of packets, each containing a set of measurements (GSR, PPG, etc.) along with a timestamp from the Shimmer's own internal clock. The \texttt{ShimmerRecorder} runs a dedicated thread to listen for these incoming packets through the Shimmer SDK's callback system. As each packet comes in, the app immediately captures a corresponding local timestamp (to mark when it was received) and then parses out the sensor values.

\textit{Figure 4.4: GSR sensor (Shimmer3) integration. The figure shows the Shimmer GSR+ device wirelessly connected to the Android smartphone via Bluetooth. In a recording session, the Android app subscribes to the Shimmer's data stream, receiving packets that contain GSR and PPG readings.}

\section{4.3 Desktop Controller Design and Functionality}

The desktop controller is a Python-based application with a comprehensive graphical user interface, acting as the command center for the whole multi-sensor system. The GUI is built with the \textbf{PyQt5} framework, and beneath it runs a collection of backend services and managers that handle tasks like device communication, data management, and synchronization. Architecturally, the desktop application is organized into layers and components that mirror many of the roles present in the Android app—albeit at a higher, system-wide level of coordination.

A \textbf{Presentation Layer} forms the top level of the desktop software. It comprises the main window and a set of UI panels (tabs) that the researcher uses to control and monitor the system. For example, the interface includes:
\begin{itemize}
\item Devices tab – to manage connected Android devices and other sensors
\item Recording tab – to start/stop recording sessions and to view live previews of sensor data
\item Calibration tab – to perform multi-camera calibration procedures
\item Files/Analysis tab – to review and manage the recorded data after a session
\end{itemize}

This user interface is designed to be intuitive and responsive, giving real-time feedback on system status (such as device connection state, battery levels, and recording progress) during an experiment.

Beneath the UI is the \textbf{Application Layer}, which implements the core logic. The centerpiece of this layer is the main application controller (or "Session Manager") on the PC, which orchestrates the workflow of each recording session: it responds to user inputs from the UI, coordinates timing across devices, and updates the UI with status information.

At the next layer down, the desktop application includes a set of \textbf{Service Layer} backend components that handle specific types of I/O and background processing. Notable services in this layer include:
\begin{itemize}
\item Network Service – implements the socket server that listens for connections from the Android apps
\item Webcam Service – controls any USB webcams attached to the PC
\item Shimmer Service – provides an alternative way to connect to Shimmer sensors directly from the PC
\item File Service – manages data storage and file operations on the PC side
\end{itemize}

All these services feed into the \textbf{Infrastructure Layer} on the PC, which handles cross-cutting concerns like logging, synchronization, and error management. A dedicated synchronization engine runs on the desktop to maintain the master clock and keep time aligned across devices.

\textit{Figure 4.5: High-level architecture of the Desktop Controller application. The figure breaks down the desktop software into its main components: a GUI layer, an application logic layer, and a service layer, with an infrastructure layer beneath containing the synchronisation engine, logging system, and error handling modules.}

\section{4.4 Communication Protocol and Synchronisation Mechanism}

A core challenge of this project is enabling reliable, low-latency communication between the PC controller and the Android devices, along with a mechanism to synchronise their clocks for coordinated actions. The system addresses this with a custom-designed communication protocol built on standard networking protocols, and an integrated synchronisation service that keeps all devices aligned to a master clock.

\subsection{Communication Protocol}

The Android app and desktop controller communicate over \textbf{TCP/IP} using a \textbf{JSON-based messaging protocol}. Upon startup, the desktop controller opens a server socket (by default on TCP port 9000) and waits for incoming connections. Each Android device, when its app is launched, will initiate a connection to the PC's IP address and port. A simple handshake is performed in which the Android sends an identifying message containing its device ID (a unique name or serial number) and a list of its capabilities.

All messages between PC and Android are formatted as JSON objects, with a length-prefixed framing (the first 4 bytes of each message indicate the message size) to ensure the stream is parsed correctly. This design avoids reliance on newline characters or other fragile delimiters that could be unreliable for binary data.

The protocol defines several message types, including:
\begin{itemize}
\item control commands (e.g. start\_recording, stop\_recording)
\item status updates (e.g. periodic status messages from Android with battery level, free storage, current recording status, etc.)
\item sensor data messages for streaming (such as preview\_frame for a JPEG preview image, or gsr\_sample for a live GSR data point)
\item acknowledgments (the devices reply to key commands with an ACK/NACK to confirm receipt)
\end{itemize}

\subsection{Synchronisation Mechanism}

Achieving time synchronisation across devices is critical because we want, for instance, a thermal frame and a GSR sample that occur at the "same time" to truly represent the same moment. In a distributed system with independent clocks, our approach is to designate the desktop PC as the \textbf{master clock} and synchronise all other devices to it.

The desktop controller runs a component called the \texttt{MasterClockSynchronizer} (or Synchronisation Engine) which fulfills two primary roles: it distributes the current master time to clients (devices) and coordinates simultaneous actions based on that time. Concretely, the PC launches a lightweight \textbf{NTP (Network Time Protocol)} server on a UDP port (default 8889) to which devices can query for time.

The Android app, upon connecting, performs an initial clock sync handshake to measure the offset between its local clock and the PC clock. Given the typical latencies on a local network are low (on the order of a few milliseconds), this offset can be estimated with high precision using techniques akin to Cristian's algorithm or NTP's exchange.

When a recording session is initiated, the PC doesn't just send a blind "start" command — it issues a \textbf{coordinated start time}. For example, the PC might determine "start recording at time T = 1622541600.000 (Unix epoch seconds)" a few hundred milliseconds in the future, and send a message to each device: "start\_recording at T with session\_id X". Each Android device receives this and waits until its local clock (synchronised to master) hits T to begin capturing data.

Because all devices are sync'd to the master within a few milliseconds accuracy, this effectively aligns the start of recording across devices to a very tight margin (usually well below 50 ms difference, often within a few ms). The devices then proceed to timestamp their data relative to this common start.

\textit{Figure 4.6: Communication and synchronisation sequence. This figure illustrates the sequence of interactions for device connection and a synchronised session start, showing how the protocol and sync mechanism work in tandem to coordinate distributed devices in time.}
