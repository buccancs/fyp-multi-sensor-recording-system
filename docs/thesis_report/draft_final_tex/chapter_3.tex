\label{chap:3}
\chapter{Requirements and System Analysis}

\section{Problem Statement and Research Context}

The system is developed to support contactless Galvanic Skin Response (GSR) prediction research. Traditional GSR measurement requires contact sensors attached to a person's skin, but this project aims to bridge contact-based and contact-free physiological monitoring. In essence, the system enables researchers to collect synchronized multi-modal data -- combining wearable GSR sensor readings with contactless signals like thermal imagery and video -- to facilitate the development of models that predict GSR without direct skin contact. This addresses a key research gap: providing a reliable way to acquire ground-truth GSR data alongside contactless sensor data in experiments, ensuring all data streams are aligned in time for analysis.

The research context for this system is physiological computing and affective computing. The focus is on stress and emotion analysis, where GSR is a common measure of sympathetic nervous system activity. By integrating thermal cameras, RGB video, and inertial sensors with the GSR sensor, the system creates a rich dataset for exploring how observable signals (like facial thermal patterns or motion) correlate with actual skin conductance changes. The multi-sensor recording platform operates in real-world environments (e.g. lab or field studies) and emphasizes temporal precision and data integrity so that subtle physiological responses can be captured and later aligned for machine learning model training. Overall, the system's goal is to facilitate experiments that would simultaneously record a participant's physiological responses and visual/thermal cues, providing a foundation for research into contactless GSR prediction methods.

\section{Requirements Engineering Approach}

\textbf{Stakeholder Analysis:} We identified four primary stakeholders whose needs shaped the requirements:

\begin{itemize}
\item \textbf{Research scientists:} Require accurate, high-fidelity data and an easy-to-use system during experiments.
\item \textbf{Study participants:} Require unobtrusive monitoring for comfort and privacy (hence the push for contactless methods and minimal wearable hardware).
\item \textbf{Technical developers/maintainers:} Require the software to be maintainable, extensible, and testable for long-term use.
\item \textbf{Ethics committees (IRBs):} Concerned with participant safety and data security/privacy.
\end{itemize}

Each stakeholder group contributed distinct requirements. For example, researchers emphasized precise data synchronization and multi-modal integration; participants influenced requirements for comfort and anonymity; developers demanded a modular architecture and high code quality for reliability; and ethics boards highlighted the need for secure data handling.

Given the experimental nature of the project, the requirements process was iterative and incremental. We followed an agile-like approach: initial core requirements were derived from the research objectives (e.g. ``record thermal and video data synchronized with GSR''), and a prototype system was quickly built to test feasibility. As we integrated actual sensors and conducted trial runs, new requirements and refinements emerged (for instance, the need for automatic device re-connection if a drop occurs, or a way to log stimulus events during recording). The development was evolutionary, with each code commit often implementing or refining a specific requirement (for example, adding the Shimmer sensor integration or improving time synchronization). This process ensured that requirements stayed aligned with practical implementation feedback.

We also followed standard software requirements specification practices (per IEEE guidelines) to document each requirement with a unique identifier, a description, and a priority. Requirements were categorized as functional or non-functional for clarity. Throughout development, we placed a strong emphasis on validation and testing to ensure each requirement was met. A comprehensive test suite (with over 95\% code coverage) automatically checked that each functional requirement (device communication, data recording, etc.) worked as expected in real-world scenarios. In summary, a combination of up-front analysis and continuous iteration produced a set of requirements that guided the system's design and implementation.

\section{Functional Requirements Overview}

Table~\ref{tab:functional-requirements} lists the Functional Requirements (FR) for the multi-sensor recording system. Each requirement has a unique ID and a priority level (H = High, M = Medium). These requirements describe what the system must do to meet the needs of coordinating multiple devices, acquiring various sensor data streams, synchronizing and storing data, and supporting the researcher's workflow.

\begin{table}[htbp]
\centering
\caption{Functional Requirements}
\label{tab:functional-requirements}
\begin{tabular}{|p{1cm}|p{10cm}|c|}
\hline
\textbf{ID} & \textbf{Functional Requirement Description} & \textbf{Priority} \\
\hline
FR-01 & \textbf{Centralized Multi-Device Coordination:} Provide a PC-based master controller (application) that can connect to and manage multiple remote recording devices (Android smartphones). This allows one operator to initiate and control recording sessions on all devices from a single interface. & H \\
\hline
FR-02 & \textbf{User Interface for Session Control:} The PC controller shall have a graphical user interface (GUI) for setting up sessions, displaying device status, and controlling recordings (start/stop). The GUI should display all connected devices and let the user monitor the recording process in real time. & H \\
\hline
FR-03 & \textbf{High-Precision Synchronization:} Synchronize all data streams (video frames, thermal frames, GSR samples) to a common timeline. All devices should start recording nearly simultaneously, achieving time alignment within $\sim$1 ms. Each frame or sample must be timestamped to allow precise cross-modal alignment. & H \\
\hline
FR-04 & \textbf{Visual Video Capture:} Each Android recording device shall capture high-resolution RGB video of the participant during the session. The system should support at least 30 fps at HD (720p) or higher (up to the device's capabilities, e.g. 1080p or 4K). The video recording should be continuous for the session duration and saved in a standard format (e.g. MP4). & H \\
\hline
FR-05 & \textbf{Thermal Imaging Capture:} If a device has a thermal camera, capture thermal infrared video in parallel with the RGB video. Thermal frames must be recorded at the highest available resolution and frame rate (device-dependent) and time-synchronized with the other streams. This provides contactless skin temperature data corresponding to the participant's physiological state. & H \\
\hline
FR-06 & \textbf{GSR Sensor Integration:} Integrate Shimmer GSR sensor devices to collect ground-truth physiological signals (electrodermal activity, etc.). The PC can interface with the Shimmer either directly via Bluetooth or via an Android phone acting as a relay. All connected GSR sensors should be handled concurrently, and their data samples (GSR conductance, plus any other channels like PPG or accelerometer) must be timestamped and synchronized with the session timeline. & H \\
\hline
FR-07 & \textbf{Session Management and Metadata:} Allow the user to create a new recording session which is automatically assigned a unique Session ID. During a session, the controller maintains metadata (session start time, optional configured duration, list of active devices/sensors). When a session starts, each device and sensor is recorded in the session metadata; when the session stops, the end time and duration are also recorded. A session metadata file (e.g. JSON or text) shall be saved summarizing the session details for future reference. & H \\
\hline
FR-08 & \textbf{Local Data Storage (Offline-First):} All recording devices shall store captured data locally on the device during the session (rather than streaming everything) to avoid dependence on continuous network connectivity. Each phone saves its video stream as files locally (and the PC saves any data it captures), and GSR data is logged to a local file (e.g. CSV) on whichever system is collecting it. Each data file is timestamped or contains timestamps internally. This offline-first design ensures no data is lost if the network connection is disrupted, maximizing reliability. & H \\
\hline
FR-09 & \textbf{Data Aggregation and Transfer:} After a session ends, the system shall automatically aggregate the distributed data. The PC controller will instruct each Android device to transfer its recorded files (videos, sensor data, etc.) to the PC over the network. Files are sent in chunks with verification -- the PC confirms file sizes and integrity upon receipt. All files from the session are collected into the PC's session folder for central storage. (If automatic transfer fails or is unavailable, manual file retrieval is allowed as a fallback.) & M \\
\hline
FR-10 & \textbf{Real-Time Status Monitoring:} The PC interface shall display real-time status updates from each connected device, including indicators such as recording state (recording or idle), battery level, available storage, and connection health. During an active session, the operator can see that all devices are recording and get alerts (e.g. low battery warnings) in real time. Optionally, the PC may also show a low-frame-rate preview (thumbnail) of each video stream for verification. These status and preview updates help the user ensure data quality throughout the session. & M \\
\hline
FR-11 & \textbf{Event Annotation:} The system shall allow the researcher to annotate events during a recording session. For example, if a stimulus is presented at a certain time, the researcher can log an event via the PC app (or a hardware trigger). The event is recorded with a timestamp (relative to session start) and a brief description or type. All such events are saved (e.g. in a stimulus\_events.csv file in the session folder) to facilitate aligning external events with the physiological data during analysis. & M \\
\hline
FR-12 & \textbf{Sensor Calibration Mode:} Provide a mode or tools to calibrate sensors and configure their settings before a session. This includes the ability to capture calibration data for the cameras (e.g. to spatially align a thermal camera with the RGB camera using a reference pattern) and to adjust sensor settings (focus, exposure, thermal sensitivity, etc.) as needed. Calibration data (like checkerboard images or known thermal target images) are stored in a dedicated calibration folder for the session or device. This ensures the multi-modal data can be properly registered and any sensor biases can be corrected in post-processing. & M \\
\hline
FR-13 & \textbf{Post-Session Data Processing:} Support optional post-processing steps on the recorded data to enrich the dataset. For example, after a session, a hand segmentation algorithm could be run on the video frames to identify and crop the participant's hand region (since GSR is typically measured on the hand). If this feature is enabled, the PC controller will automatically invoke the hand segmentation module on the session's video files and save the results (segmented images or masks) in the session folder. This post-processing is configurable by the user and helps automate part of the data analysis preparation. & M \\
\hline
\end{tabular}
\end{table}

\textbf{Discussion:} The functional requirements above cover the core capabilities of the system. Together they ensure that the multi-sensor recording system can capture synchronized data from multiple devices and sensors and manage that data effectively for research use. Coordination of multiple devices and tight time sync (FR-01, FR-02, FR-03) were top priorities, as precise alignment of different data modalities is essential. Requirements FR-04, FR-05, and FR-06 address the data collection needs for each modality (visual video, thermal imaging, and GSR), reflecting the system's multi-modal scope. Session handling and data management (FR-07, FR-08, FR-09) form the backbone that guarantees recordings are well-organized and safely stored (for example, by creating session metadata and using on-device storage to prevent data loss). Real-time feedback and user control (FR-10, FR-11) improve the system's usability during experiments, allowing the operator to monitor progress and mark important moments. Finally, FR-12 and FR-13 provide advanced capabilities (sensor calibration and post-processing) that enhance data quality and utility; these are medium priority since the system can function without them, but they add value for achieving high-quality research results. Many of these requirements are directly supported by the implementation -- for instance, the code's ShimmerManager class demonstrates the multi-sensor integration and error handling for GSR sensors, and the session management logic creates metadata files and directory structures as specified. The next section discusses the constraints and quality attributes (non-functional requirements) that the system must also meet.

\section{Non-Functional Requirements}

Beyond the features and behaviors described above, the system must satisfy several Non-Functional Requirements (NFR) defining performance, reliability, usability, and other quality attributes. Table~\ref{tab:non-functional-requirements} summarizes the key NFRs (with unique IDs and priorities). These ensure the system not only works, but works effectively and reliably in its intended contexts (e.g. research labs, possibly mobile or field environments with human participants).

\begin{table}[htbp]
\centering
\caption{Non-Functional Requirements}
\label{tab:non-functional-requirements}
\begin{tabular}{|p{1cm}|p{10cm}|c|}
\hline
\textbf{ID} & \textbf{Non-Functional Requirement Description} & \textbf{Priority} \\
\hline
NFR-01 & \textbf{Real-Time Performance:} The system shall operate in real time, handling incoming data without significant delay. All components must be efficient enough to capture video at full frame rate and sensor data at full sampling rate, with no frame drops or excessive buffering. For example, the Android app should sustain 30 FPS video recording while sampling GSR at $\sim$50 Hz simultaneously. The end-to-end latency from capturing a sample/frame to logging it with a timestamp should be very low (well below 100 ms) to keep the system responsive. & High \\
\hline
NFR-02 & \textbf{Synchronization Accuracy:} The system's time synchronization and triggering mechanisms must be precise (see FR-03). The design should ensure any timestamp difference between devices is only on the order of a few milliseconds. In practice this may require time sync protocols or clock calibration. Each data sample is tagged with both the device's local time and a unified reference time for alignment during analysis. This ensures the dataset is properly time-aligned across modalities. & High \\
\hline
NFR-03 & \textbf{Reliability and Fault Tolerance:} The system must be reliable during long recording sessions. It should handle errors gracefully -- for example, if a device temporarily disconnects (due to a network drop or power issue), the system will attempt to reconnect automatically and continue the session without crashing. Data already recorded should remain safe (saved locally on devices). The system should not lose or corrupt data even if an interruption occurs; any partial data should be cleanly saved up to that point. Robust error handling and recovery mechanisms are in place (e.g. retry logic for device connections and file transfers). & High \\
\hline
NFR-04 & \textbf{Data Integrity and Accuracy:} Ensure the integrity and accuracy of all recorded data. All data files (videos, sensor CSVs, etc.) should be verified for correctness after recording -- for example, during file transfer the system confirms file sizes and acknowledges receipt. Timestamps must remain consistent and accurate (with no significant clock drift during a session). The GSR sensor data should be sampled at a stable rate and recorded with correct units (e.g., microSiemens for conductance) without clipping or quantization errors. This is crucial for the scientific validity of the dataset. & High \\
\hline
NFR-05 & \textbf{Scalability (Multi-Device Support):} The architecture should scale to multiple recording devices running concurrently. Adding more Android devices (or additional Shimmer sensors) to a session should have only a manageable (approximately linear) impact on performance. The network and PC controller must handle the bandwidth of multiple video streams and sensor feeds. At minimum, the system should support at least two Android devices plus one or two Shimmer sensors recording together. The design (e.g. multi-threaded server, asynchronous I/O) allows adding more devices with minimal modifications. & Medium \\
\hline
NFR-06 & \textbf{Usability and Accessibility:} The system's user interface and workflow shall be designed for ease of use by researchers who may not be software experts. The PC application should be straightforward to install and run, and starting a recording session should be simple (for example, devices auto-discover the PC and a single click begins recording). Visual feedback (related to FR-10) is provided to reassure the user that devices are recording properly. The Android app should require minimal interaction -- ideally it launches and connects automatically to the PC. Clear notifications or dialogs should guide the user if any issues occur (such as missing permissions or errors). The system should also be well-documented so that new users can learn to operate it quickly. & High \\
\hline
NFR-07 & \textbf{Maintainability and Extensibility:} Design the software with clean code and a modular architecture to facilitate maintenance and future extensions. For example, the Android app follows an MVVM (Model-View-ViewModel) architecture with dependency injection (Hilt) to separate concerns, making it easier to modify or upgrade components (such as replacing the camera subsystem or adding a new sensor) without affecting other parts. The PC software likewise separates the GUI, networking, and data management into distinct modules. We enforced code quality metrics (e.g. complexity limits) and maintained a high level of automated test coverage. This way, developers can refactor or add features with confidence, and the system remains sustainable as a research platform. & Medium \\
\hline
NFR-08 & \textbf{Portability:} The system should be portable and not rely on specialized or expensive hardware beyond the sensors and standard devices. The PC controller is a cross-platform Python application that runs on typical laptops or desktops (requiring only moderate processing power, $\sim$4 GB RAM, and Python 3.8+). The Android app runs on common Android devices (Android 8.0 or above) and supports a range of phone models, as long as they have the required sensors (camera, etc.) and Bluetooth for the Shimmer. This allows the system to be deployed in different laboratories or even off-site (using a standard Wi-Fi router or hotspot). Communication between PC and mobiles uses standard interfaces (TCP/IP network with JSON messages) with no wired connections needed, providing flexibility in where and how the system can be used. & Medium \\
\hline
NFR-09 & \textbf{Security and Data Privacy:} Basic security measures should be in place even in controlled research settings. Network communication (commands and file transfers over JSON) should be confined to a secure local network, and only authorized devices (ones that perform the correct handshake) are allowed to connect to the PC controller to prevent unauthorized access. Additionally, participant data (video and physiological signals) are sensitive, so the system should support options to encrypt stored data or otherwise protect data at rest in line with institutional guidelines. For example, recorded files can be kept on encrypted drives or kept pseudonymized by not using personal identifiers in file names. (Full real-time encryption of streams is not implemented in this version, but this requirement is noted to encourage ethical data handling.) & Medium \\
\hline
\end{tabular}
\end{table}

\textbf{Discussion:} These non-functional requirements underscore the system's quality attributes needed for research use. Real-time performance (NFR-01) and precise synchronization (NFR-02) ensure the data quality meets scientific standards -- the system can capture high-resolution, high-frequency data in sync, which is essential for meaningful analysis. Reliability and data integrity (NFR-03, NFR-04) are critical because experimental sessions are often unrepeatable; the system must not crash or lose data during a trial. Scalability (NFR-05) anticipates that the research may expand to more devices or participants -- the system's distributed architecture (multi-threaded server and modular device handling) was designed to accommodate growth with minimal rework. Usability (NFR-06) was a high priority because a complicated setup could impede researchers; features like automatic device discovery and real-time feedback were included to make the system as user-friendly as possible. Maintainability and extensibility (NFR-07) have been addressed by rigorous software engineering practices (we extensively refactored and documented the code to manage complexity), meaning the system can be updated or improved (e.g., adding a new sensor type or algorithm) by future developers with relative ease. Portability (NFR-08) ensures the system can be used in various settings -- in a lab or out in the field -- without requiring heavy infrastructure. Finally, security and privacy (NFR-09) reflect an ethical dimension: while not the primary focus, the system is designed to run on closed local networks and can incorporate data protection measures so that sensitive participant data is safeguarded. In summary, the NFRs complement the functional requirements by ensuring the system operates efficiently, reliably, and responsibly in practice.

\section{Use Case Scenarios}

To illustrate intended usage, this section describes several primary use case scenarios. These scenarios represent typical workflows for the multi-sensor recording system and demonstrate how the functional requirements work together to support research activities. The main actor is the Researcher using the PC Master Controller, with secondary actors being the Recording Devices (Android phones running the app) and the Participant(s) being recorded. The scenarios assume that all devices have been set up on the same network and the software is running on the PC and phones.

\subsection{Use Case 1: Multi-Participant Recording Session}

\textbf{``Conduct a synchronized multi-sensor recording for one or more participants.''}

In this primary scenario, a researcher conducts an experimental session involving physiological monitoring. The steps are as follows:

\textbf{Setup:} The researcher powers on all the Android devices (for example, two smartphones, each positioned to record a different participant) and starts the recording app on each. Each participant is fitted with a Shimmer GSR sensor (worn on the fingers) which is either connected directly to the PC or paired with one of the phones. All devices join the same Wi-Fi network. The researcher launches the PC Controller application and sees that each device has automatically registered with the PC (each phone sends a ``hello'' announcement to the PC, and appears in the PC's UI as an available device). The PC interface shows each device's identifier and its capabilities (e.g., ``Device A: camera + thermal'' and ``Device B: camera + GSR'').

\textbf{Initiate Recording:} The researcher optionally enters session details in the PC interface (like a session name or notes) and clicks ``Start Session.'' The PC controller broadcasts a start recording command to all connected devices. Each Android device receives the command and begins recording its data: the cameras start capturing video (saving to MP4 files locally), and if a device has a Shimmer sensor paired, it starts streaming GSR data to a local file. (If the PC itself is also recording, e.g. via a USB webcam, it begins recording at this time as well.) All devices are synchronized to start together -- they either begin immediately upon receiving the command or use a coordinated start time so their clocks align (the system accounts for network latency by prepping devices in advance). The PC UI updates to show that each device is recording (e.g., an indicator or timer for each), and a session timer begins on the PC.

\textbf{Data Collection:} During the session (which could range from a few minutes to an hour or more), the system continuously collects data from all devices. Each smartphone writes video frames to its local storage and periodically sends status updates to the PC (including info like battery level, recording duration, file size, etc., per FR-10). The Shimmer sensors continuously stream GSR readings (and possibly additional signals like PPG or accelerometer). If a Shimmer is connected directly to the PC, the PC logs those readings in real time; if a Shimmer is connected via a phone, the phone relays the sensor data packets to the PC over the network or simply stores them to merge with its own file later. All data streams are timestamped consistently (each device maintains a synchronized clock or uses timestamps from the PC) so they align on a common timeline. If any device encounters an error (for example, a camera error or a Bluetooth disconnection), it automatically attempts to recover (restart the camera, reconnect the sensor) without requiring user intervention, as long as the session is ongoing. The researcher can glance at the PC dashboard to monitor progress -- for instance, a small preview image might update for each device, confirming the video feeds, and status text like ``Device A: Recording 120 s, Battery 85\%'' shows that things are running smoothly.

\textbf{Concurrent Participants:} This use case supports multiple participants simultaneously. For example, if two participants are interacting together, each participant is recorded by a separate phone and has a separate GSR sensor. The PC coordinates both data streams. Essentially, the steps above occur in parallel for each device -- because of the system's scalable design, it can handle two (or more) sets of data as easily as one. The PC's session management will log both devices under the same session ID, and all the data will share a synchronized timeline. This enables the researcher to capture social or group scenarios with synchronized physiological measurements for each person.

\textbf{Stop Session:} When the experiment or recording duration is complete, the researcher clicks ``Stop Session'' on the PC. The PC controller sends a stop command to all devices. Each Android device stops recording: it finalizes its video file (ensuring the file is properly saved) and finalizes any sensor data file. Each device then sends a final status update (e.g., ``Saved 300 s of video, file size 500 MB''). At this point, the PC begins the data aggregation process (per FR-09): it requests each device to transmit its recorded files. One by one, the phones send their files to the PC -- for example, Device A sends its session\_\textless timestamp\textgreater\_rgb.mp4 and session\_\textless timestamp\textgreater\_thermal.mp4 files, Device B sends its session\_\textless timestamp\textgreater\_rgb.mp4 and session\_\textless timestamp\textgreater\_sensors.csv. The PC receives these files in chunks over the network and writes them into a structured session folder on the PC (e.g., recordings/session\_\textless timestamp\textgreater/DeviceA\_rgb.mp4, etc.). The PC verifies that each received file's size matches what the device reported. Once all files from all devices have been received, the PC marks the session as completed, updates the session metadata (inserting the end time and duration), and shows a summary to the researcher (for example, ``Session completed: 2 devices, 2 video files, 1 sensor file, duration 5:00''). The researcher now has all the data centrally on the PC and can proceed to analyze it.

\textbf{Post-conditions:} As a result of this use case, all relevant multi-modal data is recorded and consolidated. The session folder on the PC contains all files for the session, organized by device or type -- for example, each device's video files, any thermal video, the GSR CSV file(s), plus the session metadata and an events log. The researcher thus obtains a complete, time-synchronized dataset from the multi-participant session, ready for model training or other analysis.

\subsection{Use Case 2: System Calibration and Configuration}

\textbf{``Calibrate sensors and configure system settings prior to recording.''}

This secondary use case is typically performed before data collection sessions as a preparatory step. The goal is to ensure all sensors are properly configured and aligned, so that the data collected will be of high quality.

\textbf{Camera Calibration:} The researcher calibrates the alignment between the RGB and thermal cameras (either the two sensors on one device, or one on each of two devices). Using a calibration function in the system (accessible via the PC UI or on the device app), the researcher places a known reference object (such as a checkerboard pattern or a thermal reference pad) in view of the cameras. They then trigger a calibration capture, prompting the system to capture a set of images from the RGB and thermal cameras at the same time. These image pairs are saved in a calibration/ folder for that session. Later, the researcher can use these images to compute a spatial transformation that maps thermal images to the RGB images (this may be done using an external script or a provided calibration tool). The resulting calibration parameters (e.g., a transformation matrix) can be saved and later used during data analysis so that the thermal and RGB data overlay correctly.

\textbf{Time Synchronization Check:} The researcher runs a synchronization test to confirm that all devices are properly time-aligned. This involves triggering a simultaneous event (like flashing a light or making a sound) that all devices can detect, then checking that the timestamps recorded by each device for this event are within the acceptable tolerance (e.g., $<$5 ms difference). If the sync is off, the system provides feedback and may re-run the NTP synchronization process to correct any drift.

\textbf{Sensor Configuration:} The researcher adjusts settings for each sensor as needed for the experimental protocol. For example, they might set the camera focus to a specific distance, adjust exposure settings, or configure the thermal camera's temperature range. For the Shimmer GSR sensor, they might set the sampling rate or enable additional channels (like heart rate or accelerometer). These settings are saved as part of the session configuration and can be reused for future sessions with similar setups.

\textbf{Validation:} Before proceeding to actual data collection, the researcher runs a brief test recording (e.g., 30 seconds) to confirm that all devices are working properly, data is being saved in the expected formats, and timestamps are aligned. This test helps catch any issues early rather than discovering problems partway through an important experimental session.