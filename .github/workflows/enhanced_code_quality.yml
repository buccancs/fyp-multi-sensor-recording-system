name: Enhanced Code Quality Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run weekly quality analysis
    - cron: '0 2 * * 1'

env:
  PYTHON_VERSION: '3.9'
  JAVA_VERSION: '17'

jobs:
  python-quality-metrics:
    name: Python Code Quality & Metrics
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/test-requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r test-requirements.txt
        pip install radon xenon complexity-report

    - name: Run complete Python linting
      run: |
        echo "::group::Black formatting check"
        black --check --diff PythonApp/
        echo "::endgroup::"
        
        echo "::group::isort import sorting check"
        isort --check-only --diff PythonApp/
        echo "::endgroup::"
        
        echo "::group::flake8 style check"
        flake8 PythonApp/ --output-file=quality-reports/flake8-report.txt || true
        echo "::endgroup::"
        
        echo "::group::pylint analysis"
        pylint PythonApp/ --output-format=json > quality-reports/pylint-report.json || true
        echo "::endgroup::"

    - name: Generate Python complexity metrics
      run: |
        mkdir -p quality-reports/complexity
        
        echo "::group::Cyclomatic complexity analysis"
        radon cc PythonApp/ --json > quality-reports/complexity/cyclomatic-complexity.json
        radon cc PythonApp/ --show-complexity --min=C
        echo "::endgroup::"
        
        echo "::group::Maintainability index"
        radon mi PythonApp/ --json > quality-reports/complexity/maintainability-index.json
        radon mi PythonApp/ --show --min=B
        echo "::endgroup::"
        
        echo "::group::Halstead complexity"
        radon hal PythonApp/ --json > quality-reports/complexity/halstead-complexity.json
        echo "::endgroup::"
        
        echo "::group::Raw metrics"
        radon raw PythonApp/ --json > quality-reports/complexity/raw-metrics.json
        echo "::endgroup::"

    - name: Check complexity thresholds
      run: |
        echo "::group::Complexity threshold validation"
        # Check for functions with complexity > 15 (matching detekt threshold)
        xenon --max-absolute C --max-modules B --max-average A PythonApp/ || {
          echo "⚠️ Code complexity exceeds recommended thresholds"
          echo "Functions should have complexity < 15 (per project guidelines)"
          exit 1
        }
        echo "::endgroup::"

    - name: Security analysis
      run: |
        echo "::group::Security vulnerability scan"
        bandit -r PythonApp/ -f json -o quality-reports/security-report.json || true
        bandit -r PythonApp/ -ll
        echo "::endgroup::"

    - name: Type checking
      run: |
        echo "::group::MyPy static type analysis"
        mypy PythonApp/ --json-report quality-reports/mypy-report || true
        echo "::endgroup::"

    - name: Technical Debt Audit
      run: |
        echo "::group::complete Technical Debt Audit"
        python scripts/tech_debt_audit.py --report --category python
        
        # Copy audit report to quality reports directory
        mkdir -p quality-reports/tech-debt
        cp audit_reports/tech_debt_audit_*.md quality-reports/tech-debt/ || true
        
        # Generate summary metrics for CI
        python scripts/tech_debt_audit.py --category python --format=json > quality-reports/tech-debt/python-debt-metrics.json || true
        
        echo "::endgroup::"

    - name: Upload Python quality reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: python-quality-reports
        path: quality-reports/
        retention-days: 30

  kotlin-quality-metrics:
    name: Kotlin Code Quality & Metrics
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up JDK
      uses: actions/setup-java@v4
      with:
        java-version: ${{ env.JAVA_VERSION }}
        distribution: 'temurin'
        
    - name: Cache Gradle dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.gradle/caches
          ~/.gradle/wrapper
        key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*', '**/gradle-wrapper.properties') }}
        restore-keys: |
          ${{ runner.os }}-gradle-

    - name: Make gradlew executable
      run: chmod +x ./gradlew

    - name: Run Detekt analysis
      run: |
        mkdir -p quality-reports/kotlin
        echo "::group::Detekt static analysis"
        ./gradlew detekt || true
        cp AndroidApp/build/reports/detekt/detekt.xml quality-reports/kotlin/ || true
        cp AndroidApp/build/reports/detekt/detekt.html quality-reports/kotlin/ || true
        echo "::endgroup::"

    - name: Generate Kotlin metrics report
      run: |
        echo "::group::Kotlin complexity metrics"
        # Use detekt's complexity report
        ./gradlew detektBaseline || true
        
        # Extract complexity metrics from detekt report
        if [ -f "AndroidApp/build/reports/detekt/detekt.xml" ]; then
          python3 -c "
import xml.etree.ElementTree as ET
import json
import sys

try:
    tree = ET.parse('AndroidApp/build/reports/detekt/detekt.xml')
    root = tree.getroot()
    
    complexity_issues = []
    for error in root.findall('.//error'):
        rule_id = error.get('source', '')
        if 'Complex' in rule_id:
            complexity_issues.append({
                'file': error.get('filename', ''),
                'line': error.get('line', ''),
                'rule': rule_id,
                'message': error.get('message', '')
            })
    
    metrics = {
        'total_complexity_issues': len(complexity_issues),
        'complexity_violations': complexity_issues,
        'threshold_compliant': len(complexity_issues) == 0
    }
    
    with open('quality-reports/kotlin/complexity-metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)
        
    print(f'Generated Kotlin complexity metrics: {len(complexity_issues)} issues found')
except Exception as e:
    print(f'Error generating complexity metrics: {e}', file=sys.stderr)
"
        echo "::endgroup::"

    - name: Upload Kotlin quality reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: kotlin-quality-reports
        path: quality-reports/kotlin/
        retention-days: 30

  complete-quality-dashboard:
    name: Quality Dashboard & Trend Analysis
    runs-on: ubuntu-latest
    needs: [python-quality-metrics, kotlin-quality-metrics]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download quality reports
      uses: actions/download-artifact@v4
      with:
        path: downloaded-reports/

    - name: Set up Python for analysis
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Generate quality dashboard
      run: |
        pip install jinja2 matplotlib plotly
        
        cat > generate_dashboard.py << 'EOF'
import json
import os
from datetime import datetime
from pathlib import Path

def generate_quality_dashboard():
    """Generate complete quality dashboard"""
    
    # Collect all quality metrics
    quality_data = {
        'timestamp': datetime.now().isoformat(),
        'commit_sha': os.environ.get('GITHUB_SHA', 'unknown'),
        'branch': os.environ.get('GITHUB_REF_NAME', 'unknown'),
        'python_metrics': {},
        'kotlin_metrics': {},
        'overall_score': 0
    }
    
    # Process Python metrics
    python_reports = Path('downloaded-reports/python-quality-reports')
    if python_reports.exists():
        # Load complexity metrics
        complexity_file = python_reports / 'complexity/cyclomatic-complexity.json'
        if complexity_file.exists():
            with open(complexity_file) as f:
                quality_data['python_metrics']['complexity'] = json.load(f)
        
        # Load maintainability metrics
        maintainability_file = python_reports / 'complexity/maintainability-index.json'
        if maintainability_file.exists():
            with open(maintainability_file) as f:
                quality_data['python_metrics']['maintainability'] = json.load(f)
    
    # Process Kotlin metrics
    kotlin_reports = Path('downloaded-reports/kotlin-quality-reports')
    if kotlin_reports.exists():
        complexity_file = kotlin_reports / 'complexity-metrics.json'
        if complexity_file.exists():
            with open(complexity_file) as f:
                quality_data['kotlin_metrics'] = json.load(f)
    
    # Calculate overall quality score
    python_score = calculate_python_score(quality_data['python_metrics'])
    kotlin_score = calculate_kotlin_score(quality_data['kotlin_metrics'])
    quality_data['overall_score'] = (python_score + kotlin_score) / 2
    
    # Generate HTML dashboard
    dashboard_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Code Quality Dashboard</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .metric-card {{ 
                border: 1px solid #ddd; 
                border-radius: 8px; 
                padding: 15px; 
                margin: 10px 0; 
                background-color: #f9f9f9; 
            }}
            .score {{ font-size: 24px; font-weight: bold; }}
            .good {{ color: #28a745; }}
            .warning {{ color: #ffc107; }}
            .danger {{ color: #dc3545; }}
        </style>
    </head>
    <body>
        <h1>🔍 Code Quality Dashboard</h1>
        <p><strong>Generated:</strong> {quality_data['timestamp']}</p>
        <p><strong>Commit:</strong> {quality_data['commit_sha'][:8]}</p>
        <p><strong>Branch:</strong> {quality_data['branch']}</p>
        
        <div class="metric-card">
            <h2>📊 Overall Quality Score</h2>
            <div class="score {get_score_class(quality_data['overall_score'])}">{quality_data['overall_score']:.1f}/10.0</div>
        </div>
        
        <div class="metric-card">
            <h2>🐍 Python Code Metrics</h2>
            <p>Complexity threshold compliance: {"✅ PASS" if python_score > 7 else "⚠️ NEEDS ATTENTION"}</p>
        </div>
        
        <div class="metric-card">
            <h2>🏗️ Kotlin Code Metrics</h2>
            <p>Complexity threshold compliance: {"✅ PASS" if kotlin_score > 7 else "⚠️ NEEDS ATTENTION"}</p>
        </div>
        
        <div class="metric-card">
            <h2>📈 Recommendations</h2>
            <ul>
                <li>{"✅" if quality_data['overall_score'] > 8 else "🔧"} Maintain complexity <15 per function</li>
                <li>{"✅" if quality_data['overall_score'] > 7 else "🔧"} Regular refactoring reviews</li>
                <li>{"✅" if quality_data['overall_score'] > 6 else "🔧"} Documentation improvements needed</li>
            </ul>
        </div>
    </body>
    </html>
    """
    
    # Save dashboard
    with open('quality-dashboard.html', 'w') as f:
        f.write(dashboard_html)
    
    # Save metrics JSON
    with open('quality-metrics.json', 'w') as f:
        json.dump(quality_data, f, indent=2)
    
    print(f"Quality dashboard generated - Overall score: {quality_data['overall_score']:.1f}/10.0")
    return quality_data['overall_score']

def calculate_python_score(metrics):
    """Calculate Python quality score (0-10)"""
    if not metrics:
        return 5.0  # Neutral score if no metrics
    
    # Base score calculation from complexity and maintainability
    score = 8.0
    
    # Adjust based on complexity (if available)
    if 'complexity' in metrics:
        # Count high complexity functions
        high_complexity_count = 0
        for module_data in metrics['complexity'].values():
            if isinstance(module_data, list):
                high_complexity_count += sum(1 for func in module_data if func.get('complexity', 0) > 15)
        
        # Penalize high complexity
        score -= min(high_complexity_count * 0.5, 3.0)
    
    return max(0.0, min(10.0, score))

def calculate_kotlin_score(metrics):
    """Calculate Kotlin quality score (0-10)"""
    if not metrics:
        return 5.0  # Neutral score if no metrics
    
    # Base score
    score = 8.0
    
    # Adjust based on complexity violations
    if 'total_complexity_issues' in metrics:
        score -= min(metrics['total_complexity_issues'] * 0.3, 4.0)
    
    # Bonus for threshold compliance
    if metrics.get('threshold_compliant', False):
        score += 1.0
    
    return max(0.0, min(10.0, score))

def get_score_class(score):
    """Get CSS class for score display"""
    if score >= 8:
        return 'good'
    elif score >= 6:
        return 'warning'
    else:
        return 'danger'

if __name__ == '__main__':
    score = generate_quality_dashboard()
    
    # Set output for GitHub Actions
    print(f"::set-output name=quality_score::{score:.1f}")
    
    # Fail if quality is too low
    if score < 6.0:
        print("::error::Code quality score is below acceptable threshold (6.0)")
        exit(1)
EOF

        python generate_dashboard.py

    - name: Upload quality dashboard
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: quality-dashboard
        path: |
          quality-dashboard.html
          quality-metrics.json
        retention-days: 90

    - name: Comment PR with quality metrics
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          try {
            const qualityData = JSON.parse(fs.readFileSync('quality-metrics.json', 'utf8'));
            
            const comment = `## 🔍 Code Quality Report
            
            **Overall Quality Score:** ${qualityData.overall_score.toFixed(1)}/10.0
            
            ### 📊 Key Metrics
            - **Python Code:** ${qualityData.python_metrics ? '✅ Analyzed' : '❌ No metrics'}
            - **Kotlin Code:** ${qualityData.kotlin_metrics ? '✅ Analyzed' : '❌ No metrics'}
            - **Complexity Compliance:** ${qualityData.overall_score > 7 ? '✅ PASS' : '⚠️ NEEDS REVIEW'}
            
            ### 🎯 Recommendations
            ${qualityData.overall_score > 8 ? '✅ Code quality is excellent!' : ''}
            ${qualityData.overall_score <= 8 && qualityData.overall_score > 6 ? '🔧 Consider refactoring complex functions (target: <15 complexity)' : ''}
            ${qualityData.overall_score <= 6 ? '⚠️ Significant quality improvements needed. Please review complexity and add documentation.' : ''}
            
            <details>
            <summary>📋 Quality Standards Checklist</summary>
            
            - [ ] All functions have complexity < 15
            - [ ] Maintainability index > B grade
            - [ ] No security vulnerabilities detected
            - [ ] Type hints present where applicable
            - [ ] Documentation for complex logic
            </details>`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not create quality report comment:', error.message);
          }