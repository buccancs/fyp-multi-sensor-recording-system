name: Advanced Testing Pipeline

on:
  push:
    # Run on all commits to any branch
  pull_request:
    # Run on all pull requests
  schedule:
    # Run nightly comprehensive tests at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - comprehensive
          - basic
          - e2e
          - visual
          - load
          - browser
          - hardware
          - android
      skip_hardware:
        description: 'Skip hardware-in-the-loop tests'
        required: false
        default: 'true'
        type: boolean

env:
  PYTHON_VERSION: '3.10'
  JAVA_VERSION: '17'
  NODE_VERSION: '18'
  
  # Test environment configuration
  GSR_TEST_CI_MODE: true
  GSR_TEST_HEADLESS: true
  GSR_TEST_LOG_LEVEL: INFO
  
  # Advanced testing configuration
  APPIUM_VERSION: '2.0.0'
  PLAYWRIGHT_BROWSERS: 'chromium firefox webkit'
  HARDWARE_TEST_TIMEOUT: 300
  VISUAL_TEST_THRESHOLD: 0.02

jobs:
  setup-test-matrix:
    name: Setup Test Matrix
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.matrix.outputs.matrix }}
      test-suite: ${{ steps.matrix.outputs.test-suite }}
    
    steps:
    - name: Determine test matrix
      id: matrix
      run: |
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          TEST_SUITE="${{ github.event.inputs.test_suite }}"
        elif [[ "${{ github.event_name }}" == "schedule" ]]; then
          TEST_SUITE="comprehensive"
        elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
          TEST_SUITE="basic"
        else
          TEST_SUITE="comprehensive"
        fi
        
        echo "test-suite=$TEST_SUITE" >> $GITHUB_OUTPUT
        
        case $TEST_SUITE in
          "basic")
            MATRIX='{"include":[{"name":"unit","markers":"unit"},{"name":"integration","markers":"integration"}]}'
            ;;
          "e2e")
            MATRIX='{"include":[{"name":"e2e","markers":"e2e"}]}'
            ;;
          "visual")
            MATRIX='{"include":[{"name":"visual","markers":"visual"}]}'
            ;;
          "load")
            MATRIX='{"include":[{"name":"load","markers":"load"}]}'
            ;;
          "browser")
            MATRIX='{"include":[{"name":"browser","markers":"browser"}]}'
            ;;
          "hardware")
            MATRIX='{"include":[{"name":"hardware","markers":"hardware_loop"}]}'
            ;;
          "android")
            MATRIX='{"include":[{"name":"android","markers":"android"}]}'
            ;;
          "comprehensive")
            MATRIX='{"include":[{"name":"unit","markers":"unit"},{"name":"integration","markers":"integration"},{"name":"e2e","markers":"e2e"},{"name":"visual","markers":"visual"},{"name":"load","markers":"load"},{"name":"browser","markers":"browser"},{"name":"android","markers":"android"}]}'
            ;;
        esac
        
        echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  # Basic test jobs (always run)
  python-tests:
    name: Python Tests
    runs-on: ubuntu-latest
    needs: setup-test-matrix
    if: contains(needs.setup-test-matrix.outputs.test-suite, 'basic') || contains(needs.setup-test-matrix.outputs.test-suite, 'comprehensive')
    
    strategy:
      matrix:
        python-version: ['3.10']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libffi-dev \
          libssl-dev \
          libjpeg-dev \
          libpng-dev \
          libfreetype6-dev \
          xvfb \
          libbluetooth-dev \
          libusb-1.0-0-dev
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -e ".[dev]"
        pip install pytest-xvfb
    
    - name: Run unit tests
      run: |
        pytest tests/ -m "unit and not hardware_loop" \
          --cov=PythonApp \
          --cov-report=xml:coverage-python-${{ matrix.python-version }}.xml \
          --junitxml=junit-python-${{ matrix.python-version }}.xml \
          -v
    
    - name: Upload coverage
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage-python-${{ matrix.python-version }}.xml
        flags: python-${{ matrix.python-version }}
        name: python-coverage-${{ matrix.python-version }}
        token: ${{ secrets.CODECOV_TOKEN }}
        fail_ci_if_error: false

  android-tests:
    name: Android Tests
    runs-on: ubuntu-latest
    needs: setup-test-matrix
    if: contains(needs.setup-test-matrix.outputs.test-suite, 'basic') || contains(needs.setup-test-matrix.outputs.test-suite, 'comprehensive')
    
    strategy:
      matrix:
        include:
          - api-level: 35
            target: google_apis
            arch: x86_64
            test-type: unit
          - api-level: 35
            target: google_apis
            arch: x86_64
            test-type: instrumented
          - api-level: 35
            target: google_apis
            arch: x86_64
            test-type: ui
          - api-level: 35
            target: google_apis
            arch: x86_64
            test-type: integration
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up JDK ${{ env.JAVA_VERSION }}
      uses: actions/setup-java@v4
      with:
        java-version: ${{ env.JAVA_VERSION }}
        distribution: 'temurin'
    
    - name: Setup Android SDK
      uses: android-actions/setup-android@v3
    
    - name: Enable KVM group perms (for emulator)
      if: matrix.test-type != 'unit'
      run: |
        echo 'KERNEL=="kvm", GROUP="kvm", MODE="0666", OPTIONS+="static_node=kvm"' | sudo tee /etc/udev/rules.d/99-kvm4all.rules
        sudo udevadm control --reload-rules
        sudo udevadm trigger --name-match=kvm
    
    - name: Cache Gradle dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.gradle/caches
          ~/.gradle/wrapper
        key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*') }}
    
    - name: Cache AVD
      if: matrix.test-type != 'unit'
      uses: actions/cache@v4
      id: avd-cache
      with:
        path: |
          ~/.android/avd/*
          ~/.android/adb*
        key: avd-${{ matrix.api-level }}-${{ matrix.target }}-${{ matrix.arch }}
    
    - name: Create AVD and generate snapshot for caching
      if: matrix.test-type != 'unit' && steps.avd-cache.outputs.cache-hit != 'true'
      uses: reactivecircus/android-emulator-runner@v2
      with:
        api-level: ${{ matrix.api-level }}
        target: ${{ matrix.target }}
        arch: ${{ matrix.arch }}
        force-avd-creation: false
        emulator-options: -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim -camera-back none
        disable-animations: true
        script: echo "Generated AVD snapshot for caching."
    
    - name: Make gradlew executable
      run: chmod +x ./gradlew
    
    - name: Clear Gradle caches
      run: |
        ./gradlew clean
        rm -rf ~/.gradle/caches/
        rm -rf .gradle/
    
    - name: Build Android app
      run: ./gradlew assembleDevDebug assembleDevDebugAndroidTest
    
    - name: Run Android unit tests
      if: matrix.test-type == 'unit'
      run: |
        ./gradlew testDevDebugUnitTest \
          --continue \
          -Pandroid.testInstrumentationRunnerArguments.class=com.multisensor.recording.testsuite.ComprehensiveTestSuite
    
    - name: Run Android instrumented tests
      if: matrix.test-type == 'instrumented'
      uses: reactivecircus/android-emulator-runner@v2
      with:
        api-level: ${{ matrix.api-level }}
        target: ${{ matrix.target }}
        arch: ${{ matrix.arch }}
        force-avd-creation: false
        emulator-options: -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim -camera-back none
        disable-animations: true
        script: |
          ./gradlew connectedDevDebugAndroidTest \
            --continue \
            -Pandroid.testInstrumentationRunnerArguments.class=com.multisensor.recording.testsuite.IntegrationTestSuite
    
    - name: Run Android UI tests
      if: matrix.test-type == 'ui'
      uses: reactivecircus/android-emulator-runner@v2
      with:
        api-level: ${{ matrix.api-level }}
        target: ${{ matrix.target }}
        arch: ${{ matrix.arch }}
        force-avd-creation: false
        emulator-options: -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim -camera-back none
        disable-animations: true
        script: |
          ./gradlew connectedDevDebugAndroidTest \
            --continue \
            -Pandroid.testInstrumentationRunnerArguments.class=com.multisensor.recording.NavigationTest,com.multisensor.recording.IDEIntegrationUITest
    
    - name: Run Android integration tests
      if: matrix.test-type == 'integration'
      uses: reactivecircus/android-emulator-runner@v2
      with:
        api-level: ${{ matrix.api-level }}
        target: ${{ matrix.target }}
        arch: ${{ matrix.arch }}
        force-avd-creation: false
        emulator-options: -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim -camera-back none
        disable-animations: true
        script: |
          ./gradlew connectedDevDebugAndroidTest \
            --continue \
            -Pandroid.testInstrumentationRunnerArguments.package=com.multisensor.recording.integration
    
    - name: Generate Android test coverage report
      if: always()
      run: ./gradlew jacocoTestReport --continue || echo "Coverage report generation failed"
    
    - name: Upload Android test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: android-test-results-${{ matrix.test-type }}-api${{ matrix.api-level }}
        path: |
          AndroidApp/build/reports/tests/
          AndroidApp/build/reports/androidTests/
          AndroidApp/build/reports/coverage/
          AndroidApp/build/outputs/androidTest-results/
        retention-days: 7
    
    - name: Upload Android coverage to Codecov
      if: matrix.test-type == 'unit'
      uses: codecov/codecov-action@v4
      with:
        file: ./AndroidApp/build/reports/jacoco/jacocoTestReport/jacocoTestReport.xml
        flags: android-${{ matrix.test-type }}
        name: android-coverage-${{ matrix.test-type }}
        token: ${{ secrets.CODECOV_TOKEN }}
        fail_ci_if_error: false

  # Advanced testing jobs
  advanced-tests:
    name: ${{ matrix.name }} Tests
    runs-on: ubuntu-latest
    needs: [setup-test-matrix, python-tests, android-tests]
    if: fromJSON(needs.setup-test-matrix.outputs.matrix).include
    
    strategy:
      matrix: ${{ fromJSON(needs.setup-test-matrix.outputs.matrix) }}
      fail-fast: false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up JDK ${{ env.JAVA_VERSION }}
      if: matrix.markers == 'e2e' || matrix.markers == 'integration'
      uses: actions/setup-java@v4
      with:
        java-version: ${{ env.JAVA_VERSION }}
        distribution: 'temurin'
    
    - name: Set up Node.js ${{ env.NODE_VERSION }}
      if: matrix.markers == 'browser' || matrix.markers == 'e2e'
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
    
    - name: Setup Android SDK
      if: matrix.markers == 'e2e' || matrix.markers == 'integration'
      uses: android-actions/setup-android@v3
    
    - name: Enable KVM group perms (for Android emulator)
      if: matrix.markers == 'e2e'
      run: |
        echo 'KERNEL=="kvm", GROUP="kvm", MODE="0666", OPTIONS+="static_node=kvm"' | sudo tee /etc/udev/rules.d/99-kvm4all.rules
        sudo udevadm control --reload-rules
        sudo udevadm trigger --name-match=kvm
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libffi-dev \
          libssl-dev \
          libjpeg-dev \
          libpng-dev \
          libfreetype6-dev \
          xvfb \
          libbluetooth-dev \
          libusb-1.0-0-dev \
          libasound2-dev \
          libgtk-3-dev \
          libdrm2 \
          libxss1 \
          libgconf-2-4 \
          libxtst6 \
          libxrandr2 \
          libasound2 \
          libpangocairo-1.0-0 \
          libatk1.0-0 \
          libcairo-gobject2 \
          libgtk-3-0 \
          libgdk-pixbuf2.0-0
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -e ".[dev]"
        pip install pytest-xvfb pytest-timeout
    
    # Cache for Android E2E tests
    - name: Cache AVD for E2E tests
      if: matrix.markers == 'e2e'
      uses: actions/cache@v4
      id: avd-cache-e2e
      with:
        path: |
          ~/.android/avd/*
          ~/.android/adb*
        key: avd-e2e-35-google_apis-x86_64
    
    - name: Create AVD for E2E tests
      if: matrix.markers == 'e2e' && steps.avd-cache-e2e.outputs.cache-hit != 'true'
      uses: reactivecircus/android-emulator-runner@v2
      with:
        api-level: 35
        target: google_apis
        arch: x86_64
        force-avd-creation: false
        emulator-options: -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim -camera-back none
        disable-animations: true
        script: echo "Generated AVD snapshot for E2E testing."
    
    # Appium setup for E2E tests
    - name: Setup Appium
      if: matrix.markers == 'e2e' || matrix.markers == 'appium'
      run: |
        npm install -g appium@${{ env.APPIUM_VERSION }}
        npm install -g @appium/doctor
        appium driver install uiautomator2
        appium-doctor --android || echo "Appium doctor warnings ignored for CI"
    
    - name: Start Appium server
      if: matrix.markers == 'e2e' || matrix.markers == 'appium'
      run: |
        appium server --port 4723 --allow-insecure chromedriver_autodownload &
        sleep 10
        curl -f http://localhost:4723/wd/hub/status || echo "Appium server status check failed"
    
    # Playwright setup for browser tests
    - name: Install Playwright browsers
      if: matrix.markers == 'browser'
      run: |
        playwright install chromium firefox webkit
        playwright install-deps
    
    # Build Android app for E2E tests
    - name: Build Android app for E2E tests
      if: matrix.markers == 'e2e'
      run: |
        chmod +x ./gradlew
        ./gradlew clean
        rm -rf ~/.gradle/caches/ || true
        rm -rf .gradle/ || true
        ./gradlew assembleDevDebug
    
    # Start Web dashboard for network tests
    - name: Start Web dashboard
      if: matrix.markers == 'e2e' || matrix.markers == 'load' || matrix.markers == 'browser'
      run: |
        cd PythonApp
        python -m web_ui.web_dashboard &
        sleep 5
        curl -f http://localhost:5000/ || echo "Web dashboard health check failed"
    
    # Run the specific test suite
    - name: Run ${{ matrix.name }} tests
      timeout-minutes: 60
      run: |
        case "${{ matrix.markers }}" in
          "e2e")
            # Start Android emulator for E2E tests
            if [ -n "$ANDROID_HOME" ]; then
              echo "Starting Android emulator for E2E tests..."
              $ANDROID_HOME/emulator/emulator -avd test -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim -camera-back none &
              
              # Wait for emulator to be ready
              adb wait-for-device
              adb shell input keyevent 82 &
              sleep 10
              
              # Install the app
              adb install AndroidApp/build/outputs/apk/dev/debug/AndroidApp-dev-debug.apk
            fi
            
            xvfb-run -a pytest tests/e2e/ -m "e2e" \
              --timeout=600 \
              --tb=short \
              -v \
              --junitxml=junit-${{ matrix.name }}.xml
            ;;
          "visual")
            xvfb-run -a pytest tests/visual/ -m "visual" \
              --timeout=300 \
              --tb=short \
              -v \
              --junitxml=junit-${{ matrix.name }}.xml
            ;;
          "load")
            pytest tests/load/ -m "load" \
              --timeout=900 \
              --tb=short \
              -v \
              --junitxml=junit-${{ matrix.name }}.xml
            ;;
          "browser")
            pytest tests/browser/ -m "browser" \
              --timeout=400 \
              --tb=short \
              -v \
              --junitxml=junit-${{ matrix.name }}.xml
            ;;
          "android")
            # Run comprehensive Android tests with emulator
            if [ -n "$ANDROID_HOME" ]; then
              echo "Starting Android emulator for comprehensive testing..."
              $ANDROID_HOME/emulator/emulator -avd test -no-snapshot-save -no-window -gpu swiftshader_indirect -noaudio -no-boot-anim -camera-back none &
              
              # Wait for emulator to be ready
              adb wait-for-device
              adb shell input keyevent 82 &
              sleep 10
              
              # Install the app
              adb install AndroidApp/build/outputs/apk/dev/debug/AndroidApp-dev-debug.apk
            fi
            
            pytest tests/ -m "android" \
              --timeout=1200 \
              --tb=short \
              -v \
              --junitxml=junit-${{ matrix.name }}.xml
            ;;
          *)
            pytest tests/ -m "${{ matrix.markers }}" \
              --timeout=300 \
              --tb=short \
              -v \
              --junitxml=junit-${{ matrix.name }}.xml
            ;;
        esac
    
    - name: Upload test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ${{ matrix.name }}-test-artifacts
        path: |
          junit-${{ matrix.name }}.xml
          tests/visual/screenshots/
          tests/visual/test_results/
          tests/load/test_results/
          tests/browser/screenshots/
          tests/e2e/screenshots/
          tests/e2e/test_results/
        retention-days: 7

  # Hardware-in-the-loop testing (separate job due to special requirements)
  hardware-tests:
    name: Hardware-in-the-Loop Tests
    runs-on: self-hosted  # Requires self-hosted runner with hardware
    needs: [setup-test-matrix, python-tests]
    if: contains(needs.setup-test-matrix.outputs.test-suite, 'hardware') && github.event.inputs.skip_hardware != 'true'
    timeout-minutes: 60
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -e ".[dev]"
        pip install pybluez pyusb pyserial
    
    - name: Check hardware availability
      run: |
        python -c "
        import sys
        sys.path.append('tests/hardware')
        from hardware_utils import validate_test_environment
        
        result = validate_test_environment()
        print(f'Hardware test environment ready: {result[\"ready\"]}')
        
        if not result['ready']:
            print('Issues:', result['issues'])
            sys.exit(1)
        "
    
    - name: Run hardware-in-the-loop tests
      timeout-minutes: 30
      run: |
        pytest tests/hardware/ -m "hardware_loop" \
          --timeout=${{ env.HARDWARE_TEST_TIMEOUT }} \
          --tb=short \
          -v \
          --junitxml=junit-hardware.xml
    
    - name: Upload hardware test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: hardware-test-results
        path: |
          junit-hardware.xml
          tests/hardware/test_results/
        retention-days: 30

  # Performance monitoring
  performance-monitoring:
    name: Performance Monitoring
    runs-on: ubuntu-latest
    needs: advanced-tests
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-artifacts
    
    - name: Generate performance report
      run: |
        python -c "
        import os
        import json
        import glob
        from pathlib import Path
        
        # Collect performance data from test artifacts
        performance_data = {
            'timestamp': '$(date -Iseconds)',
            'commit': '${{ github.sha }}',
            'branch': '${{ github.ref_name }}',
            'test_results': {}
        }
        
        # Process load test results
        load_results = glob.glob('test-artifacts/load-*/test_results/*.json')
        if load_results:
            with open(load_results[0]) as f:
                load_data = json.load(f)
                performance_data['test_results']['load'] = load_data
        
        # Process browser test results
        browser_results = glob.glob('test-artifacts/browser-*/screenshots/')
        performance_data['test_results']['browser_compatibility'] = len(browser_results) > 0
        
        # Save performance data
        with open('performance-report.json', 'w') as f:
            json.dump(performance_data, f, indent=2)
        
        print('Performance report generated')
        "
    
    - name: Upload performance report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: performance-report.json
        retention-days: 90

  # Test summary and reporting
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [python-tests, android-tests, advanced-tests]
    if: always()
    
    steps:
    - name: Download test results
      uses: actions/download-artifact@v4
      with:
        path: all-test-results
    
    - name: Generate test summary
      run: |
        python -c "
        import os
        import xml.etree.ElementTree as ET
        import glob
        import json
        from pathlib import Path
        
        summary = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'skipped_tests': 0,
            'test_suites': {}
        }
        
        # Process JUnit XML files
        junit_files = glob.glob('all-test-results/**/junit-*.xml', recursive=True)
        
        for junit_file in junit_files:
            try:
                tree = ET.parse(junit_file)
                root = tree.getroot()
                
                suite_name = Path(junit_file).stem.replace('junit-', '')
                
                tests = int(root.get('tests', 0))
                failures = int(root.get('failures', 0))
                errors = int(root.get('errors', 0))
                skipped = int(root.get('skipped', 0))
                
                summary['test_suites'][suite_name] = {
                    'tests': tests,
                    'passed': tests - failures - errors - skipped,
                    'failed': failures + errors,
                    'skipped': skipped
                }
                
                summary['total_tests'] += tests
                summary['passed_tests'] += tests - failures - errors - skipped
                summary['failed_tests'] += failures + errors
                summary['skipped_tests'] += skipped
                
            except Exception as e:
                print(f'Error processing {junit_file}: {e}')
        
        # Generate markdown summary
        with open('test-summary.md', 'w') as f:
            f.write('# Advanced Testing Pipeline Summary\n\n')
            f.write(f'**Total Tests:** {summary[\"total_tests\"]} | ')
            f.write(f'**Passed:** {summary[\"passed_tests\"]} | ')
            f.write(f'**Failed:** {summary[\"failed_tests\"]} | ')
            f.write(f'**Skipped:** {summary[\"skipped_tests\"]}\n\n')
            
            success_rate = (summary['passed_tests'] / summary['total_tests'] * 100) if summary['total_tests'] > 0 else 0
            f.write(f'**Success Rate:** {success_rate:.1f}%\n\n')
            
            f.write('## Test Suite Results\n\n')
            for suite, results in summary['test_suites'].items():
                status = '✅' if results['failed'] == 0 else '❌'
                f.write(f'- {status} **{suite.title()}**: ')
                f.write(f'{results[\"passed\"]}/{results[\"tests\"]} passed')
                if results['failed'] > 0:
                    f.write(f' ({results[\"failed\"]} failed)')
                if results['skipped'] > 0:
                    f.write(f' ({results[\"skipped\"]} skipped)')
                f.write('\n')
        
        with open('test-summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f'Test summary: {summary[\"passed_tests\"]}/{summary[\"total_tests\"]} tests passed')
        "
    
    - name: Comment PR with test summary
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          if (fs.existsSync('test-summary.md')) {
            const summary = fs.readFileSync('test-summary.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
          }
    
    - name: Upload test summary
      uses: actions/upload-artifact@v4
      with:
        name: test-summary
        path: |
          test-summary.md
          test-summary.json
        retention-days: 30

  # Deploy test reports (for main branch)
  deploy-reports:
    name: Deploy Test Reports
    runs-on: ubuntu-latest
    needs: [test-summary, performance-monitoring]
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || github.event_name == 'schedule')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: reports
    
    - name: Generate HTML reports
      run: |
        mkdir -p public/reports
        
        # Copy test artifacts
        cp -r reports/* public/reports/
        
        # Generate index page
        cat > public/index.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Multi-Sensor Recording System - Test Reports</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .header { background: #f5f5f5; padding: 15px; border-radius: 5px; }
                .reports { margin-top: 20px; }
                .report-link { display: block; margin: 10px 0; padding: 10px; border: 1px solid #ddd; border-radius: 5px; text-decoration: none; }
                .report-link:hover { background: #f0f0f0; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Multi-Sensor Recording System</h1>
                <h2>Advanced Testing Pipeline Reports</h2>
                <p>Generated: $(date)</p>
                <p>Commit: ${{ github.sha }}</p>
            </div>
            
            <div class="reports">
                <h3>Available Reports</h3>
                <a href="reports/visual-test-artifacts/" class="report-link">Visual Regression Test Results</a>
                <a href="reports/load-test-artifacts/" class="report-link">Load Testing Results</a>
                <a href="reports/browser-test-artifacts/" class="report-link">Cross-Browser Compatibility Results</a>
                <a href="reports/performance-report/" class="report-link">Performance Monitoring Data</a>
                <a href="reports/test-summary/" class="report-link">Test Execution Summary</a>
            </div>
        </body>
        </html>
        EOF
    
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./public
        destination_dir: test-reports