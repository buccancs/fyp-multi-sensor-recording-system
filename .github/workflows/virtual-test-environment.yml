name: Virtual Test Environment

on:
  push:
    branches: [ main, develop, master, 'copilot/**' ]
    paths:
      - 'PythonApp/**'
      - 'tests/integration/virtual_environment/**'
      - '.github/workflows/virtual-test-environment.yml'
      - 'run_local_test.sh'
      - 'README.md'
      - 'VIRTUAL_TEST_INTEGRATION_GUIDE.md'
  pull_request:
    branches: [ main, develop, master ]
    paths:
      - 'PythonApp/**'
      - 'tests/integration/virtual_environment/**'
      - '.github/workflows/virtual-test-environment.yml'
      - 'run_local_test.sh'
      - 'README.md'
      - 'VIRTUAL_TEST_INTEGRATION_GUIDE.md'
  workflow_dispatch:
    inputs:
      test_scenario:
        description: 'Test scenario to run'
        required: true
        default: 'ci'
        type: choice
        options:
          - ci
          - quick
          - stress
          - sync
      device_count:
        description: 'Number of virtual devices'
        required: true
        default: '3'
        type: string
      duration_minutes:
        description: 'Test duration in minutes'
        required: true
        default: '2.0'
        type: string

env:
  PYTHON_VERSION: '3.11'
  GSR_TEST_CI_MODE: true
  GSR_TEST_HEADLESS: true
  GSR_TEST_LOG_LEVEL: INFO

jobs:
  # Quick validation test for all PRs
  quick-test:
    name: Quick Virtual Test
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libffi-dev \
          libssl-dev \
          libjpeg-dev \
          libpng-dev \
          libfreetype6-dev \
          xvfb
          
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -e .
        pip install pytest pytest-asyncio psutil numpy opencv-python-headless
        
    - name: Run quick virtual test
      run: |
        cd tests/integration/virtual_environment
        python test_runner.py --scenario quick --devices 2 --duration 0.5 --verbose
        
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quick-test-results
        path: tests/integration/virtual_environment/test_results/
        retention-days: 7

  # Comprehensive CI test
  ci-test:
    name: CI Virtual Test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 15
    
    strategy:
      matrix:
        scenario: [ci, quick]
        device_count: [2, 3, 5]
        exclude:
          - scenario: quick
            device_count: 5
            
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libffi-dev \
          libssl-dev \
          libjpeg-dev \
          libpng-dev \
          libfreetype6-dev \
          xvfb
          
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -e .
        pip install pytest pytest-asyncio psutil numpy opencv-python-headless
        
    - name: Set test parameters
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "TEST_SCENARIO=${{ github.event.inputs.test_scenario }}" >> $GITHUB_ENV
          echo "DEVICE_COUNT=${{ github.event.inputs.device_count }}" >> $GITHUB_ENV
          echo "DURATION_MINUTES=${{ github.event.inputs.duration_minutes }}" >> $GITHUB_ENV
        else
          echo "TEST_SCENARIO=${{ matrix.scenario }}" >> $GITHUB_ENV
          echo "DEVICE_COUNT=${{ matrix.device_count }}" >> $GITHUB_ENV
          echo "DURATION_MINUTES=2.0" >> $GITHUB_ENV
        fi
        
    - name: Run virtual test with xvfb
      run: |
        cd tests/integration/virtual_environment
        xvfb-run -a --server-args="-screen 0 1024x768x24" \
          python test_runner.py \
          --scenario $TEST_SCENARIO \
          --devices $DEVICE_COUNT \
          --duration $DURATION_MINUTES \
          --verbose
          
    - name: Validate test results
      run: |
        cd tests/integration/virtual_environment
        # Check if test report exists and shows success
        if [ -f test_results/*_report.json ]; then
          echo "Test report found:"
          cat test_results/*_report.json | jq '.summary'
          
          # Check if test passed
          PASSED=$(cat test_results/*_report.json | jq -r '.summary.overall_passed')
          if [ "$PASSED" != "true" ]; then
            echo "Test failed according to report"
            exit 1
          fi
        else
          echo "No test report found"
          exit 1
        fi
        
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: ci-test-results-${{ matrix.scenario }}-${{ matrix.device_count }}
        path: tests/integration/virtual_environment/test_results/
        retention-days: 30

  # Docker-based test
  docker-test:
    name: Docker Virtual Test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      run: |
        cd tests/integration/virtual_environment
        docker build -t gsr-virtual-test:ci -f Dockerfile ../../..
        
    - name: Run Docker test
      run: |
        cd tests/integration/virtual_environment
        mkdir -p test_results
        docker run --rm \
          -v "$(pwd)/test_results:/app/test_results" \
          -e GSR_TEST_CI_MODE=true \
          -e GSR_TEST_LOG_LEVEL=INFO \
          gsr-virtual-test:ci \
          --scenario ci --devices 3 --duration 2.0 --verbose
          
    - name: Validate Docker test results
      run: |
        cd tests/integration/virtual_environment
        if [ -f test_results/*_report.json ]; then
          echo "Docker test report:"
          cat test_results/*_report.json | jq '.summary'
          
          PASSED=$(cat test_results/*_report.json | jq -r '.summary.overall_passed')
          if [ "$PASSED" != "true" ]; then
            echo "Docker test failed"
            exit 1
          fi
        else
          echo "No Docker test report found"
          exit 1
        fi
        
    - name: Upload Docker test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: docker-test-results
        path: tests/integration/virtual_environment/test_results/
        retention-days: 30

  # Performance baseline test
  performance-test:
    name: Performance Baseline Test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    timeout-minutes: 25
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libffi-dev libssl-dev libjpeg-dev libpng-dev xvfb
        python -m pip install --upgrade pip setuptools wheel
        pip install -e .
        pip install pytest pytest-asyncio psutil numpy opencv-python-headless
        
    - name: Run performance test
      run: |
        cd tests/integration/virtual_environment
        xvfb-run -a --server-args="-screen 0 1024x768x24" \
          python test_runner.py \
          --scenario stress \
          --devices 5 \
          --duration 5.0 \
          --verbose
          
    - name: Analyse performance results
      run: |
        cd tests/integration/virtual_environment
        if [ -f test_results/*_report.json ]; then
          echo "Performance test results:"
          cat test_results/*_report.json | jq '.performance'
          
          # Check performance thresholds
          MEMORY_MB=$(cat test_results/*_report.json | jq -r '.performance.peak_memory_mb')
          CPU_PERCENT=$(cat test_results/*_report.json | jq -r '.performance.peak_cpu_percent')
          
          echo "Peak memory usage: ${MEMORY_MB}MB"
          echo "Peak CPU usage: ${CPU_PERCENT}%"
          
          # Performance thresholds for CI
          if (( $(echo "$MEMORY_MB > 500" | bc -l) )); then
            echo "WARNING: High memory usage: ${MEMORY_MB}MB"
          fi
          
          if (( $(echo "$CPU_PERCENT > 80" | bc -l) )); then
            echo "WARNING: High CPU usage: ${CPU_PERCENT}%"
          fi
        fi
        
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: tests/integration/virtual_environment/test_results/
        retention-days: 60

  # Test summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [quick-test, ci-test]
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: all-test-results
        
    - name: Generate test summary
      run: |
        echo "# Virtual Test Environment Summary" > summary.md
        echo "" >> summary.md
        echo "## Test Results" >> summary.md
        echo "" >> summary.md
        
        for result_dir in all-test-results/*/; do
          if [ -d "$result_dir" ]; then
            artifact_name=$(basename "$result_dir")
            echo "### $artifact_name" >> summary.md
            
            report_file=$(find "$result_dir" -name "*_report.json" | head -1)
            if [ -f "$report_file" ]; then
              overall_passed=$(jq -r '.summary.overall_passed' "$report_file")
              devices=$(jq -r '.summary.devices_connected' "$report_file")
              samples=$(jq -r '.summary.data_samples_collected' "$report_file")
              errors=$(jq -r '.summary.error_count' "$report_file")
              
              if [ "$overall_passed" = "true" ]; then
                echo "- Status: ✅ PASSED" >> summary.md
              else
                echo "- Status: ❌ FAILED" >> summary.md
              fi
              
              echo "- Devices: $devices" >> summary.md
              echo "- Data samples: $samples" >> summary.md
              echo "- Errors: $errors" >> summary.md
            else
              echo "- Status: ❓ NO REPORT" >> summary.md
            fi
            echo "" >> summary.md
          fi
        done
        
        cat summary.md
        
    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });